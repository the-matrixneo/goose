/*! For license information please see a75eef8a.0a024cd9.js.LICENSE.txt */
(self.webpackChunkgoose=self.webpackChunkgoose||[]).push([[5021],{46792:(e,t,n)=>{"use strict";n.d(t,{$:()=>a});n(96540);var i=n(74848);const a=e=>{let{children:t,className:n="",variant:a="default",size:r="default",...s}=e;return(0,i.jsx)("button",{className:`flex rounded-full focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-accent dark:focus:ring-offset-gray-900 ${{default:"bg-black dark:bg-white text-white dark:text-black hover:bg-accent/90 dark:hover:bg-accent/80",ghost:"bg-transparent hover:bg-gray-100 dark:hover:bg-gray-700 dark:text-gray-300",link:"bg-transparent text-accent hover:underline hover:text-textProminent dark:text-accent/90"}[a]} ${{default:"px-6 py-3",icon:"p-2"}[r]} ${n}`,...s,children:t})}},47995:(e,t,n)=>{"use strict";n.d(t,{R:()=>a});var i=n(74848);function a(e){let{groups:t,selectedValues:n,onChange:a}=e;return(0,i.jsx)("div",{className:"w-64 pr-8",children:t.map((e=>(0,i.jsxs)("div",{className:"mb-8",children:[(0,i.jsx)("h3",{className:"text-lg font-medium mb-4 text-textProminent",children:e.title}),(0,i.jsx)("div",{className:"space-y-2",children:e.options.map((t=>(0,i.jsxs)("label",{className:"flex items-center justify-between group cursor-pointer",children:[(0,i.jsxs)("div",{className:"flex items-center",children:[(0,i.jsx)("input",{type:"checkbox",checked:(n[e.title]||[]).includes(t.value),onChange:()=>((e,t)=>{const i=n[e]||[],r=i.includes(t)?i.filter((e=>e!==t)):[...i,t];a(e,r)})(e.title,t.value),className:"form-checkbox h-4 w-4 text-purple-600 transition duration-150 ease-in-out"}),(0,i.jsx)("span",{className:"ml-2 text-sm text-textStandard group-hover:text-textProminent",children:t.label})]}),void 0!==t.count&&(0,i.jsx)("span",{className:"text-sm text-textSubtle",children:t.count})]},t.value)))})]},e.title)))})}},36107:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>g});var i=n(96540),a=n(62636),r=n(56289),s=n(74848);function o(e){let{recipe:t}=e;const n=t.author?.contact||null,[o,c]=(0,i.useState)(!1),[l,d]=(0,i.useState)({}),p=t.parameters?.filter((e=>"required"===e.requirement))||[],u=t.parameters?.filter((e=>"required"!==e.requirement))||[],m=p.length>0;return(0,s.jsxs)("div",{className:"relative w-full h-full",children:[(0,s.jsxs)(r.A,{to:`/recipes/detail?id=${t.id}`,className:"block no-underline hover:no-underline h-full",children:[(0,s.jsx)("div",{className:"absolute inset-0 rounded-2xl bg-purple-500 opacity-10 blur-2xl"}),(0,s.jsxs)("div",{className:"relative z-10 w-full h-full rounded-2xl border border-zinc-200 dark:border-zinc-700 bg-white dark:bg-[#1A1A1A] flex flex-col justify-between p-6 transition-shadow duration-200 ease-in-out hover:shadow-[0_0_0_2px_rgba(99,102,241,0.4),_0_4px_20px_rgba(99,102,241,0.1)]",children:[(0,s.jsxs)("div",{className:"space-y-4",children:[(0,s.jsxs)("div",{children:[(0,s.jsx)("h3",{className:"font-semibold text-base text-zinc-900 dark:text-white leading-snug",children:t.title}),(0,s.jsx)("p",{className:"text-sm text-zinc-600 dark:text-zinc-400 mt-1",children:t.description})]}),t.extensions.map(((e,t)=>{const n="string"==typeof e?e:e.name,i=n?.replace(/MCP/i,"").trim();return(0,s.jsx)("span",{className:"inline-flex items-center h-7 px-3 rounded-full border border-zinc-300 bg-zinc-100 text-zinc-700 dark:border-zinc-700 dark:bg-zinc-900 dark:text-zinc-300 text-xs font-medium",children:i},t)})),t.activities?.length>0&&(0,s.jsx)("div",{className:"border-t border-zinc-200 dark:border-zinc-700 pt-2 mt-2 flex flex-wrap gap-2",children:t.activities.map(((e,t)=>(0,s.jsx)("span",{className:"inline-flex items-center h-7 px-3 rounded-full border border-zinc-300 bg-zinc-100 text-zinc-700 dark:border-zinc-700 dark:bg-zinc-900 dark:text-zinc-300 text-xs font-medium",children:e},t)))})]}),(0,s.jsxs)("div",{className:"flex justify-between items-center pt-6 mt-2",children:[(0,s.jsx)("a",{href:t.recipeUrl,className:"text-sm font-medium text-purple-600 hover:underline dark:text-purple-400",target:"_blank",rel:"noopener noreferrer",onClick:e=>e.stopPropagation(),children:"Launch in Goose Desktop \u2192"}),(0,s.jsxs)("div",{className:"relative group",children:[(0,s.jsx)("button",{onClick:e=>{e.preventDefault(),e.stopPropagation(),(()=>{if(m)return d({}),void c(!0);const e=`goose run --recipe documentation/src/pages/recipes/data/recipes/${t.id}.yaml`;navigator.clipboard.writeText(e),a.Ay.success("CLI command copied!")})()},className:"text-sm font-medium text-zinc-700 bg-zinc-200 dark:bg-zinc-700 dark:text-white dark:hover:bg-zinc-600 px-3 py-1 rounded hover:bg-zinc-300 cursor-pointer",children:"Copy CLI Command"}),(0,s.jsx)("div",{className:"absolute bottom-full mb-2 left-1/2 -translate-x-1/2 hidden group-hover:block bg-zinc-800 text-white text-xs px-2 py-1 rounded shadow-lg whitespace-nowrap z-50",children:"Copies the CLI command to run this recipe"})]}),n&&(0,s.jsxs)("a",{href:`https://github.com/${n}`,target:"_blank",rel:"noopener noreferrer",className:"flex items-center gap-2 text-sm text-zinc-500 hover:underline dark:text-zinc-300",title:"Recipe author",onClick:e=>e.stopPropagation(),children:[(0,s.jsx)("img",{src:`https://github.com/${n}.png`,alt:n,className:"w-5 h-5 rounded-full"}),"@",n]})]})]})]}),o&&(0,s.jsx)("div",{className:"absolute top-0 left-0 w-full h-full bg-black bg-opacity-70 flex justify-center items-center z-50",children:(0,s.jsxs)("div",{className:"bg-white dark:bg-zinc-800 p-6 rounded-lg w-full max-w-md",children:[(0,s.jsx)("h3",{className:"text-lg font-semibold mb-4 text-zinc-900 dark:text-white",children:"Fill in parameters"}),[...p,...u].map((e=>(0,s.jsxs)("div",{className:"mb-3",children:[(0,s.jsxs)("label",{className:"block text-sm text-zinc-700 dark:text-zinc-200 mb-1",children:[e.key," ","required"!==e.requirement&&(0,s.jsx)("span",{className:"text-zinc-400",children:"(optional)"})]}),(0,s.jsx)("input",{type:"text",value:l[e.key]||"",onChange:t=>d((n=>({...n,[e.key]:t.target.value}))),className:"w-full px-3 py-2 border border-zinc-300 dark:border-zinc-600 rounded bg-white dark:bg-zinc-700 text-zinc-900 dark:text-white"})]},e.key))),(0,s.jsxs)("div",{className:"flex justify-end gap-3",children:[(0,s.jsx)("button",{onClick:()=>c(!1),className:"text-sm text-zinc-500 hover:underline dark:text-zinc-300",children:"Cancel"}),(0,s.jsx)("button",{onClick:()=>{const e=Object.entries(l).map((e=>{let[t,n]=e;return`${t}=${n}`})).join(" "),n=`goose run --recipe documentation/src/pages/recipes/data/recipes/${t.id}.yaml --params ${e}`;navigator.clipboard.writeText(n),c(!1),a.Ay.success("CLI command copied with params!")},className:"bg-purple-600 text-white px-4 py-2 rounded text-sm hover:bg-purple-700",children:"Copy Goose CLI Command"})]})]})})]})}var c=n(54631),l=n(51657),d=n(43938),p=n(52362),u=n(46792),m=n(47995),f=n(69158),h=n(25191);function g(){const[e,t]=(0,i.useState)([]),[n,a]=(0,i.useState)(""),[g,y]=(0,i.useState)({}),[v,b]=(0,i.useState)(!1),[w,k]=(0,i.useState)(!0),[x,_]=(0,i.useState)(null),[C,A]=(0,i.useState)(1),P=10,S=[{title:"Extensions Used",options:Array.from(new Set(e.flatMap((e=>e.extensions?.length?e.extensions.map((e=>("string"==typeof e?e:e.name).toLowerCase().replace(/\s+/g,"-"))):[])))).map((e=>{let t=e.replace(/-mcp$/,"").replace(/-/g," ");return t="github"===t.toLowerCase()?"GitHub":t.replace(/\b\w/g,(e=>e.toUpperCase())),{label:t,value:e}}))}];(0,i.useEffect)((()=>{const e=setTimeout((async()=>{try{k(!0),_(null);const e=await(0,c.q)(n);t(e)}catch(e){const t=e instanceof Error?e.message:"Unknown error";_(`Failed to load recipes: ${t}`),console.error("Error loading recipes:",e)}finally{k(!1)}}),300);return()=>clearTimeout(e)}),[n]);let R=e;return Object.entries(g).forEach((e=>{let[t,n]=e;n.length>0&&(R=R.filter((e=>"Extensions Used"!==t||(e.extensions?.some((e=>{const t="string"==typeof e?e:e.name;return n.includes(t.toLowerCase().replace(/\s+/g,"-"))}))??!1))))})),(0,s.jsx)(d.A,{children:(0,s.jsxs)("div",{className:"container mx-auto px-4 py-8 md:p-24",children:[(0,s.jsxs)("div",{className:"pb-8 md:pb-16",children:[(0,s.jsxs)("div",{className:"flex justify-between items-start mb-4",children:[(0,s.jsx)("h1",{className:"text-4xl md:text-[64px] font-medium text-textProminent",children:"Recipes Cookbook"}),(0,s.jsxs)(u.$,{onClick:()=>window.open("https://github.com/block/goose/blob/main/CONTRIBUTING_RECIPES.md","_blank"),className:"bg-purple-600 hover:bg-purple-700 text-white flex items-center gap-2 cursor-pointer",children:[(0,s.jsx)("svg",{xmlns:"http://www.w3.org/2000/svg",width:"20",height:"20",viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:"2",strokeLinecap:"round",strokeLinejoin:"round",children:(0,s.jsx)("path",{d:"M12 5v14M5 12h14"})}),"Submit Recipe"]})]}),(0,s.jsxs)("p",{className:"text-textProminent",children:["Save time and skip setup \u2014 launch any"," ",(0,s.jsx)(r.A,{to:"/docs/guides/recipes/session-recipes",className:"text-purple-600 hover:underline",children:"Goose agent recipe"})," ","shared by the community with a single click."]})]}),(0,s.jsx)("div",{className:"search-container mb-6 md:mb-8",children:(0,s.jsx)("input",{className:"bg-bgApp font-light text-textProminent placeholder-textPlaceholder w-full px-3 py-2 md:py-3 text-2xl md:text-[40px] leading-tight md:leading-[52px] border-b border-borderSubtle focus:outline-none focus:ring-purple-500 focus:border-borderProminent caret-[#FF4F00] pl-0",placeholder:"Search for recipes by keyword",value:n,onChange:e=>{a(e.target.value),A(1)}})}),(0,s.jsx)("div",{className:"md:hidden mb-4",children:(0,s.jsxs)(u.$,{onClick:()=>b(!v),children:[v?(0,s.jsx)(f.A,{size:20}):(0,s.jsx)(h.A,{size:20}),v?"Close Filters":"Show Filters"]})}),(0,s.jsxs)("div",{className:"flex flex-col md:flex-row gap-8",children:[(0,s.jsx)("div",{className:(v?"block":"hidden")+" md:block md:w-64 mt-6",children:(0,s.jsx)(m.R,{groups:S,selectedValues:g,onChange:(e,t)=>{y((n=>({...n,[e]:t}))),A(1)}})}),(0,s.jsxs)("div",{className:"flex-1",children:[(0,s.jsx)("div",{className:""+(n?"pb-2":"pb-4 md:pb-8"),children:(0,s.jsx)("p",{className:"text-gray-600",children:n?`${R.length} result${1!==R.length?"s":""} for "${n}"`:""})}),x&&(0,s.jsx)(p.A,{type:"danger",title:"Error",children:(0,s.jsx)("p",{children:x})}),w?(0,s.jsx)("div",{className:"py-8 text-xl text-gray-600",children:"Loading recipes..."}):0===R.length?(0,s.jsx)(p.A,{type:"info",children:(0,s.jsx)("p",{children:n?"No recipes found matching your search.":"No recipes have been submitted yet."})}):(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)("div",{className:"grid grid-cols-1 lg:grid-cols-2 gap-4 md:gap-6",children:R.slice((C-1)*P,C*P).map((e=>(0,s.jsx)(l.P.div,{initial:{opacity:0},animate:{opacity:1},exit:{opacity:0},transition:{duration:.6},children:(0,s.jsx)(o,{recipe:e})},e.id)))}),R.length>P&&(0,s.jsxs)("div",{className:"flex justify-center items-center gap-2 md:gap-4 mt-6 md:mt-8",children:[(0,s.jsx)(u.$,{onClick:()=>A((e=>Math.max(e-1,1))),disabled:1===C,className:"px-3 md:px-4 py-2 rounded-md border border-border bg-surfaceHighlight hover:bg-surface text-textProminent disabled:opacity-50 disabled:cursor-not-allowed transition-colors text-sm md:text-base",children:"Previous"}),(0,s.jsxs)("span",{className:"text-textProminent text-sm md:text-base",children:["Page ",C," of ",Math.ceil(R.length/P)]}),(0,s.jsx)(u.$,{onClick:()=>A((e=>Math.min(Math.ceil(R.length/P),e+1))),disabled:C>=Math.ceil(R.length/P),className:"px-3 md:px-4 py-2 rounded-md border border-border bg-surfaceHighlight hover:bg-surface text-textProminent disabled:opacity-50 disabled:cursor-not-allowed transition-colors text-sm md:text-base",children:"Next"})]})]})]})]})]})})}},54631:(e,t,n)=>{"use strict";n.d(t,{d:()=>a,q:()=>r});const i=n(75878);function a(e){return s().find((t=>t.id===e))||null}async function r(e){const t=s();return e?t.filter((t=>t.title?.toLowerCase().includes(e.toLowerCase())||t.description?.toLowerCase().includes(e.toLowerCase())||t.action?.toLowerCase().includes(e.toLowerCase())||t.activities?.some((t=>t.toLowerCase().includes(e.toLowerCase()))))):t}function s(){return i.keys().map((e=>function(e){const t={id:e.id||e.title?.toLowerCase().replace(/\s+/g,"-")||"untitled-recipe",title:e.title||"Untitled Recipe",description:e.description||"No description provided.",instructions:e.instructions,prompt:e.prompt,extensions:Array.isArray(e.extensions)?e.extensions.map((e=>"string"==typeof e?{type:"builtin",name:e}:e)):[],activities:Array.isArray(e.activities)?e.activities:[],version:e.version||"1.0.0",author:"string"==typeof e.author?{contact:e.author}:e.author||void 0,action:e.action||void 0,persona:e.persona||void 0,tags:e.tags||[],recipeUrl:"",localPath:`documentation/src/pages/recipes/data/recipes/${e.id}.yaml`};if(Array.isArray(e.parameters)){for(const t of e.parameters)"required"!==t.requirement||t.value||(t.value=`{{${t.key}}}`);t.parameters=e.parameters}const n={title:t.title,description:t.description,instructions:t.instructions,prompt:t.prompt,activities:t.activities,extensions:t.extensions,parameters:t.parameters||[]},i=function(e){if("undefined"!=typeof window&&window.btoa)return window.btoa(unescape(encodeURIComponent(e)));return Buffer.from(e).toString("base64")}(JSON.stringify(n));return t.recipeUrl=`goose://recipe?config=${i}`,t}({...i(e).default||i(e),id:e.replace(/^.*[\\/]/,"").replace(/\.(yaml|yml)$/,"")})))}},46611:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Analyse PR",author:{contact:"douwe"},description:"Analyse a pr",instructions:"Your job is to analyse and explain a PR",activities:["Query authentication logs","Investigate Sentry reports","Correlate device usage with auth events","Query Snowflake user identity tables","Review repo code for auth issues"],parameters:[{key:"pr",input_type:"string",requirement:"required",description:"name of the pull request"},{key:"repo",input_type:"string",requirement:"optional",description:"name of the repo. uses the current one if not selected",default:""}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing and retrieving formating preferences that might be present"}],prompt:"Analyze the pr with the name {{ pr }}. Find out what has changed, try to figure out why these\nchanges were made and tell the user in detail what you found out.\n{% if repo %}\nWe are working with the {{ repo }} repository, so make sure to add that to all commands.\n{% endif %}\n\nSteps:\n1. Find the actual pull request. {{ pr }} is the name or part of it. You can just run\n   `gh pr list`\n   and see which prs are open. Note which one the user is talking about\n2. Look at what is changed. You can run:\n   `gh pr view <pr-number> --comments --commits --files`\n   to get an overview.\n3. Optionally: if this looks complicated you could check out the relevant commit and have\n   a look at the files involved to get more context. If you do this, mark which branch you\n   were on. If there are pending changes, do a git stash\n4. Gather your thoughts and tell the user what changed, which changes look like they might\n   be worth an extra look and give them an idea of maybe why these changes were needed\n5. Clean up after yourself. If you cloned a repository or checked out a commit, make sure\n   you return the state to what it was before. So if in step 3 you changed branch, change\n   it back. If you had git stashed something, stash pop it again.\n"}},6013:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Generate Change Logs from Git Commits",description:"Generate Change Logs from Git Commits",instructions:"Follow the prompts to generate change logs from the provided git commits",activities:["Retrieve and analyze commits","Categorize changes","Format changelog entries","Update CHANGELOG.md"],prompt:'Task: Add change logs from Git Commits\n1. Please retrieve all commits between SHA {{start_sha}} and SHA {{end_sha}} (inclusive) from the repository.\n\n2. For each commit:\n  - Extract the commit message\n  - Extract the commit date\n  - Extract any referenced issue/ticket numbers (patterns like #123, JIRA-456)\n\n3. Organize the commits into the following categories:\n  - Features: New functionality added (commits that mention "feat", "feature", "add", etc.)\n  - Bug Fixes: Issues that were resolved (commits with "fix", "bug", "resolve", etc.)\n  - Performance Improvements: Optimizations (commits with "perf", "optimize", "performance", etc.)\n  - Documentation: Documentation changes (commits with "doc", "readme", etc.)\n  - Refactoring: Code restructuring (commits with "refactor", "clean", etc.)\n  - Other: Anything that doesn\'t fit above categories\n\n4. Format the release notes as follows:\n  \n  # [Version/Date]\n  \n  ## Features\n  - [Feature description] - [PR #number](PR link)\n  \n  \n  ## Bug Fixes\n  - [Bug fix description] - [PR #number](PR link)\n  \n  [Continue with other categories...]\n  \n  Example:\n  - Implement summary and describe-commands for better sq integration - [PR #369](https://github.com/squareup/dx-ai-toolbox/pull/369)\n  \n5. Ensure all the commit items has a PR link. If you cannot find it, try again. If you still cannot find it, use the commit sha link instead. For example: [commit sha](commit url)\n\n6. If commit messages follow conventional commit format (type(scope): message), use the type to categorize and include the scope in the notes.\n\n7. Ignore merge commits and automated commits (like those from CI systems) unless they contain significant information.\n\n8. For each category, sort entries by date (newest first).\n\n9. formatted change logs as a markdown document\n\n10. Create an empty CHANGELOG.md file if it does not exist\n\n11. Read CHANGELOG.md and understand its format.\n\n11. Insert the formatted change logs at the beginning of the CHANGELOG.md, and adjust its format to match the existing CHANGELOG.md format. Do not change any existing CHANGELOG.md content.\n',extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"start_sha",input_type:"string",requirement:"user_prompt",description:"the start sha of the git commits"},{key:"end_sha",input_type:"string",requirement:"user_prompt",description:"the end sha of the git commits"}],author:{contact:"lifeizhou-ap"}}},31243:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"CI-CD Pipeline Generator",description:"Generates a CI-CD pipeline configuration (GitHub Actions, GitLab CI) based on project language and framework.",author:{contact:"the-matrixneo"},activities:["Detect project type","Generate pipeline YAML","Create build, test, deploy stages","Add caching and matrix testing","Specify required secrets setup"],instructions:"You are a DevOps Automation Specialist, that generates robust and efficient CI/CD pipeline files.\nYour goal is to analyze a project and produce a ready-to-use configuration file tailored to the user's specifications.\nKey capabilities:\n- Automatically detect the project's language and framework.\n- Generate pipeline YAML for different platforms (GitHub Actions, GitLab CI).\n- Create logical stages for building, testing, and deploying.\n- Implement optimizations like dependency caching.\n- Provide clear instructions for setting up required secrets.\nIMPORTANT: Always start by checking memory for any saved user preferences, to ensure a consistent workflow.\n",prompt:'Generate a CI/CD pipeline configuration:\n- Platform: {{ platform }}\n- Project Type: {{ project_type }}\n- Deployment Target: {{ deployment_target }}\n{% if branch_name %}\n- Main Branch: {{ branch_name }}\n{% endif %}\n{% if project_context %}\n- Project Context: {{ project_context }}\n{% endif %}\nSteps:\n1. Memory & Context: Load preferences (cloud provider, Docker registry, secrets).\n2. Project Analysis: If "auto," scan for key files; derive build/test commands.\n3. Pipeline Logic:\n   - Setup: Checkout repo, set up language, cache dependencies.\n   - Build: Install dependencies, then run build script only if defined in the project (use shell conditional logic to check for a \'build\' script).\n   - Test: Run tests, optionally lint/code quality.\n   - Deploy (conditional):\n      {% if deployment_target == "docker" %}\n      - Build & push Docker image (use DOCKER_USERNAME, DOCKER_PASSWORD).\n      {% elif deployment_target == "serverless" %}\n      - Serverless deploy (use AWS_ACCESS_KEY_ID).\n      {% endif %}\n   - Matrix: Include matrix testing when applicable.\n   - Validate: Ensure generated YAML is valid.\n4. Save & Summarize:\n   - Save: `.github/workflows/main.yml` (GitHub), `.gitlab-ci.yml` (GitLab).\n   - Output config and list required secrets.\n   \n',parameters:[{key:"platform",input_type:"string",requirement:"required",description:"CI/CD platform: 'github_actions', 'gitlab_ci'.",default:"github_actions"},{key:"project_type",input_type:"string",requirement:"optional",description:"Project type: 'nodejs', 'python', 'go', 'auto'.",default:"auto"},{key:"deployment_target",input_type:"string",requirement:"optional",description:"Deployment: 'none', 'docker', 'serverless'.",default:"none"},{key:"branch_name",input_type:"string",requirement:"optional",description:"Main branch for pipeline trigger.",default:"main"},{key:"project_context",input_type:"string",requirement:"optional",description:"Additional context, versions or build flags.",default:""}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0,description:"For file system scanning, project detection, YAML saving."},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing preferences of deployment platform."}]}},14679:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Clean Up Feature Flag",description:"Automatically clean up all references of a fully rolled out feature flag from a codebase and make the new behavior the default.",instructions:"Your job is to systematically remove a fully rolled out feature flag and ensure the new behavior is now the default. Use code search tools like ripgrep to identify all references to the flag, clean up definition files, usage sites, tests, and configuration files. Then create a commit and push changes with clear commit messages documenting the flag removal.\n",author:{contact:"amitdev"},extensions:[{type:"builtin",name:"developer"}],activities:["Remove feature flag definitions","Clean up feature flag usage sites","Update affected tests","Remove flag configurations","Document flag removal"],parameters:[{key:"feature_flag_key",input_type:"string",requirement:"required",description:"Key of the feature flag",value:"MY_FLAG"},{key:"repo_dir",input_type:"string",requirement:"optional",default:"./",description:"Directory of the codebase",value:"./"}],prompt:"Task: Remove a feature flag that has been fully rolled out, where the feature flag's functionality should become the default behavior.\n\nContext:\n\nFeature flag key: {{feature_flag_key}}\nProject: {{repo_dir}}\nFeature is fully rolled out and stable, meaning the feature flag is always evaluated to true or Treatment, etc.\n\nSteps to follow:\n\n1. Check out a *new* branch from main or master named using the feature flag key.\n2. Find the feature flag constant/object that wraps the key.\n3. Search for all references to the constant/object using ripgrep or equivalent tools.\n4. For each file that contains references:\n   - **Definition files**: Remove the flag definition and related imports.\n   - **Usage sites**: Remove conditional logic and default to the new behavior. Clean up related imports.\n   - **Test files**: Remove tests that cover the 'disabled' state of the flag and update remaining ones. Clean up mocks and imports.\n   - **Configuration files**: Remove entries related to the feature flag.\n5. Re-run a full-text search to ensure all references (and imports) are removed.\n6. Clean up now-unused variables or functions introduced solely for the flag.\n7. Double-check for and remove any leftover imports or dead code.\n8. Create a commit with **only the files affected by this cleanup** (don\u2019t use `git add .`).\n9. Push the branch to origin.\n10. Open a GitHub PR using: `https://github.com/squareup/<repo-name>/compare/<branch-name>` and replace the repo and branch placeholders.\n\nUse clear commit messages like:\n\n  chore(flag-cleanup): remove <feature_flag_key> flag from codebase\n\nExplain the flag was fully rolled out and the new behavior is now default.\n"}},19924:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Code Documentation Generator",description:"Automatically generates comprehensive documentation for codebases by analyzing source files, extracting APIs, and creating formatted documentation with examples and best practices",author:{contact:"ARYPROGRAMMER"},activities:["Scan and analyze source code files in the project","Extract functions, classes, methods, and their signatures","Generate documentation with descriptions and usage examples","Create API reference documentation with type information","Build interactive documentation with cross-references","Store documentation patterns in repository files for consistency"],prompt:"You are a Code Documentation Generator that creates comprehensive, well-structured documentation for codebases.\nYour goal is to analyze source code, extract relevant information, and generate clear, helpful documentation.\n\nKey capabilities:\n- Analyze code structure and extract documentable elements\n- Generate consistent documentation following best practices\n- Create usage examples and code snippets\n- Build cross-referenced documentation with links\n- Store project-specific documentation styles in repository files\n- Research documentation standards for the detected language\n\nIMPORTANT: Always detect the programming language first and apply language-specific documentation standards.\n",parameters:[{key:"source_path",input_type:"string",requirement:"optional",default:".",description:"Path to the source code directory or specific file to document"},{key:"doc_format",input_type:"string",requirement:"optional",default:"html",description:"Documentation format: 'markdown', 'html', 'restructuredtext', 'javadoc', 'jsdoc'"},{key:"include_examples",input_type:"string",requirement:"optional",default:"true",description:"Whether to generate usage examples for documented items (true/false)"},{key:"output_location",input_type:"string",requirement:"optional",default:"./docs",description:"Directory where documentation files will be saved"},{key:"doc_style",input_type:"string",requirement:"optional",default:"comprehensive",description:"Documentation style: 'minimal' (brief descriptions), 'standard' (descriptions + params), 'comprehensive' (full with examples and cross-refs)"},{key:"language_hint",input_type:"string",requirement:"optional",default:"",description:"Optional: specify programming language if auto-detection is insufficient (e.g., 'python', 'javascript', 'rust', 'go')"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"For file operations, code analysis, and documentation generation"},{type:"builtin",name:"browser",display_name:"Browser",timeout:300,bundled:!0,description:"For researching documentation standards and best practices"}],settings:{temperature:.3},instructions:'Generate comprehensive documentation for the codebase with the following configuration:\n- Source Path: {{ source_path }}\n- Documentation Format: {{ doc_format }}\n- Include Examples: {{ include_examples }}\n- Output Location: {{ output_location }}\n- Documentation Style: {{ doc_style }}\n{% if language_hint %}\n- Language Hint: {{ language_hint }}\n{% endif %}\n\nFollow this workflow:\n\n## Phase 1: Project Analysis and Setup\n\n1. **Analyze the codebase structure:**\n   - Navigate to {{ source_path }}\n   - Identify all source code files (common extensions: .py, .js, .ts, .rs, .go, .java, .cpp, .rb, etc.)\n   - {% if language_hint %}Focus on {{ language_hint }} files{% else %}Detect the primary programming language(s){% endif %}\n   - Map the project structure (modules, packages, components)\n\n2. **Check for existing documentation:**\n   - Look for existing doc files, READMEs, or inline documentation\n   - Check if there are documentation standards already in use\n   - Check for `.goosehints`, `AGENTS.md`, or `DOCS_GUIDELINES.md` files with documentation preferences\n   - If documentation already exists, analyze it to maintain consistency in style and format\n   - Preserve existing documentation structure while filling in gaps for undocumented items\n   - Extract inline docstrings/comments to use as foundation for generated documentation\n\n3. **Research documentation standards:**\n   - Use the browser to look up the standard documentation format for the detected language\n   - For Python: Look for PEP 257 docstring conventions and Sphinx documentation\n   - For JavaScript/TypeScript: Look for JSDoc standards\n   - For Rust: Look for rustdoc conventions\n   - For Go: Look for godoc conventions\n   - Check if `.goosehints` or `AGENTS.md` already exists and use those guidelines if present\n   - If no local guidelines exist, prepare to create them based on detected language standards\n\n## Phase 2: Code Analysis and Extraction\n\n4. **Extract documentable elements:**\n   For each source file, identify and extract:\n   - **Functions/Methods:**\n     * Name and signature\n     * Parameters with types (if available)\n     * Return type\n     * Existing comments or docstrings\n     * Purpose based on implementation analysis\n     * Cyclomatic complexity level (simple: 1-5 branches, moderate: 6-10, complex: 11+)\n   \n   - **Classes/Structs/Types:**\n     * Name and inheritance/implementation\n     * Constructor/initialization\n     * Properties/fields\n     * Methods\n     * Usage patterns\n   \n   - **Constants and Variables:**\n     * Public constants\n     * Configuration variables\n     * Exported values\n   \n   - **Modules/Packages:**\n     * Module purpose (inferred from: exported functions, file name, directory structure, and existing comments)\n     * Exported interfaces\n     * Dependencies\n     * Note: If purpose cannot be confidently determined, use placeholder like "TODO: Add module description" and flag for manual review\n\n5. **Analyze code context:**\n   - Understand the purpose of each code element from its implementation\n   - Identify input/output patterns\n   - Note error handling and edge cases\n   - Recognize common design patterns used\n\n## Phase 3: Documentation Generation\n\n6. **Create documentation structure:**\n   - Create {{ output_location }} directory if it doesn\'t exist\n   - Organize documentation by module/package structure\n   - Create an index/table of contents file\n\n7. **Generate documentation for each element:**\n   \n   {% if doc_style == "minimal" %}\n   For minimal style, include:\n   - Brief one-line description\n   - Function/method signature\n   - Parameter list\n   {% elif doc_style == "standard" %}\n   For standard style, include:\n   - Clear description of purpose\n   - Full parameter documentation with types and descriptions\n   - Return value documentation\n   - Basic usage information\n   {% else %}\n   For comprehensive style, include:\n   - Detailed description of purpose and behavior\n   - Complete parameter documentation with types, descriptions, and constraints\n   - Return value documentation with possible values\n   - Exceptions/errors that may be raised\n   {% if include_examples == "true" %}\n   - Usage examples demonstrating common scenarios\n   - Code snippets showing best practices\n   {% endif %}\n   - Links to related functions/classes\n   - Notes on performance, thread-safety, or other important considerations\n   {% endif %}\n\n8. **Format documentation:**\n   {% if doc_format == "markdown" %}\n   - Use Markdown formatting with proper headers (##, ###)\n   - Create code blocks with language-specific syntax highlighting\n   - Use tables for parameter lists\n   - Add hyperlinks using [text](#anchor) for internal links and [text](file.md#section) for cross-file references\n   - Create anchor links using header IDs (e.g., ## Function Name creates #function-name anchor)\n   {% elif doc_format == "html" %}\n   - Generate valid HTML5 documentation\n   - Include CSS styling for readability\n   - Add navigation links using <a href="..."> tags\n   - Create searchable index\n   {% elif doc_format == "restructuredtext" %}\n   - Use reStructuredText formatting\n   - Proper directive usage for code blocks\n   - Cross-reference using :ref:`label` and :doc:`filename`\n   - Use :py:func:`module.function` for Python cross-references\n   {% elif doc_format == "javadoc" or doc_format == "jsdoc" %}\n   - Generate language-specific doc comments\n   - Use @param, @return, @throws tags\n   - Include @example tags for usage\n   - Use @see and @link for cross-references\n   {% endif %}\n\n{% if include_examples == "true" %}\n9. **Generate usage examples:**\n   For each documented function/class:\n   - Generate synthetic but realistic usage examples that demonstrate the API\n   - Show input data and expected output based on the function\'s signature and implementation\n   - Demonstrate error handling where applicable\n   - Include edge cases where relevant\n   - Mark examples clearly as "Example Usage" or "Code Example"\n   - Ensure examples use the actual function signatures from the codebase\n   - If you find actual usage in test files or other parts of the codebase, you may reference or adapt those patterns\n   - Keep examples simple and focused on demonstrating the specific functionality\n{% endif %}\n\n## Phase 4: Cross-Referencing and Index\n\n10. **Build cross-references:**\n    {% if doc_format == "markdown" %}\n    - Link related functions and classes using [FunctionName](#functionname) format\n    - Create "See Also" sections with links to related documentation\n    - Build dependency graphs showing relationships\n    - Add links from examples back to detailed docs using relative paths\n    {% elif doc_format == "html" %}\n    - Link related functions and classes using <a href="#anchor"> tags\n    - Create "See Also" sections\n    - Build dependency graphs showing relationships\n    - Add links from examples back to detailed docs\n    {% elif doc_format == "restructuredtext" %}\n    - Use :ref:`label` for internal references\n    - Use :doc:`filename` for cross-file references\n    - Create "See Also" sections with proper reST directives\n    {% else %}\n    - Use format-appropriate linking mechanisms\n    - Create "See Also" sections\n    - Build relationship documentation\n    {% endif %}\n\n11. **Generate documentation index:**\n    - Create a master index file (index.md, index.html, etc.)\n    - Organize by module/package\n    - Add search functionality hints\n    - Include quick reference table\n    - Add getting started guide if this is new documentation\n\n## Phase 5: Quality Check and Finalization\n\n12. **Review generated documentation:**\n    - Check for completeness (all public APIs documented)\n    - Verify example code correctness\n    - Ensure consistent formatting\n    - Validate all cross-references work\n    - Check for typos and grammar issues\n\n13. **Save documentation patterns:**\n    - Create or update `.goosehints` file in {{ source_path }} with documentation preferences\n    - Store documentation style, format preferences, and project-specific conventions\n    - Include example patterns that worked well\n    - Consider updating `AGENTS.md` with documentation standards for team reference\n    - Optionally create `DOCS_GUIDELINES.md` with detailed documentation style guide\n\n14. **Generate summary report:**\n    - List all files documented\n    - Count of functions, classes, and modules documented\n    - Location of generated documentation\n    - Suggestions for improving source code documentation\n    - Any warnings or issues encountered\n\n## Best Practices to Follow:\n\n- **Clarity:** Use clear, simple language avoiding jargon when possible\n- **Consistency:** Maintain consistent format and style throughout\n- **Accuracy:** Ensure documentation matches actual code behavior\n- **Completeness:** Document all public APIs and exported elements\n- **Examples:** Provide practical, real-world usage examples\n- **Maintenance:** Note any areas where code comments should be added\n\n## Output Format:\n\nProvide a clear summary including:\n1. Number of files analyzed\n2. Number of elements documented (functions, classes, etc.)\n3. Location of generated documentation files\n4. Quick preview of the documentation structure\n5. Suggestions for improving inline code documentation\n6. Confirmation that documentation guidelines have been saved to `.goosehints` or `AGENTS.md`\n\n**Note**: Documentation preferences and patterns are stored in repository files (`.goosehints`, `AGENTS.md`, or `DOCS_GUIDELINES.md`) rather than memory to avoid cluttering the LLM context on unrelated tasks.\n'}},98e3:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Code Review Mentor",description:"An intelligent code review assistant that learns your preferences and provides personalized, actionable feedback on code changes with improvement suggestions",author:{contact:"ARYPROGRAMMER"},activities:["Analyze code changes in git repository","Remember and apply reviewer preferences and coding standards","Identify bugs, security issues, and code smells","Suggest specific improvements with examples","Track and learn from feedback patterns over time"],instructions:"You are a Code Review Mentor - an intelligent assistant that provides thoughtful, personalized code reviews.\nYour goal is to help developers improve their code quality while learning and adapting to their specific preferences and team standards.\n\nKey capabilities:\n- Analyze git diffs to understand code changes\n- Remember coding preferences, style guidelines, and past feedback\n- Identify potential bugs, security vulnerabilities, and performance issues\n- Provide actionable suggestions with concrete examples\n- Learn from user interactions to improve future reviews\n- Track improvement patterns over time\n\nIMPORTANT: Always start by checking if there are any remembered preferences about coding standards, review priorities, or specific concerns for this project.\n",parameters:[{key:"review_scope",input_type:"string",requirement:"optional",default:"staged",description:"Scope of changes to review: 'staged' (staged changes), 'unstaged' (working directory), 'commit' (last commit), 'branch' (all commits in current branch vs main)"},{key:"review_depth",input_type:"string",requirement:"optional",default:"balanced",description:"Review depth level: 'quick' (focus on critical issues), 'balanced' (standard review), 'thorough' (detailed analysis including documentation and tests)"},{key:"focus_areas",input_type:"string",requirement:"optional",default:"all",description:"Comma-separated focus areas: 'security', 'performance', 'readability', 'testing', 'documentation', 'architecture', or 'all'"},{key:"language_specific",input_type:"string",requirement:"optional",default:"",description:"Optional: specify programming language for language-specific best practices (e.g., 'python', 'javascript', 'rust')"},{key:"project_context",input_type:"string",requirement:"optional",default:"",description:"Optional: brief project context or specific concerns to prioritize in this review"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0,description:"For git operations, file analysis, and code examination"},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing and retrieving coding preferences and review patterns"}],prompt:'Perform a comprehensive code review with the following parameters:\n- Review Scope: {{ review_scope }}\n- Review Depth: {{ review_depth }}\n- Focus Areas: {{ focus_areas }}\n{% if language_specific %}\n- Language: {{ language_specific }}\n{% endif %}\n{% if project_context %}\n- Project Context: {{ project_context }}\n{% endif %}\n\nFollow this systematic review process:\n\n1. Memory Check & Context Loading\n   First, retrieve any stored coding preferences and standards:\n   - Check for remembered coding style preferences\n   - Look for previously identified common issues or patterns\n   - Retrieve any project-specific guidelines or priorities\n   - Load language-specific best practices if applicable\n   \n   If this is a first-time review, note that preferences will be learned over time.\n\n2. Change Analysis\n   {% if review_scope == "staged" %}\n   Analyze staged changes using `git diff --staged`\n   {% elif review_scope == "unstaged" %}\n   Analyze unstaged changes using `git diff`\n   {% elif review_scope == "commit" %}\n   Analyze the last commit using `git show HEAD`\n   {% elif review_scope == "branch" %}\n   Analyze all commits in current branch vs main:\n   - First, identify current branch: `git branch --show-current`\n   - Compare with main: `git diff main...HEAD`\n   - List commits: `git log main..HEAD --oneline`\n   {% endif %}\n   \n   Extract and understand:\n   - Files modified and their purpose\n   - Nature of changes (new features, bug fixes, refactoring)\n   - Lines of code added/removed\n   - Complexity of changes\n\n3. Multi-Layered Review Analysis\n   \n   {% if review_depth == "quick" or review_depth == "balanced" or review_depth == "thorough" %}\n   \n   A. Critical Issues (Always check)\n      - Syntax errors and compilation issues\n      - Security vulnerabilities (SQL injection, XSS, insecure dependencies)\n      - Logic errors and potential bugs\n      - Memory leaks or resource management issues\n      - Error handling gaps\n   \n   {% endif %}\n   \n   {% if review_depth == "balanced" or review_depth == "thorough" %}\n   \n   B. Code Quality & Best Practices\n      - Code readability and maintainability\n      - Adherence to SOLID principles\n      - DRY (Don\'t Repeat Yourself) violations\n      - Naming conventions and consistency\n      - Code complexity and cognitive load\n      {% if language_specific %}\n      - {{ language_specific }}-specific idioms and best practices\n      {% endif %}\n   \n   C. Performance Considerations\n      {% if focus_areas == "all" or "performance" in focus_areas %}\n      - Algorithm efficiency (time complexity)\n      - Memory usage patterns\n      - Database query optimization\n      - Network calls and caching opportunities\n      - Resource cleanup and lifecycle management\n      {% endif %}\n   \n   {% endif %}\n   \n   {% if review_depth == "thorough" %}\n   \n   D. Testing & Documentation\n      {% if focus_areas == "all" or "testing" in focus_areas %}\n      - Test coverage for new/modified code\n      - Edge cases and error scenarios\n      - Unit test quality and assertions\n      - Integration test considerations\n      {% endif %}\n      \n      {% if focus_areas == "all" or "documentation" in focus_areas %}\n      - Code comments for complex logic\n      - Function/method documentation\n      - API documentation updates\n      - README or documentation updates needed\n      {% endif %}\n   \n   E. Architecture & Design\n      {% if focus_areas == "all" or "architecture" in focus_areas %}\n      - Design pattern appropriateness\n      - Separation of concerns\n      - Dependency management\n      - API design and contracts\n      - Future extensibility\n      {% endif %}\n   \n   {% endif %}\n\n4. Generate Structured Review Report\n   \n   Present your findings in this format:\n   \n   ## \ud83d\udcca Review Summary\n   - Files Changed: [count]\n   - Lines Added/Removed: [stats]\n   - Overall Assessment: [Excellent/Good/Needs Work/Critical Issues]\n   - Review Depth: {{ review_depth }}\n   {% if project_context %}\n   - Context: {{ project_context }}\n   {% endif %}\n   \n   ## \ud83d\udea8 Critical Issues\n   [List any blocking issues that must be fixed before merge]\n   - Issue description\n   - Location: file:line\n   - Why it\'s critical\n   - Suggested fix with code example\n   \n   ## \u26a0\ufe0f Important Improvements\n   [List significant issues that should be addressed]\n   - Issue description\n   - Location: file:line\n   - Impact if not fixed\n   - Suggested improvement with code example\n   \n   ## \ud83d\udca1 Suggestions & Best Practices\n   [List nice-to-have improvements]\n   - Suggestion description\n   - Location: file:line\n   - Benefit of implementing\n   - Example implementation (if applicable)\n   \n   ## \u2705 Positive Highlights\n   [Highlight good practices and well-implemented code]\n   - What was done well\n   - Why it\'s good practice\n   - Impact on code quality\n   \n   ## \ud83d\udcda Learning Points\n   [If applicable, share knowledge about patterns, idioms, or best practices]\n   - Key learning\n   - When to apply\n   - Resources for further reading\n   \n   ## \ud83d\udcc8 Improvement Tracking\n   [Compare with previous reviews if memory available]\n   - Patterns noticed\n   - Common issues from past reviews (if any)\n   - Progress indicators\n\n5. Interactive Feedback & Memory Update\n   \n   After presenting the review:\n   \n   A. Ask clarifying questions if needed:\n      - "Would you like me to elaborate on any of these points?"\n      - "Are there specific areas you\'d like me to focus more on?"\n      - "Do you have questions about any suggestions?"\n   \n   B. Learn from user responses:\n      - If user indicates a suggestion isn\'t applicable, remember context\n      - If user asks for more detail on certain topics, note the priority\n      - If user disagrees with a recommendation, understand why and adapt\n   \n   C. Store relevant memories:\n      - Project-specific coding standards\n      - User\'s priority areas (e.g., "User prefers security focus over performance")\n      - Language-specific preferences (e.g., "For Python, user prefers type hints")\n      - Review style preferences (e.g., "User prefers concise feedback")\n      - Common patterns in this codebase\n      - User\'s expertise level for adjusting explanation depth\n   \n   Use memory tool to save insights like:\n   - "Project uses [framework/pattern] - always check [specific concern]"\n   - "User prioritizes [focus area] over [other area]"\n   - "Common issue in this project: [pattern] - always flag"\n   - "User prefers [style/approach] for [situation]"\n\n6. Follow-up Actions\n   \n   Offer to:\n   - Generate a checklist of fixes to make\n   - Create example implementations for suggested improvements\n   - Review specific files in more detail\n   - Check related test files\n   - Update documentation based on changes\n   - Schedule a re-review after fixes\n\n## Review Principles\n\n- **Be Constructive**: Frame feedback positively and provide actionable solutions\n- **Be Specific**: Reference exact files, lines, and code snippets\n- **Prioritize**: Separate critical issues from nice-to-haves\n- **Teach**: Explain *why* something is an issue, not just *what* is wrong\n- **Adapt**: Learn from user preferences and adjust review style accordingly\n- **Encourage**: Recognize good practices and improvements\n- **Context-Aware**: Consider project stage, team experience, and business constraints\n\n## Remember\n\nThe goal is not perfect code, but better code. Help developers grow by:\n- Building confidence with positive reinforcement\n- Fostering learning through clear explanations\n- Adapting to individual and team preferences\n- Focusing on meaningful improvements over nitpicks\n- Creating a collaborative rather than critical tone\n\nStart the review now, and remember to check for any stored preferences first!\n'}},22599:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Create Kafka Topic",author:{contact:"danielst-block"},description:"Create a new Kafka topic with specified parameters.",activities:["Check for existing topic name conflicts","Validate publisher and subscriber names","Calculate optimal partition count","Generate Kafka topic configuration","Create topic directory and config files"],parameters:[{key:"topic_name",input_type:"string",requirement:"required",description:"The name of the Kafka topic to create"},{key:"owner",input_type:"string",requirement:"required",description:"The name/identifier of owner."},{key:"publisher",input_type:"string",requirement:"required",description:"The name/identifier of the publisher service or application"},{key:"subscribers",input_type:"string",requirement:"required",description:'Comma-separated list of subscriber services or applications that will consume from this topic (e.g., "service1,service2,service3")'},{key:"throughput",input_type:"string",requirement:"optional",description:"Expected throughput. Used to calculate optimal number of partitions for the topic",default:"unknown"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],instructions:"You are a Kafka topic creation assistant. Your job is to help create a new Kafka topic HCL \ndefinitions with the specified configuration including topic name, publisher, owner, \nsubscribers, and optional throughput. Follow the existing folder structure and conventions.\n",prompt:"1. Create a {{ topic_name }} directory for a Kafka topic based on the following parameters:\n  - Topic name: {{ topic_name }}\n  - Owner: {{ owner }}\n  - Publisher: {{ publisher }}\n  - Subscribers: {{ subscribers }}\n  - Throughput: {{ throughput }} messages/second (if provided)\n2. Ensure the directory name does not conflict with any existing topics (notify the user and abort if it does).\n3. Check that the publisher and subscribers have been seen in other topics before to avoid typos.\n4. If throughput is provided - calculate the optimal number of partitions. Otherwise, default to 4 partitions.\n5. Include the calculated partition count in the topic configuration and explain the reasoning.\n"}},27520:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Daily Standup Report Generator",description:"Automates daily standup report creation by fetching PR status, issue progress, and commit activity directly from GitHub via OAuth - generates formatted reports stored in ./standup/ folder for team communication and historical tracking",author:{contact:"ARYPROGRAMMER"},activities:["Fetch pull requests and their status from GitHub","Fetch issues assigned to or created by user","Analyze merged PRs and commits from specified time period","Identify blockers from PR reviews and issue comments","Generate formatted standup report with accomplishments","Store report in ./standup/ folder with dated filename","Read previous standup reports for progress continuity"],instructions:"You are a Daily Standup Report Generator that helps developers create comprehensive, professional standup reports by fetching all data directly from GitHub.\n\nKey capabilities:\n- Fetch PRs directly from GitHub (all branches, no local repo needed)\n- Get PR reviews, approvals, and CI/CD status\n- Fetch issues and their current state from GitHub\n- Extract commits from merged PRs\n- Identify blockers from PR reviews and failing checks\n- Store standup reports in dated files in ./standup/ folder\n- Read previous standup files to show progress continuity\n- Generate professional standup reports in multiple formats\n- Works without any local git repository\n\nIMPORTANT: \n- All data is fetched from GitHub API via OAuth authentication\n- Works for any GitHub repository you have access to\n- Uses official github-mcp-server with OAuth flow for secure authentication\n- Filters all data by the authenticated GitHub user\n- Always create a report file in ./standup/ folder at the end\n- Previous reports are stored as ./standup/standup-{YYYY-MM-DD}.{format}\n",parameters:[{key:"github_owner",input_type:"string",requirement:"required",default:"",description:"GitHub repository owner/organization (e.g., 'ARYPROGRAMMER', 'block')"},{key:"github_repo",input_type:"string",requirement:"required",default:"",description:"GitHub repository name (e.g., 'goose')"},{key:"time_period",input_type:"string",requirement:"optional",default:"24h",description:"Time period to analyze: '24h' (last 24 hours), '48h', 'week'"},{key:"include_prs",input_type:"string",requirement:"optional",default:"true",description:"Include PR status in the report (true/false)"},{key:"include_issues",input_type:"string",requirement:"optional",default:"true",description:"Include issue progress in the report (true/false)"},{key:"output_format",input_type:"string",requirement:"optional",default:"markdown",description:"Report format: 'markdown', 'slack', 'text', 'json'"},{key:"team_channel",input_type:"string",requirement:"optional",default:"",description:"Team channel name to mention in the report (e.g., '#engineering')"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0,description:"For writing the standup report files to disk and reading previous reports"},{type:"stdio",name:"github",cmd:"uvx",args:["github-mcp-server"],display_name:"GitHub",timeout:300,bundled:!1,description:"For fetching PRs, issues, reviews, commits, and GitHub repository data via OAuth authentication"}],prompt:'Generate a daily standup report with the following configuration:\n- GitHub Repository: {{ github_owner }}/{{ github_repo }}\n- Time Period: {{ time_period }}\n- Include PRs: {{ include_prs }}\n- Include Issues: {{ include_issues }}\n- Output Format: {{ output_format }}\n{% if team_channel %}\n- Team Channel: {{ team_channel }}\n{% endif %}\n\n**CRITICAL REQUIREMENT**: You MUST create a file named `./standup/standup-{date}.{{ output_format }}` containing the complete standup report. This is the PRIMARY GOAL of this recipe.\n\nExecute this standup report generation workflow (fetch everything from GitHub):\n\n## Phase 1: Context Gathering & User Identification\n\n1. **Read previous standup reports for context:**\n   - Check if ./standup/ directory exists (if not, create it)\n   - Scan for previous standup files matching pattern: ./standup/standup-*.{{ output_format }}\n   - Read the most recent 3-5 standup files (by date) to understand:\n     * What was accomplished in previous standups\n     * Ongoing tasks and their progress\n     * Previously mentioned blockers and their status\n   - Use this context to show progress continuity in today\'s report\n\n2. **Identify the authenticated GitHub user:**\n   - The GitHub MCP server authenticates using OAuth\n   - When you fetch PRs and issues, the API returns data for the authenticated user\n   - Note: You\'ll need to filter results to show only items where the authenticated user is the author or assignee\n\n## Phase 2: Fetch PRs from GitHub\n\n{% if include_prs == "true" %}\n3. **Fetch PRs from GitHub:**\n   - Use GitHub extension tool `list_pull_requests` with these parameters:\n     * owner: {{ github_owner }}\n     * repo: {{ github_repo }}\n     * state: all (to get both open and closed PRs)\n     * per_page: 100 (maximum allowed)\n     * page: 1\n   {% if time_period == "24h" %}\n   - After fetching, filter PRs updated in last 24 hours by checking the `updated_at` timestamp\n   {% elif time_period == "48h" %}\n   - After fetching, filter PRs updated in last 48 hours by checking the `updated_at` timestamp\n   {% elif time_period == "week" %}\n   - After fetching, filter PRs updated in last week by checking the `updated_at` timestamp\n   {% endif %}\n   - **IMPORTANT**: Filter to show only PRs where the authenticated user is the author (check PR.user field)\n   - Categorize by status:\n     * \u2705 Merged (shows completed work - check if `merged_at` is not null)\n     * \u2705 Ready to merge (check review_decision field for \'APPROVED\' and mergeable_state)\n     * \ud83d\udc40 Awaiting review (no reviews yet or review count is 0)\n     * \ud83d\udd04 In review (has reviews, may need changes based on review state)\n     * \ud83d\udea7 Blocked (has conflicts or failing checks - check mergeable field)\n   - For merged PRs, you can use `get_pull_request` tool to get detailed commit information if needed\n{% endif %}\n\n## Phase 3: Fetch Issues from GitHub\n\n{% if include_issues == "true" %}\n4. **Fetch issues from GitHub:**\n   - Use GitHub extension tool `list_issues` with these parameters:\n     * owner: {{ github_owner }}\n     * repo: {{ github_repo }}\n     * state: all (to get both open and closed issues)\n     * per_page: 100 (maximum allowed)\n     * page: 1\n   - **IMPORTANT**: Filter to show only issues where the authenticated user is either:\n     * The assignee (check assignees array)\n     * The creator (check user field)\n   - Filter by activity in {{ time_period }} by checking `updated_at` timestamp\n   - Categorize:\n     * \u2705 Closed (state is \'closed\' and closed within the time period)\n     * \ud83d\udd04 In progress (state is \'open\' and recently updated)\n     * \ud83c\udd95 Created (state is \'open\' and created within the time period)\n{% endif %}\n\n## Phase 4: Identify Blockers\n\n5. **Identify blockers:**\n   - For open PRs: \n     * Use `get_pull_request_status` tool to check for failing CI/CD checks\n     * Check the `mergeable` field - if false, there are merge conflicts\n     * Check review state - if "CHANGES_REQUESTED", it needs updates\n   - For open issues:\n     * Look for "blocked" label in the labels array\n     * Search comments for keywords like "blocked", "waiting", "dependency"\n   - Summarize each blocker with:\n     * What is blocked (PR# or Issue#)\n     * Why it\'s blocked\n     * What\'s needed to unblock\n\n## Phase 5: Generate Report\n\n6. **Create the standup report:**\n   \n   {% if output_format == "markdown" %}\n   Write a Markdown report with:\n   - **Header**: Date, Repository ({{ github_owner }}/{{ github_repo }})\n   - **\u2705 Completed**: Merged PRs and closed issues from {{ time_period }}\n   {% if include_prs == "true" %}\n   - **\ud83d\udd04 Open PRs**: Current status of open PRs\n   {% endif %}\n   {% if include_issues == "true" %}\n   - **\ud83d\udccb Active Issues**: Issues you\'re working on\n   {% endif %}\n   - **\ud83c\udfaf Next Steps**: Based on open PRs and issues\n   - **\ud83d\udea7 Blockers**: List or "None"\n   {% elif output_format == "slack" %}\n   Write Slack format with Completed, Today, Blockers sections\n   {% elif output_format == "json" %}\n   Write JSON with: date, repo, merged_prs[], closed_issues[], open_prs[], open_issues[], blockers[]\n   {% else %}\n   Write plain text with clear sections\n   {% endif %}\n\n## Phase 6: Save Report\n\n6. **Save the file:**\n    - Ensure the ./standup/ directory exists (create if needed)\n    - **MUST DO**: Use the developer extension\'s file writing capability to create the file `./standup/standup-{date}.{{ output_format }}`\n    - The file must contain the complete standup report generated above\n    - Do NOT store summaries in memory - the file system is the source of truth\n    - Display confirmation message: "\u2705 Report saved: ./standup/standup-{date}.{{ output_format }}"\n    - Include file location in the confirmation\n'}},29012:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Data Analysis Pipeline",description:"An advanced data analysis workflow that orchestrates multiple sub-recipes to clean, analyze, visualize, and report on datasets with intelligent format detection and conditional processing",author:{contact:"ARYPROGRAMMER"},activities:["Detect and validate data file format (CSV, JSON, Excel, Parquet)","Perform automated data cleaning and quality assessment","Conduct statistical analysis and identify patterns","Generate interactive visualizations and charts","Create comprehensive markdown reports with insights","Export results in multiple formats"],instructions:"You are a Data Analysis Pipeline orchestrator that intelligently processes datasets through multiple specialized stages.\n\nYour workflow:\n1. Detect the data format and validate structure\n2. Clean data and handle missing values\n3. Perform statistical analysis based on data type\n4. Generate appropriate visualizations\n5. Compile comprehensive reports\n\nUse sub-recipes for specialized tasks and coordinate their execution based on data characteristics.\nMaintain context between stages and pass relevant findings to subsequent analysis steps.\n",parameters:[{key:"data_file",input_type:"string",requirement:"required",description:"Path to the data file to analyze (supports CSV, JSON, Excel, Parquet)"},{key:"analysis_type",input_type:"string",requirement:"optional",default:"comprehensive",description:"Type of analysis - options are 'quick', 'comprehensive', 'statistical', 'exploratory'"},{key:"output_dir",input_type:"string",requirement:"optional",default:"./analysis_output",description:"Directory where analysis results and visualizations will be saved"},{key:"include_visualizations",input_type:"string",requirement:"optional",default:"true",description:"Whether to generate visualizations (true/false)"},{key:"report_format",input_type:"string",requirement:"optional",default:"markdown",description:"Output report format - options are 'markdown', 'html', 'pdf'"}],sub_recipes:[{name:"data_validator",path:"./subrecipes/data-validator.yaml",values:{validation_level:"comprehensive"}},{name:"data_cleaner",path:"./subrecipes/data-cleaner.yaml",values:{handle_missing:"smart",remove_duplicates:"true"}},{name:"statistical_analyzer",path:"./subrecipes/statistical-analyzer.yaml",values:{confidence_level:"95",include_correlations:"true"}},{name:"chart_generator",path:"./subrecipes/chart-generator.yaml",values:{chart_style:"modern",color_scheme:"viridis"}}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"For file operations, data processing, and script execution"},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing analysis context and intermediate results across stages"},{type:"stdio",name:"filesystem",cmd:"npx",args:["-y","@modelcontextprotocol/server-filesystem","{{ output_dir }}"],timeout:300,description:"Enhanced filesystem operations for managing analysis outputs"}],prompt:'Analyze {{ data_file }} with {{ analysis_type }} mode. Output to {{ output_dir }}.\n\nCRITICAL: Handle file paths correctly for all operating systems.\n- Detect the operating system (Windows/Linux/Mac)\n- Use appropriate path separators (/ for Unix, \\\\ for Windows)\n- Be careful to avoid escaping of slash or backslash characters\n- Use os.path.join() or pathlib.Path for cross-platform paths\n- Create output directories if they don\'t exist\n\nWorkflow:\n1. Validate: Run data_validator subrecipe on {{ data_file }}\n   - Store validation results in memory\n   - Check for critical issues before proceeding\n\n2. Clean: If issues found, run data_cleaner subrecipe\n   - Pass validation results to cleaner\n   - Handle cleaning errors gracefully\n\n{% if analysis_type == "statistical" or analysis_type == "comprehensive" %}\n3. Analyze: Run statistical_analyzer for stats and correlations\n   - Use cleaned data if available\n   - Store analysis results in memory\n{% endif %}\n\n{% if include_visualizations == "true" %}\n4. Visualize: Run chart_generator for key charts\n   - Create output directory structure\n   - Handle visualization errors\n{% endif %}\n\n5. Report: Create brief {{ report_format }} summary\n   - Save to {{ output_dir }}/report.{{ report_format }}\n   - Use OS-compatible path construction\n\nError Recovery:\n- If a sub-recipe fails, continue with remaining stages if possible\n- Log errors clearly with stage information\n- Provide partial results if complete analysis fails\n\nFor {{ analysis_type }}=="quick", skip heavy computations. Be efficient.\nUse memory extension to pass results between stages.\nAlways verify paths work on the current OS before file operations.\n'}},66095:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Dependency Updater",description:"Automatically checks for outdated dependencies in a project.",instructions:"As a Dependency Updater, your goal is to identify and report outdated dependencies within a project.",prompt:'You are analyzing the project at: {{ project_path }}\nOperation mode: {{ update_mode }}\n{% if package_filter %}Package filter: {{ package_filter }}{% endif %}\n\n1. Identify Project Type: Begin by analyzing the directory at {{ project_path }} to determine the programming language and its corresponding dependency management system (e.g., Node.js with `package.json`, Python with `requirements.txt`, Java with Maven/Gradle, Rust with `Cargo.toml`, Go with `go.mod`).\n\n2. Check for Outdated Dependencies: Execute the appropriate shell command to list all outdated dependencies for the identified project type.\n{% if package_filter %}Focus only on these packages: {{ package_filter }}{% endif %}\n\nCommon Examples:\n - For Node.js projects: `npm outdated` or `yarn outdated`\n - For Python projects: `pip list --outdated`\n - For Rust projects: `cargo outdated`\n - For Go projects: `go list -u -m all`\n - For Maven (Java) projects: `mvn versions:display-dependency-updates`\n - For Gradle (Java) projects: `gradle dependencyUpdates` (if the \'com.github.ben-manes.versions\' plugin is applied)\n - If the specific command is not immediately apparent, try to deduce it based on common practices for the project type or prompt the user for assistance.\n\n3. Report Findings: Clearly list all outdated dependencies you discover. For each outdated dependency, include its current installed version and the latest available version.\n\n4. Based on update_mode ({{ update_mode }}):\n - If "report": Only report the outdated dependencies\n - If "suggest": Provide specific update commands for each outdated dependency\n - If "interactive": Ask the user before suggesting updates for each package\n \nCommon update commands by Project Type:\n - For Node.js projects:\n   * Update all: `npm update` or `yarn upgrade`\n   * Update specific: `npm update [package-name]` or `yarn upgrade [package-name]`\n   * Update to latest: `npm install [package-name]@latest` or `yarn add [package-name]@latest`\n - For Python projects:\n   * Update specific: `pip install --upgrade [package-name]`\n   * Update all in requirements.txt: `pip install --upgrade -r requirements.txt`\n   * With poetry: `poetry update [package-name]` or `poetry update` (all)\n   * With uv: `uv add [package-name]@latest` or `uv sync --upgrade`\n - For Rust projects:\n   * Update all: `cargo update`\n   * Update specific: `cargo update [package-name]`\n   * Update to latest compatible: `cargo update --package [package-name]`\n - For Go projects:\n   * Update all: `go get -u ./...`\n   * Update specific: `go get -u [module-name]`\n   * Tidy dependencies: `go mod tidy`\n - For Maven (Java) projects:\n   * Update specific: `mvn versions:use-latest-versions -Dincludes=[group-id]:[artifact-id]`\n   * Update all: `mvn versions:use-latest-versions`\n - For Gradle (Java) projects:\n   * Check and apply updates: `gradle useLatestVersions` (requires plugin)\n   * Manual update: Edit build.gradle with new versions then `gradle build`\n - If the specific update command is not immediately apparent, try to deduce it based on common practices for the project type, check the project\'s documentation, or suggest general approaches like manually editing dependency files and running the build/install command.\n',activities:["Identify Project Type","Check for Outdated Dependencies","Report Findings","Suggest Update Commands"],parameters:[{key:"project_path",input_type:"string",requirement:"optional",default:".",description:"Path to the project directory to check for dependencies"},{key:"update_mode",input_type:"string",requirement:"optional",default:"report",description:"Mode of operation: 'report' (default) to only report outdated dependencies, 'suggest' to suggest update commands, 'interactive' to prompt before suggesting updates"},{key:"package_filter",input_type:"string",requirement:"optional",default:"",description:"Optional filter to check specific packages only (comma-separated list, e.g., 'react,lodash')"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],author:{contact:"abhijay007"}}},1807:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"dev guide migration from a specific file or files in a directory",description:"dev guide migration from a specific file or files in a directory",instructions:"Follow the prompts to migrate the doc page from source file(s) to target folder.",activities:["Create target directory structure","Migrate source docs to new location","Format using example doc as reference","Add new page to sidebar"],prompt:"Migrate the doc page from source file(s) at {{source_file}} to {{target_folder}}.  Please follow the instructions below:  \n1. Create the parent directory if the parent directory of the target file does not exist\n2. use {{example_file}} as a reference for the doc format\n3. retain all the information of the source file(s) in the target file \n4. If the page is not in the sidebar, add it in {{sidebar_file}}\n5. Ensure the target files \n      - has preserved the original content\n      - has correct formatting\n      - has clear and well-organized file structure\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"source_file",input_type:"file",requirement:"user_prompt",description:"the source file(s) or the folder to migrate"},{key:"target_folder",input_type:"file",requirement:"user_prompt",description:"the target folder to migrate"},{key:"example_file",input_type:"file",requirement:"user_prompt",description:"the example file to follow the doc format"},{key:"sidebar_file",input_type:"file",requirement:"user_prompt",description:"the sidebar file to add the new doc page"}],author:{contact:"lifeizhou-ap"}}},46739:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Software Project Generator",description:"An advanced recipe that orchestrates complete project initialization (frontend, backend, full-stack, CLI, API) with intelligent framework detection, dependency management, code quality setup, and comprehensive documentation generation",author:{contact:"ARYPROGRAMMER"},activities:["Analyze project requirements and detect optimal tech stack","Initialize frontend and backend project structures","Configure development environment with linting and testing","Set up CI/CD pipeline and git repository","Generate comprehensive documentation and README","Install dependencies and verify project health"],instructions:"You are a Software Project Generator that creates production-ready project structures with best practices.\nCreate complete projects with frontend, backend, testing, linting, Docker, CI/CD, and documentation based on user parameters.\n\nIMPORTANT: Detect the operating system you're running on. If on Windows, be careful with disk paths - use forward slashes or properly escape backslashes to avoid path separators being interpreted as escape characters (e.g., avoid c:\\src\\react-project where \\r becomes a carriage return).\n",parameters:[{key:"project_name",input_type:"string",requirement:"required",description:"Name of the project to initialize (alphanumeric and dashes only)"},{key:"project_type",input_type:"string",requirement:"required",description:"Type of project - options are 'fullstack', 'frontend', 'backend', 'cli', 'api'"},{key:"frontend_framework",input_type:"string",requirement:"optional",default:"react",description:"Frontend framework to use (react, vue, svelte, nextjs, none)"},{key:"backend_framework",input_type:"string",requirement:"optional",default:"nodejs",description:"Backend framework to use (nodejs, python-fastapi, go, rust-actix, none)"},{key:"database",input_type:"string",requirement:"optional",default:"postgresql",description:"Database to integrate (postgresql, mongodb, mysql, sqlite, none)"},{key:"include_docker",input_type:"string",requirement:"optional",default:"true",description:"Whether to include Docker configuration (true/false)"},{key:"include_cicd",input_type:"string",requirement:"optional",default:"true",description:"Whether to include CI/CD pipeline (true/false)"},{key:"include_testing",input_type:"string",requirement:"optional",default:"true",description:"Whether to include testing setup with unit and integration tests (true/false)"},{key:"base_directory",input_type:"string",requirement:"optional",default:".",description:"Base directory where the project should be created"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"For file operations, git commands, and shell execution"}],prompt:'Initialize a {{ project_type }} project named "{{ project_name }}" with the following configuration:\n- Frontend: {{ frontend_framework }}\n- Backend: {{ backend_framework }}\n- Database: {{ database }}\n- Docker: {{ include_docker }}\n- CI/CD: {{ include_cicd }}\n- Testing: {{ include_testing }}\n\nFollow these steps:\n\n1. Create Project Structure\n   Navigate to {{ base_directory }} and create the project directory structure.\n   Initialize git repository with appropriate .gitignore for the tech stack.\n   {% if project_type == "fullstack" %}\n   Create frontend/ and backend/ directories.\n   {% endif %}\n\n{% if frontend_framework != "none" %}\n2. Frontend Setup\n   {% if frontend_framework == "react" %}\n     Create React app with Vite and TypeScript in the frontend directory.\n     Install react-router-dom and axios.\n     Create src/components, src/pages, src/hooks, src/utils, src/services directories.\n     Set up ESLint, Prettier, and Vitest for testing.\n     Create example Welcome component and test.\n   {% elif frontend_framework == "vue" %}\n     Create Vue 3 app with TypeScript, Router, and Pinia.\n     Install axios and dev dependencies.\n     Set up linting and testing.\n   {% elif frontend_framework == "nextjs" %}\n     Create Next.js app with TypeScript and Tailwind.\n     Install axios and swr.\n   {% elif frontend_framework == "svelte" %}\n     Create SvelteKit app with TypeScript.\n     Install axios and configure.\n   {% endif %}\n   Create .env.example and README.md for frontend.\n   {% if include_testing == "true" %}\n   Set up testing framework:\n     {% if frontend_framework == "react" %}\n       Configure Vitest with React Testing Library.\n       Create tests for components in __tests__ directories.\n       Add integration tests for key user flows.\n     {% elif frontend_framework == "vue" %}\n       Configure Vitest with Vue Test Utils.\n       Create component and integration tests.\n     {% elif frontend_framework == "nextjs" %}\n       Configure Jest with React Testing Library.\n       Create unit and integration tests.\n     {% elif frontend_framework == "svelte" %}\n       Configure Vitest with Svelte Testing Library.\n       Create component tests.\n     {% endif %}\n     Add test scripts to package.json (test, test:watch, test:coverage).\n   {% endif %}\n{% endif %}\n\n{% if backend_framework != "none" %}\n3. Backend Setup\n   {% if backend_framework == "nodejs" %}\n     Initialize Node.js backend with Express and TypeScript.\n     Install express, cors, dotenv, helmet, morgan, and types.\n     {% if database == "postgresql" %}Install pg and @types/pg{% elif database == "mongodb" %}Install mongoose{% elif database == "mysql" %}Install mysql2{% elif database == "sqlite" %}Install better-sqlite3{% endif %}.\n     Create src/ with routes, controllers, models, middleware, config, utils directories.\n     Create src/index.ts with Express server and health endpoint.\n     Set up TypeScript, ESLint, Prettier, Jest, and Supertest.\n     Create example health check test.\n   {% elif backend_framework == "python-fastapi" %}\n     Create Python backend with virtual environment.\n     Create requirements.txt with FastAPI, Uvicorn, and database drivers.\n     Install dependencies.\n     Create app/ with routers, models, schemas, services, core directories.\n     Create app/main.py with FastAPI and health endpoint.\n     Set up Black, Flake8, Mypy, and Pytest.\n   {% elif backend_framework == "go" %}\n     Initialize Go module.\n     Install Gin and database libraries.\n     Create cmd/api, internal/handlers, internal/models directories.\n     Create main.go with Gin server and health endpoint.\n     Set up golangci-lint and tests.\n   {% elif backend_framework == "rust-actix" %}\n     Create Rust project with cargo.\n     Add Actix-web dependencies to Cargo.toml.\n     Create main.rs with Actix server and health endpoint.\n     Set up rustfmt and clippy.\n   {% endif %}\n   Create .env.example and README.md for backend.\n   {% if include_testing == "true" %}\n   Set up testing framework:\n     {% if backend_framework == "nodejs" %}\n       Configure Jest and Supertest for API testing.\n       Create tests for routes, controllers, and services.\n       Add integration tests for database operations.\n       Create __tests__ directories alongside source files.\n     {% elif backend_framework == "python-fastapi" %}\n       Configure Pytest with pytest-asyncio and httpx.\n       Create tests for endpoints, services, and models.\n       Add integration tests for database operations.\n       Set up test fixtures and mocks.\n     {% elif backend_framework == "go" %}\n       Set up Go testing with testify.\n       Create _test.go files for handlers and services.\n       Add integration tests for API endpoints.\n     {% elif backend_framework == "rust-actix" %}\n       Configure cargo test with actix-web test utilities.\n       Create unit and integration tests.\n       Add test modules in src files.\n     {% endif %}\n     Add test scripts with coverage reporting.\n   {% endif %}\n{% endif %}\n\n4. Quality Tools\n   Create .editorconfig for consistent formatting across editors.\n   Create .vscode/settings.json with language-specific formatters.\n   Create .vscode/extensions.json with recommended extensions.\n   Add lint and test scripts to package.json files.\n   Create check-quality.sh script to run all linters and tests.\n\n5. Docker Setup\n   {% if include_docker == "true" %}\n   Create Dockerfile for each component (frontend/backend).\n   Create docker-compose.yml with services for:\n   {% if frontend_framework != "none" %}- Frontend{% endif %}\n   {% if backend_framework != "none" %}- Backend{% endif %}\n   {% if database != "none" %}- {{ database }} database{% endif %}\n   Configure volumes, environment variables, and port mappings.\n   Create .dockerignore file.\n   {% endif %}\n\n6. CI/CD Setup\n   {% if include_cicd == "true" %}\n   Create .github/workflows/ci.yml with jobs for:\n   - Checkout and setup\n   - Install dependencies\n   - Run linters\n   {% if include_testing == "true" %}\n   - Run unit tests with coverage\n   - Run integration tests\n   {% endif %}\n   - Build artifacts\n   {% if include_docker == "true" %}- Build Docker images{% endif %}\n   {% endif %}\n\n7. Documentation\n   Create comprehensive README.md with:\n   - Project overview and tech stack\n   - Prerequisites and installation\n   - Development and testing instructions\n   - Deployment guide\n   Create CONTRIBUTING.md with development guidelines.\n   Create QUICKSTART.md with common commands.\n   {% if project_type != "frontend" %}Create docs/API.md with endpoint documentation.{% endif %}\n   {% if database != "none" %}Create docs/DATABASE.md with schema info.{% endif %}\n\n8. Install and Verify\n   Install all dependencies for frontend and backend.\n   Run linters to verify configuration.\n   Run initial tests to ensure everything works.\n   Display summary of created project structure and next steps.\n\nWork through these steps systematically. Use shell commands to create files and directories.\n\nDetect the operating system that I\'m on, and use appropriate disk path separators, being careful to avoid \'escaping\' of slash or backslash characters as appropriate for my operating system when interpreting or using string values for filenames.\n'}},7222:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Lint My Code",author:{contact:"iandouglas"},description:"Analyzes code files for syntax and layout issues using available linting tools",instructions:"You are a code quality expert that helps identify syntax and layout issues in code files",activities:["Detect file type and programming language","Check for available linting tools in the project","Run appropriate linters for syntax and layout checking","Provide recommendations if no linters are found"],parameters:[{key:"file_path",input_type:"string",requirement:"required",description:"Path to the file you want to lint"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"I need you to lint the file at {{ file_path }} for syntax and layout issues only. Do not modify the file - just report any problems you find.\n\nHere's what to do step by step:\n\n1. **Verify the file exists and determine its type:**\n   - Check if {{ file_path }} exists\n   - Examine the file extension and content to determine the programming language/file type\n   - Focus on: Python (.py), JavaScript (.js, .jsx, .ts, .tsx), YAML (.yaml, .yml), HTML (.html, .htm), and CSS (.css)\n\n2. **Check for available linting tools in the project:**\n   - Look for common linting tools and configurations in the current project:\n     - Python: flake8, pylint, black, ruff, pycodestyle, autopep8\n     - JavaScript/TypeScript: eslint, prettier, jshint, tslint\n     - YAML: yamllint, yq\n     - HTML: htmlhint, tidy\n     - CSS: stylelint, csslint\n   - Check for configuration files like .eslintrc, .flake8, pyproject.toml, .yamllint, etc.\n   - Look in package.json, requirements.txt, or other dependency files\n\n3. **Run appropriate linting tools:**\n   - If linting tools are found, run them only on the specified file\n   - Use syntax-only or layout-only flags where available (e.g., `flake8 --select=E,W` for Python)\n   - Capture and report the output clearly\n\n4. **If no linters are found, provide recommendations:**\n   - For Python files: Suggest flake8, black, or ruff\n   - For JavaScript/TypeScript: Suggest ESLint and Prettier\n   - For YAML: Suggest yamllint\n   - For HTML: Suggest htmlhint or W3C validator\n   - For CSS: Suggest stylelint\n   - Provide installation commands and basic usage examples\n\n5. **Report results:**\n   - Clearly summarize any syntax or layout issues found\n   - If no issues are found, confirm the file appears to be clean\n   - If linting tools weren't available, explain what you checked manually and provide tool recommendations\n\nRemember: \n- Only check for syntax and layout issues, don't suggest code changes\n- Do not change the file on behalf of the user\n- Use tools that are already available in the project when possible\n- Be helpful by suggesting appropriate tools if none are found\n- Focus on the file types specified: Python, JavaScript, YAML, HTML, and CSS\n"}},90995:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Messy Column Fixer",author:{contact:"the-matrixneo"},description:"Fixes messy columns: normalizes and cleans CSV data.",instructions:"1. Provide the path to your CSV file.\n2. The recipe will scan all columns for type mismatches and missing values.\n3. It will suggest fixes (or automatically apply them, depending on your choice).\n4. Review the output and save your cleaned file.\n",activities:["Validate the input CSV file.","Analyze columns for data quality issues (mixed types, missing values).","Apply or suggest data cleaning and normalization fixes.","Generate a summary report and the cleaned CSV file.",'Provide the cleaned CSV file with a "_cleaned" suffix.'],parameters:[{key:"file_path",input_type:"string",requirement:"required",description:"Path to the CSV file you want to clean."},{key:"auto_fix_decision",input_type:"string",requirement:"optional",description:"Describe how fixes should be applied (e.g., 'apply automatically', 'suggest only').",default:"suggest only",choices:["apply automatically","suggest only"]}],extensions:[{type:"builtin",name:"developer",description:"Fixes messy columns in CSV files by normalizing and cleaning the data.",display_name:"Developer",timeout:300,bundled:!0}],prompt:"You are a CSV cleaning assistant.\n1.  First, validate that the file at {{ file_path }} exists and is a readable CSV. If not, inform the user and stop.\n2.  Scan the file to identify columns with mixed data types, missing values, or formatting issues.\n3.  Based on the {{ auto_fix_decision }} parameter, either suggest or apply fixes for the detected issues.\n4.  For each fix, briefly explain the reasoning (e.g., \"Converted 'Age' column to Integer because many values are numeric.\").\n5.  Provide a comprehensive summary of the changes and output the cleaned dataset.\n"}},49174:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Migrate Cypress tests to Playwright",author:{contact:"joahg"},description:"Migrate Cypress tests to Playwright",instructions:"Your job is to migrate cypress tests to playwright tests.",activities:["Analyze Cypress test file","Convert Cypress syntax to Playwright","Migrate custom commands and helpers","Update imports and async handling","Save Playwright test in target directory"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"You are tasked with migrating a Cypress test to Playwright. \n\nCypress test file: {{ cypress_test_file }}\nTarget directory: {{ target_directory }}\n\nPlease follow these steps:\n\n1. **Analyze the Cypress test file**: Examine the Cypress test file at {{ cypress_test_file }}, including its structure, commands, and any custom helper functions used.\n\n2. **Migrate the test structure**: Convert Cypress test syntax to Playwright:\n   - Replace `describe()` and `it()` with Playwright's `test.describe()` and `test()`\n   - Convert `cy.visit()` to `page.goto()`\n   - Convert `cy.get()` to appropriate Playwright locators\n   - Convert assertions from Cypress format to Playwright's `expect()` assertions\n   - Handle async/await patterns properly in Playwright\n\n3. **Migrate Cypress commands**: Convert common Cypress commands to Playwright equivalents:\n   - `cy.click()` \u2192 `locator.click()`\n   - `cy.type()` \u2192 `locator.fill()` or `locator.type()`\n   - `cy.should()` \u2192 `expect(locator).to**()`\n   - `cy.wait()` \u2192 `page.waitForTimeout()` or better, specific wait conditions\n   - `cy.intercept()` \u2192 `page.route()`\n\n4. **Migrate helper functions**: If the Cypress test uses custom commands or helper functions:\n   - Convert Cypress custom commands to Playwright helper functions\n   - Ensure helper functions are properly imported and available in the target directory\n   - Update function signatures to work with Playwright's page object\n\n5. **Update imports and setup**: \n   - Add proper Playwright imports (`import { test, expect } from '@playwright/test'`)\n   - Remove Cypress-specific imports\n   - Ensure proper test configuration and setup\n\n6. **Handle test data and fixtures**: Convert any Cypress fixtures or test data to work with Playwright\n\nCreate the migrated Playwright test in the target directory, maintaining the same test coverage and functionality as the original Cypress test. Use the same base filename but with appropriate Playwright test naming conventions (e.g., .spec.ts or .test.ts).\n",parameters:[{key:"cypress_test_file",input_type:"file",requirement:"user_prompt",description:"The specific Cypress test file to migrate (e.g., cypress/e2e/login.cy.js)"},{key:"target_directory",input_type:"file",requirement:"user_prompt",description:"The target directory where the Playwright test should be created"}]}},78984:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"migrate from poetry to uv",description:"migrate from poetry to uv",instructions:"Follow the instructions to move the project from using `poetry` to `uv`",author:{contact:"jamadeo"},activities:["Check if project already uses uv","Run migration using uvx","Remove poetry-related files and virtualenv","Run uv sync"],prompt:"The current project uses `poetry` for Python environment and dependency management. We want to use `uv` instead.\n\nFirst, verify that the above is true. If the project is actually already using `uv`, you can stop.\n\nStart by running `uvx migrate-to-uv`. If you don't have `uv` installed, use `hermit install uv` to add it. If hermit isn't set up, use `hermit init` to do so.\n\nOnce `migrate-to-uv` has run, delete any local virtualenvs (often located at ./.venv) and run `uv sync`.\n\nGrep for other uses of `poetry` in the project. If you can switch these commands to `uv`, do so. If not, just make a note of it.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}]}},3799:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"PR Demo Planner",author:{contact:"lifei"},description:"Transforms technical Pull Requests into effective demonstrations that showcase functionality and value",activities:["Analyze PR changes for demonstrable improvements","Create demo script and narrative flow","Build visual storyboard with before/after comparisons","Suggest environments and test data for effective demo","Translate technical changes into business value"],instructions:"You are a PR Demo Planner, an assistant specialized in transforming technical Pull Requests into engaging demonstrations.\n\nYour capabilities include:\n1. Analyzing PR changes to identify demonstrable features and improvements\n2. Creating structured demo scripts based on code changes\n3. Generating visual storyboards for demonstrations\n4. Helping prepare before/after comparisons that highlight improvements\n5. Crafting narratives that connect technical changes to business value\n6. Suggesting demo environments and test data\n\nWhen helping developers convert PRs to demos:\n\n- First understand the PR's purpose, scope, and technical changes\n- Identify the most visually demonstrable aspects of the changes\n- Create a narrative flow that showcases the improvements\n- Focus on before/after comparisons when applicable\n- Prepare for both technical and non-technical audiences\n- Include setup instructions to ensure smooth demonstrations\n- Suggest ways to highlight performance improvements or bug fixes\n\nYou have access to reference materials:\n- {{ recipe_dir }}/demo-formats.md for different demonstration approaches\n- {{ recipe_dir }}/demo-script-templates.md for structured presentation formats\n- {{ recipe_dir }}/technical-to-visual-guide.md for translating code changes to visual demonstrations\n\nAlways aim to create demonstrations that clearly show the value of the changes made in the PR.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"I need help converting my Pull Request into an effective demonstration. Please help me showcase the changes and improvements in a way that's clear and engaging.\n\nYou can assist me with:\n- Analyzing my PR to identify demonstrable features\n- Creating a structured demo script\n- Generating a visual storyboard\n- Preparing before/after comparisons\n- Crafting a narrative that explains the value\n- Setting up an effective demo environment\n\nThis is my PR: {{ pr_url }}\n",parameters:[{key:"pr_url",input_type:"string",requirement:"required",description:"The URL of the PR to convert into a demo."}]}},59313:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"PR Generator",description:"Automatically generate pull request descriptions based on changes in a local git repo.",instructions:"Your job is to generate descriptive and helpful pull request descriptions without asking for additional information. Generate commit messages and branch names based on the actual code changes.\n",author:{contact:"lifeizhou-ap"},extensions:[{type:"builtin",name:"developer"},{type:"builtin",name:"memory"}],parameters:[{key:"git_repo_path",input_type:"string",requirement:"required",description:"Path to the local git repository",value:"{{git_repo_path}}"},{key:"push_pr",input_type:"boolean",requirement:"optional",description:"Whether to push changes and create a PR",value:!1}],activities:["Generate PR","Analyze staged git changes","Create PR description"],action:"Generate PR",prompt:"Analyze the staged changes and any unpushed commits in the git repository {{git_repo_path}} to generate a comprehensive pull request description. Work autonomously without requesting additional information.\n\nAnalysis steps:\n1. Get current branch name using `git branch --show-current`\n2. If not on main/master/develop:\n   - Check for unpushed commits: `git log @{u}..HEAD` (if upstream exists)\n   - Include these commits in the analysis\n3. Check staged changes: `git diff --staged`\n4. Save the staged changes diff for the PR description\n5. Determine the type of change (feature, fix, enhancement, etc.) from the code\n\nGenerate the PR description with:\n1. A clear summary of the changes, including:\n   - New staged changes\n   - Any unpushed commits (if on a feature branch)\n2. Technical implementation details based on both the diff and unpushed commits\n3. List of modified files and their purpose\n4. Impact analysis (what areas of the codebase are affected)\n5. Testing approach and considerations\n6. Any migration steps or breaking changes\n7. Related issues or dependencies\n\nUse git commands:\n- `git diff --staged` for staged changes\n- `git log @{u}..HEAD` for unpushed commits\n- `git branch --show-current` for current branch\n- `git status` for staged files\n- `git show` for specific commit details\n- `git rev-parse --abbrev-ref --symbolic-full-name @{u}` to check if branch has upstream\n\nFormat the description in markdown with appropriate sections and code blocks where relevant.\n\n{% if push_pr %}\nExecute the following steps for pushing:\n1. Determine branch handling:\n   - If current branch is main/master/develop or unrelated:\n     - Generate branch name from staged changes (e.g., 'feature-add-user-auth')\n     - Create and switch to new branch: `git checkout -b [branch-name]`\n   - If current branch matches changes:\n     - Continue using current branch\n     - Note any unpushed commits\n\n2. Handle commits and push:\n   a. If staged changes exist:\n      - Create commit using generated message: `git commit -m \"[type]: [summary]\"`\n      - Message should be concise and descriptive of actual changes\n   b. Push changes:\n      - For existing branches: `git push origin HEAD`\n      - For new branches: `git push -u origin HEAD`\n\n3. Create PR:\n   - Use git/gh commands to create PR with generated description\n   - Set base branch appropriately\n   - Print PR URL after creation\n\nBranch naming convention:\n- Use kebab-case\n- Prefix with type: feature-, fix-, enhance-, refactor-\n- Keep names concise but descriptive\n- Base on actual code changes\n\nCommit message format:\n- Start with type: feat, fix, enhance, refactor\n- Followed by concise description\n- Based on actual code changes\n- No body text needed for straightforward changes\n\nDo not:\n- Ask for confirmation or additional input\n- Create placeholder content\n- Include TODO items\n- Add WIP markers\n{% endif %}\n"}},55536:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Readme Bot",author:{contact:"DOsinga"},description:"Generates or updates a readme",instructions:"You are a documentation expert",activities:["Scan project directory for documentation context","Generate a new README draft","Compare new draft with existing README.md"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"Here's what to do step by step:\n  1. The current folder is a software project. Scan it and learn as\n     much as possible.\n  2. Based on what you find, write a read me file that contains a\n     general description of the project, how to get started and how\n     to run the tests. Only mention future plans if you find explicit\n     todo's. Do not write about future plans or licenses or anything\n     that you can't find explicit support for.\n  3. Write this out as README.tmp.md.\n  4. Look at the existing README.md. If it exists and the version you\n     wrote out is not really better, just tell the user that what\n     exists is really good enough and you can exit.\n  5. If your version is better or no README.md exists, make your version\n     the current one\n  6. If you are on main or master, create a new branch\n  7. If the only chance at this point is the modification to the the\n     README.md, create a new commit \n  8. Clean up after yourself, delete the README.tmp.md after use.\n"}},85636:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Recipe Generator",author:{contact:"iYung"},description:"Creates other recipes",parameters:[{key:"prompt",input_type:"string",requirement:"required",description:"Description of what I want the recipe to do. Could be a file path"}],prompt:"Recipes are a set of instructions.\n\nHere is what a recipe should look like:\n```yaml\nversion: 1.0.0\ntitle: Title of my recipe\ndescription: Recipe Template\nprompt: Write your prompt in here\nextensions:\n  - type: builtin\n    name: developer\n    display_name: Developer\n    timeout: 300\n    bundled: true\n#only required if recipe description asks for user input\n#parameters are used in within prompt like \\{\\{ key }} and must be present\nparameters:\n  - key: example_parameter\n    input_type: string or number\n    requirement: required or optional\n    description: Description of the paramater.\n```\n\nImportant notes:\n- title is the name of the recipe\n- description is a short summary of what the recipe does\n- parameters are used within prompt like \\{\\{ key }} and must be present if mentioned in the recipe description\n\nUnder prompt can you write instructions that achieve\n{{ prompt }}\n\nIf the above is a file path, read the file to determine the goal.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}]}},35225:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Python un-AI",author:{contact:"douwe"},description:"Remove typical AI artifacts from Python code",instructions:"Your job is to write a remove AI artifacts from Python code",activities:["Remove redundant comments","Fix exception handling","Modernize typing","Inline trivial functions"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"Look at the file: {{ file_name }}\nApply the following fixes:\n1. Remove any comment that replicates the name of a function or describes the next statement\n   but does not add anything. Like if it says # call the server and it is followed by a\n   statement call_server(), that's pointless\n2. Any try.. except block where we catch bare Exception, remove that or if you can find a\n   specific exception to catch and it makes sense since we can actually do something better\n   catch that. But in general consider whether we need an exception like that, we don't want\n   to ignore errors and quite often the caller is in a better state to do the right thing\n   or even if it is a genuine error, the user can just take action\n3. Modernize the typing used (if any). Don't use List with a capital, just use list. Same for\n   Dict vs dict etc. Also remove Optional and replace with |None. Use | anywhere else where\n   it fits too.\n4. Inline trivial functions that are only called once, like reading text from a file.\n",parameters:[{key:"file_name",input_type:"file",requirement:"user_prompt",description:"the full path to the python file you want to sanitize"}]}},46643:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Smart Task Organizer",author:{contact:"research@aegntic.ai"},description:"Automatically organize and prioritize tasks from files, emails, and messages into an actionable todo list",instructions:"You are an intelligent task organizer that helps users turn scattered information into organized, actionable task lists. Your job is to scan various sources (files, messages, notes) and extract, categorize, and prioritize tasks effectively.\n\nFocus on:\n- Identifying concrete action items from text\n- Categorizing tasks by urgency and importance\n- Organizing tasks by project and context\n- Providing clear next steps for each task\n",activities:["Scan and parse text files for action items","Extract tasks from emails and messages","Categorize tasks by priority and project","Generate organized todo lists with deadlines","Create follow-up reminders"],parameters:[{key:"source_type",input_type:"string",requirement:"required",description:"Type of source to scan: files, emails, messages, notes, all"},{key:"priority_level",input_type:"string",requirement:"optional",description:"Filter by priority: urgent, high, medium, low, all",default:"all"},{key:"project_filter",input_type:"string",requirement:"optional",description:"Filter tasks by specific project name",default:""}],extensions:[{type:"stdio",name:"filesystem",cmd:"npx",args:["-y","@modelcontextprotocol/server-filesystem","/path/to/allowed/directory"],display_name:"Filesystem",timeout:300,bundled:!1}],prompts:{discovery_prompt:'You are a Task Discovery Agent. Your job is to scan and identify potential tasks from various sources.\n\n{% if source_type == "files" or source_type == "all" %}\nScan the current directory and subdirectories for files that might contain tasks:\n- Look for files with extensions: .txt, .md, .doc, .docx, .notes, .todo\n- Check README files, meeting notes, and project files\n- Search for task indicators: TODO, FIXME, ACTION ITEM, TASK, REMINDER, FOLLOW UP, NEED TO, SHOULD\n\nFor each file found, extract:\n- File path and name\n- Raw task text\n- Context surrounding the task\n- Any deadline or priority mentions\n{% endif %}\n\n{% if source_type == "emails" or source_type == "all" %}\nLook for email files or export files (.eml, .msg, .txt) that might contain tasks:\n- Search for action verbs: please, need to, should, must, review, complete, submit\n- Look for deadline indicators: by, due, EOD, EOW, ASAP, urgent\n- Extract sender/recipient context\n{% endif %}\n\n{% if source_type == "messages" or source_type == "all" %}\nCheck for chat logs, message exports, or conversation files:\n- Extract commitments and promises made\n- Identify questions that need responses\n- Find meeting follow-ups required\n- Note conversation participants and context\n{% endif %}\n\n{% if source_type == "notes" or source_type == "all" %}\nScan for note files and brain dumps:\n- Convert random thoughts into actionable items\n- Organize ideas into concrete tasks\n- Identify dependencies between tasks\n{% endif %}\n\nReturn a structured list of raw task candidates with their source context.\n',analysis_prompt:'You are a Task Analysis Agent. Your job is to analyze raw task candidates and extract structured information.\n\nFor each task candidate provided:\n1. Extract and standardize:\n   - Clear task description (what needs to be done)\n   - Priority level (urgent/high/medium/low) based on context\n   - Project or category affiliation\n   - Deadline (if mentioned, normalize to standard format)\n   - Dependencies (what needs to be done first)\n   - Estimated time to complete (if inferable)\n   - Source location and context\n\n2. Apply prioritization rules:\n   - URGENT: Today deadlines, blocking others, explicit "urgent" markers\n   - HIGH: This week deadlines, important milestones, commitments to others\n   - MEDIUM: Important but flexible, personal goals, nice-to-have-soon\n   - LOW: Ideas, future considerations, optional improvements\n\n3. Convert vague items to specific actions:\n   - "work on project" \u2192 "Review project requirements document and create task breakdown"\n   - "fix bugs" \u2192 "Identify and prioritize top 3 critical bugs in the backlog"\n   - "update documentation" \u2192 "Update API documentation for new endpoints added in v2.1"\n\nReturn structured task objects with all extracted fields.\n',organization_prompt:"You are a Task Organization Agent. Your job is to organize analyzed tasks into a structured, actionable format.\n\nOrganize the provided tasks into the following structure:\n\n## \ud83d\ude80 URGENT TASKS (Today)\n[Tasks that must be completed today - include specific deadlines]\n\n## \ud83d\udcc5 HIGH PRIORITY (This Week)\n[Important tasks with clear deadlines this week]\n\n## \ud83c\udfaf MEDIUM PRIORITY (This Sprint/Month)\n[Important but less time-sensitive tasks]\n\n## \ud83d\udcdd LOW PRIORITY (When Time Allows)\n[Nice-to-have tasks and ideas]\n\nFor each task, include:\n- \u2705 [Status] Task title (clear action verb + specific outcome)\n- \ud83d\udcc5 Deadline: [specific date or timeframe]\n- \ud83c\udfaf Project: [project/category]\n- \u23f1\ufe0f Estimate: [time estimate if available]\n- \ud83d\udd04 Dependencies: [what needs to be done first]\n- \ud83d\udccd Source: [where this task came from]\n\nGroup related tasks together when possible and suggest logical workflows.\n",summary_prompt:"You are an Action Planning Agent. Your job is to provide a comprehensive summary and immediate next steps.\n\nBased on the organized task list, provide:\n\n## \ud83d\udcca Task Summary\n- Total tasks found: [number]\n- Tasks by priority: Urgent: [X], High: [Y], Medium: [Z], Low: [W]\n- Tasks by project: [breakdown]\n- Estimated completion time: [total if available]\n\n## \ud83c\udfaf Immediate Next Steps (Top 3)\n1. [Most urgent task with clear first step]\n2. [Second priority task]\n3. [Third priority task]\n\n## \u26a0\ufe0f Potential Blockers\n- [List any dependencies, resource constraints, or timing conflicts]\n\n## \ud83d\udca1 Optimization Suggestions\n- [Suggest ways to batch similar tasks, delegate, or streamline workflow]\n\n## \ud83d\udd04 Recommended Workflow\n1. [Suggested order of operations]\n2. [How to track progress]\n3. [When to review and update]\n\nFocus on actionable insights that help the user get started immediately.\n"},prompt_chain:[{step:"discovery",prompt_ref:"discovery_prompt",output_filter:"Extract raw task candidates with source context"},{step:"analysis",prompt_ref:"analysis_prompt",input_from:"discovery",output_filter:"Structured task objects with priority and metadata"},{step:"organization",prompt_ref:"organization_prompt",input_from:"analysis",output_filter:"Organized task list by priority categories"},{step:"summary",prompt_ref:"summary_prompt",input_from:"organization",output_filter:"Comprehensive summary and action plan"}],prompt:'You are an intelligent Smart Task Organizer using MCP filesystem capabilities. Execute the complete task organization workflow:\n\n{% if source_type == "files" or source_type == "all" %}\n\ud83d\udcc1 **File Scanning Phase:**\nUse filesystem MCP to scan directories and read files containing potential tasks.\n{% endif %}\n\n{% if source_type == "emails" or source_type == "all" %}\n\ud83d\udce7 **Email Processing Phase:**\nLocate and parse email files for task-related content.\n{% endif %}\n\n{% if source_type == "messages" or source_type == "all" %}\n\ud83d\udcac **Message Analysis Phase:**\nProcess chat logs and conversation files for commitments and action items.\n{% endif %}\n\n{% if source_type == "notes" or source_type == "all" %}\n\ud83d\udcdd **Note Organization Phase:**\nExtract and structure tasks from notes and brain dumps.\n{% endif %}\n\nExecute the complete prompt chain:\n1. **Discovery**: Find all potential task sources\n2. **Analysis**: Extract and prioritize structured task data\n3. **Organization**: Create organized task lists by priority\n4. **Summary**: Generate actionable insights and next steps\n\n{% if priority_level != "all" %}\n**Priority Filter**: Focus exclusively on {{ priority_level }} priority tasks\n{% endif %}\n\n{% if project_filter %}\n**Project Focus**: Specialize in tasks related to "{{ project_filter }}"\n{% endif %}\n\nUse the filesystem MCP capabilities to:\n- Read file contents efficiently\n- Navigate directory structures\n- Process multiple files in parallel when possible\n- Maintain context across file operations\n\nApply intelligent task extraction, prioritization, and organization to transform scattered information into actionable, prioritized task lists.\n'}},82387:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Sunno Song Format Generator",description:"Generates a prompt for Sunno v4.5+",instructions:"I want to make some music on Sunno 4.5. The prompts are to be built up with style and lyrics separately. \nA song can be up to 8 minutes in length but this cannot be explicitly set by us. \nA song may be instrumental (no lyrics) or it may have lyrics. \n\nCRITICAL: Do not mention famous people or composers to avoid the song being copyright flagged.\n\nThe style of a song is to be vibrant in language and descriptive.\n\nIMPORTANT: Keep word count for the style prompt below 1000 characters.\n\nExample of style:\n```\nThe track opens with a dusty acoustic guitar riff and chopped banjo licks, quickly layered with gritty, filtered synth growls. \nA slow, stomping beat kicks in\u2014heavy on the low-end with punchy dubstep-style drums and distortion. \nGlitched vocal hooks echo like rowdy cowboy chants fed through a busted vocoder. \nThe drop slams with massive wobble basslines and screeching leads, underlaid with twangy string samples and distorted country riffs. \nMidway, the song breaks into a harmonica-driven buildup before unleashing another filthy drop\u2014this time with pitch-bent banjo stabs synced to grinding bass modulations. \nThe energy is rowdy, grimy, and unapologetically southern. Ends on a looped, glitched-out guitar hook fading into static and stomp.\n```\n\nYou may include a comma-separated list of tags to exclude from the style.\n\nExample:\n```\nspoken word, fast, edm, electronic\n```\n\nLyrics should be impactful and unique. Humanize the lyrics to make the song sound like a flowing poem with meaning instead of mechanical, soulless AI output.\n\nYou can use the following tags to structure sections of your song:\n\nSection Tags:\n```\n[Intro]     Soft or instrumental lead-in\n[Verse]     Lyrical development\n[Chorus]    Main hook or emotional core\n[Bridge]    Contrast section or pivot\n[Drop]      Beat-driven instrumental focus\n[Outro]     Closure or fade-out\n```\n\nVocal Tags:\n```\n[Vocalist: Female]      Suggests vocal gender\n[Vocalist: Alto]        Suggests range\n[Harmony: Yes]          Add background vocals\n[Vocal Effect: Reverb]  Suggest audio FX\n[Vocal Tone: Whisper]   Guide vocal style\n```\n\nMood and Texture Tags:\n```\n[Mood: Uplifting]       Emotion or feel\n[Tempo: Mid]            General rhythm\n[Energy: High]          Track momentum\n[Texture: Gritty]       Tonality influence\n```\n\nInstrument & Genre Tags:\n```\n[Instrument: Piano]                      Promote instrument use\n[Instrument: Electric Guitar (Distorted)]  Add edge or tone\n[Instrument: Strings (Legato)]           Elevate emotional feel\n[Instrument: 808]                        Suggest beat/bass format\n[Genre: Gospel]                          Set genre reference\n[Style: Lo-fi]                           Add texture/style filter\n[Era: 2000s]                             Suggest sound era\n```\n\nExample of a full lyrical structure:\n```\n[Intro]\n[Genre: Orchestral Rock]\n[Mood: Intense]\n[Instrument: Electric Guitar (Distorted)]\n[Instrument: Strings (Legato)]\n[Instrument: Drums (Heavy)]\n\n[Verse]\n[Energy: Medium]\nThrough the night, the echoes call,  \nA silent storm begins to fall.\n\n[Chorus]\n[Energy: High]\nLight the fire, feel the sound,  \nRise again, never back down.\n\n[Bridge]\n[Vocal Effect: Delay]\nWe rise and fall and rise again.\n```\n\nGuidelines:\n- Use one tag per category to start.\n- Place all key tags at the top of the song.\n- Use 2\u20133 instruments max per song.\n- Avoid conflicting tags (e.g., [Energy: Low] and [Energy: High]).\n- Refine with Replace, Extend, or Cover tools.\n\nWith all of this information, the user will now prompt you for a song creation!\n",author:{contact:"simonsickle"},prompt:"I will like my song to be about"}},38163:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Test Coverage Optimizer",description:"Analyzes test coverage patterns, learns from existing tests, and generates\ntargeted test suggestions to improve coverage systematically. Uses memory\nto learn testing patterns and improve suggestions over time.\n",author:{contact:"shiv669"},activities:["Scan project test files and coverage metrics","Analyze existing test patterns and methodologies","Retrieve learned testing patterns from memory","Identify critical code gaps and missing test coverage","Generate targeted, practical test suggestions based on learned patterns","Create test file templates with concrete examples","Store new patterns for continuous improvement"],instructions:"You are a Test Coverage Optimization Specialist that helps development teams\nimprove test coverage systematically and intelligently.\n\nYour goal is to analyze existing test patterns, identify critical gaps,\nand suggest high-impact new tests that developers can implement immediately.\n\nKey capabilities you possess:\n- Scan codebases for test files across multiple frameworks (pytest, jest, unittest, go-test)\n- Analyze existing test patterns and methodologies to understand team practices\n- Identify code paths that lack test coverage using static analysis\n- Remember and learn from previous testing patterns and improvements\n- Generate practical, ready-to-use test code with explanations\n- Track and improve suggestions based on feedback and patterns\n- Create customizable test templates that match team coding standards\n\nIMPORTANT: Always start by checking Memory for any saved testing patterns,\nteam preferences, and previous coverage analysis for this project.\nThis ensures your suggestions improve over time and match team standards.\n",parameters:[{key:"project_path",input_type:"string",requirement:"optional",default:".",description:"Path to project root directory (where source code is located)"},{key:"test_framework",input_type:"string",requirement:"optional",default:"auto",description:"Testing framework to target: 'jest' (JavaScript), 'pytest' (Python), 'unittest' (Python), 'go-test' (Go), or 'auto' for auto-detection"},{key:"coverage_threshold",input_type:"string",requirement:"optional",default:"80",description:"Target coverage percentage (0-100). Used to identify gap size and prioritize tests."},{key:"focus_areas",input_type:"string",requirement:"optional",default:"all",description:"Comma-separated focus areas: 'core-logic' (primary business logic), 'edge-cases' (boundary conditions), 'error-handling' (exceptions), 'integration' (API/module interactions), or 'all' for comprehensive analysis"},{key:"generate_templates",input_type:"string",requirement:"optional",default:"true",description:"Whether to generate test file templates with examples (true/false). Set to false if you only want suggestions without code templates."}],extensions:[{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"Stores and retrieves learned testing patterns, previous coverage analysis,\nand team preferences. Allows the recipe to improve suggestions over time\nby remembering what worked well in past runs.\n"},{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"Scans test files, analyzes coverage metrics, identifies code gaps,\nand generates new test code based on analysis and learned patterns\nfrom Memory. This is the primary tool for all file and code operations.\n"}],prompt:'You are analyzing test coverage for the project at {{ project_path }} to generate\nintelligent test suggestions that improve coverage to {{ coverage_threshold }}%.\n\n# =========================================================================\n# STEP 1: LOAD CONTEXT FROM MEMORY\n# =========================================================================\n# Purpose: Retrieve previously learned patterns to inform current analysis\n# Extension: Memory (loads first)\n\nStep 1: Load Testing Patterns from Memory\n  First, retrieve any previously learned testing patterns for this project:\n  - Retrieve key "{{ project_path }}_testing_style" - the team\'s preferred test structure\n  - Retrieve key "{{ project_path }}_gap_patterns" - common gap patterns from past analysis\n  - Retrieve key "{{ project_path }}_preferred_frameworks" - frameworks the team uses\n  - Retrieve key "testing_best_practices" - general testing best practices library\n  \n  If this is your first time analyzing this project, you\'ll start fresh\n  and build up knowledge over time. Store any retrieved patterns in your\n  context for use in the generation steps below.\n\n\n# =========================================================================\n# STEP 2: DETECT TEST FRAMEWORK\n# =========================================================================\n# Purpose: Identify which testing framework is used in this project\n# Extension: Developer (file scanning)\n\nStep 2: Detect Testing Framework\n  {% if test_framework == "auto" %}\n  Since test_framework is set to \'auto\', detect the framework automatically:\n    a) Search for test file patterns in {{ project_path }}:\n       - .test.js, .spec.js, __tests__/*.js files \u2192 JavaScript/Jest\n       - _test.py, test_*.py, tests/*.py files \u2192 Python/pytest or unittest\n       - *_test.go, *_test.go files \u2192 Go/go-test\n       - *.test.ts, *.spec.ts files \u2192 TypeScript/Jest\n    \n    b) Check for configuration files:\n       - package.json with "jest" dependency \u2192 Jest\n       - pytest.ini, setup.cfg, pyproject.toml \u2192 pytest\n       - go.mod \u2192 go test\n       - unittest imports in test files \u2192 Python unittest\n    \n    c) Report the detected framework with confidence level\n       Example: "Detected: pytest (high confidence - found pytest.ini)"\n  {% else %}\n  Use the specified framework: {{ test_framework }}\n  Verify that test files for {{ test_framework }} exist in {{ project_path }}\n  If no test files found, suggest creating a basic test structure for {{ test_framework }}\n  {% endif %}\n\n\n# =========================================================================\n# STEP 3: ANALYZE CURRENT TEST COVERAGE\n# =========================================================================\n# Purpose: Understand current state and identify coverage gaps\n# Extension: Developer (code scanning and analysis)\n\nStep 3: Analyze Current Test Coverage\n  Scan the project at {{ project_path }} for test coverage information:\n  \n  a) Test File Inventory:\n     - Find all test files matching the detected/specified framework\n     - Count total number of test files and individual test cases\n     - Calculate ratio: lines of test code vs lines of application code\n     - List the main test directories found\n  \n  b) Coverage Data Collection:\n     - Look for coverage reports in common locations:\n       * Python: .coverage, coverage.xml, htmlcov/, .coverage.*\n       * JavaScript: coverage/, coverage.json, coverage-final.json\n       * Go: coverage.out, coverage.html\n     - If coverage reports exist, parse them to get:\n       * Overall coverage percentage\n       * Per-file coverage breakdown\n       * Uncovered line numbers\n     - If NO coverage reports found:\n       * Estimate coverage by analyzing test file imports and function calls\n       * Identify source files that have no corresponding test files\n       * Note: "No coverage data found - will identify gaps by analysis"\n  \n  c) Gap Analysis:\n     - Compare current coverage to target {{ coverage_threshold }}%\n     - Identify files with coverage below threshold\n     - List top 10 files/modules with lowest coverage\n     - Focus analysis on {{ focus_areas }}\n     - Prioritize gaps by impact:\n       * Critical: Core business logic with <50% coverage\n       * High: Error handling and edge cases with <70% coverage\n       * Medium: Helper functions and utilities with <80% coverage\n\n\n# =========================================================================\n# STEP 4: IDENTIFY GAPS USING LEARNED PATTERNS\n# =========================================================================\n# Purpose: Use Memory\'s patterns to make smarter gap identification\n# Extension: Both (Developer analysis + Memory context)\n\nStep 4: Identify Testing Gaps (Using Learned Patterns)\n  Using both the coverage analysis AND the learned patterns from Step 1:\n  \n  a) Apply learned patterns to current analysis:\n     - If Memory contains testing style preferences, apply them\n     - Check if current gaps match previously identified gap patterns\n     - Consider team\'s preferred test granularity (unit vs integration)\n     - Note patterns like: "Team prefers testing error cases separately"\n  \n  b) Categorize and prioritize missing tests by {{ focus_areas }}:\n     {% if focus_areas == "all" or "core-logic" in focus_areas %}\n     - Core Logic Gaps:\n       * Functions/methods with no test coverage\n       * Business logic paths not exercised by tests\n       * Main workflows missing test scenarios\n     {% endif %}\n     \n     {% if focus_areas == "all" or "edge-cases" in focus_areas %}\n     - Edge Case Gaps:\n       * Boundary conditions (empty inputs, max values, null/undefined)\n       * Unusual input combinations\n       * Race conditions or timing-sensitive code\n     {% endif %}\n     \n     {% if focus_areas == "all" or "error-handling" in focus_areas %}\n     - Error Handling Gaps:\n       * Exception paths not tested\n       * Error callbacks or error states\n       * Validation failure scenarios\n     {% endif %}\n     \n     {% if focus_areas == "all" or "integration" in focus_areas %}\n     - Integration Gaps:\n       * API endpoint tests\n       * Database interaction tests\n       * External service mock/integration tests\n     {% endif %}\n  \n  c) Create prioritized list:\n     - Rank gaps by: (impact \xd7 likelihood \xd7 ease of testing)\n     - Mark which gaps align with learned patterns\n     - Identify quick wins (high impact, easy to test)\n\n\n# =========================================================================\n# STEP 5: GENERATE TEST SUGGESTIONS\n# =========================================================================\n# Purpose: Create specific, actionable test recommendations\n# Extension: Developer (code generation using learned context)\n\nStep 5: Generate Test Suggestions\n  For each identified gap, create specific test suggestions:\n  \n  a) For each high-priority gap:\n     - Write a clear, descriptive test name following framework conventions\n     - Explain what this test should verify\n     - Provide the expected test structure for {{ test_framework }}\n     - Include a brief code example showing the test skeleton\n     - If learned patterns exist, match the team\'s testing style\n     \n     Example format:\n     ```\n     Test: test_user_authentication_with_invalid_credentials\n     Purpose: Verify that authentication fails gracefully with wrong password\n     Priority: Critical (core-logic, error-handling)\n     Estimated effort: 10 minutes\n     \n     Code example for {{ test_framework }}:\n     [Framework-specific test code here]\n     ```\n  \n  b) Include implementation context:\n     - Why this test is important (coverage gap, risk mitigation)\n     - Which code path it covers\n     - Expected assertions and edge cases to test\n     - Dependencies or mocks needed\n  \n  c) Organize suggestions by priority:\n     - Critical (must-have for {{ coverage_threshold }}% target)\n     - High (important for robust coverage)\n     - Medium (nice-to-have, improves confidence)\n     \n  d) Limit initial suggestions:\n     - Provide top 5-10 most impactful tests\n     - Note: "Implement these first, then re-run for more suggestions"\n\n\n# =========================================================================\n# STEP 6: CREATE TEST TEMPLATES (Optional)\n# =========================================================================\n# Purpose: Generate ready-to-use test file templates\n# Extension: Developer (template creation)\n\nStep 6: Generate Test Templates\n  {% if generate_templates == "true" %}\n  Create test file templates that developers can immediately use:\n  \n  a) For files with NO existing tests:\n     - Create a complete new test file template\n     - Include proper imports for {{ test_framework }}\n     - Add setup/teardown methods if applicable\n     - Provide 2-3 example test functions with placeholders\n     - Match the coding style from Memory if available\n     \n  b) For files with SOME existing tests:\n     - Generate test functions to add to existing test files\n     - Match the existing test file\'s style and structure\n     - Include comments indicating where to insert the new tests\n  \n  c) Include in templates:\n     - Proper test file naming (matching framework conventions)\n     - Required imports and dependencies\n     - Mock/fixture setup if needed\n     - Clear TODOs for values to fill in\n     - Example assertions showing expected patterns\n  \n  d) Output format:\n     - Provide templates as copyable code blocks\n     - Include file paths where templates should be saved\n     - Add instructions for running the tests\n     \n  Example output:\n  ```\n  # File: tests/test_user_authentication.py\n  # Framework: pytest\n  # Purpose: Tests for user authentication module\n  \n  import pytest\n  from myapp.auth import authenticate_user\n  \n  def test_authentication_success():\n      # TODO: Replace with actual test data\n      user = {"username": "test_user", "password": "correct_password"}\n      result = authenticate_user(user)\n      assert result.success is True\n      assert result.user_id is not None\n  \n  def test_authentication_invalid_password():\n      # Test critical error path\n      user = {"username": "test_user", "password": "wrong_password"}\n      result = authenticate_user(user)\n      assert result.success is False\n      assert result.error == "Invalid credentials"\n  ```\n  {% else %}\n  Template generation is disabled. Providing suggestions only (see Step 5).\n  {% endif %}\n\n\n# =========================================================================\n# STEP 7: STORE PATTERNS IN MEMORY\n# =========================================================================\n# Purpose: Learn from this analysis to improve future runs\n# Extension: Memory (pattern storage)\n\nStep 7: Store Analysis Results and Patterns in Memory\n  Update Memory with findings from this analysis:\n  \n  a) Store project-specific patterns:\n     - Save key "{{ project_path }}_testing_style":\n       * Detected framework: {{ test_framework }}\n       * Common test patterns observed (e.g., "uses fixtures", "prefers mocks")\n       * Naming conventions detected\n       * Code style preferences\n     \n     - Save key "{{ project_path }}_gap_patterns":\n       * Types of gaps found this run\n       * Areas consistently lacking coverage\n       * Common missing test scenarios\n     \n     - Save key "{{ project_path }}_coverage_baseline":\n       * Current coverage: [calculated percentage]%\n       * Target coverage: {{ coverage_threshold }}%\n       * Date of analysis\n       * Number of tests suggested\n  \n  b) Update general knowledge:\n     - Save key "testing_best_practices":\n       * Effective test patterns encountered\n       * Framework-specific tips learned\n       * Common pitfalls to avoid\n  \n  c) Store metadata for tracking:\n     - Analysis timestamp\n     - Framework detected/used\n     - Focus areas analyzed\n     - Number of suggestions generated\n     - Estimated coverage improvement\n  \n  Note: This stored information will be used in Step 1 of the next run\n  to provide better, more contextual suggestions.\n\n\n# =========================================================================\n# STEP 8: PRESENT FINAL REPORT\n# =========================================================================\n# Purpose: Clearly communicate results and next steps to user\n# Extension: Both (summarized results)\n\nStep 8: Present Comprehensive Report\n  Provide a well-structured final report to the user:\n  \n  a) Coverage Summary:\n     ```\n     \ud83d\udcca Test Coverage Analysis for {{ project_path }}\n     \n     Current Coverage: [calculated percentage]% (based on analysis/reports)\n     Target Coverage: {{ coverage_threshold }}%\n     Coverage Gap: [target minus current]% (target - current)\n     \n     Framework Detected: {{ test_framework }}\n     Focus Areas: {{ focus_areas }}\n     Tests Analyzed: [count] test files, [count] test cases\n     ```\n  \n  b) Top Recommended Tests:\n     List the 5-10 highest priority test suggestions:\n     ```\n     \ud83c\udfaf High-Priority Test Suggestions:\n     \n     1. [CRITICAL] test_user_login_with_invalid_credentials\n        - What: Verify authentication fails gracefully\n        - Why: Core security logic, currently untested\n        - Effort: ~10 minutes\n        - Coverage gain: +2-3%\n     \n     2. [CRITICAL] test_data_validation_edge_cases\n        - What: Test empty/null input handling\n        - Why: Common source of production bugs\n        - Effort: ~15 minutes\n        - Coverage gain: +1-2%\n     \n     [... continue for top suggestions]\n     ```\n  \n  c) Implementation Guidance:\n     ```\n     \ud83d\udcdd Next Steps:\n     \n     1. Start with CRITICAL tests (biggest impact)\n     2. {% if generate_templates == "true" %}\n        Use the templates provided above - copy and customize\n        {% else %}\n        Create test files based on suggestions above\n        {% endif %}\n     3. Run tests to verify they work: {{ test_framework }} [command]\n     4. Run coverage again to see improvement\n     5. Re-run this recipe for additional suggestions\n     \n     \ud83d\udca1 Pro Tips:\n     - Implement tests iteratively (don\'t try to do everything at once)\n     - Run coverage after each batch to track progress\n     - For {{ coverage_threshold }}% target, focus on core-logic first\n     - This recipe learns - patterns will improve with each run\n     ```\n  \n  d) Coverage Projection:\n     ```\n     \ud83d\udcc8 Estimated Impact:\n     \n     If you implement the top 5 suggested tests:\n     - Estimated coverage increase: +[percentage]%\n     - New coverage projection: [new total]%\n     - Remaining gap to target: [remaining gap]%\n     \n     Tests needed to reach {{ coverage_threshold }}%: approximately [number] more tests\n     ```\n  \n  e) Learned Patterns Summary:\n     ```\n     \ud83e\udde0 Memory Updated:\n     \n     - Stored testing style preferences for {{ project_path }}\n     - Saved [count] gap patterns for future reference\n     - Updated coverage baseline\n     - Future runs will provide better suggestions based on this analysis\n     ```\n\n\n# =========================================================================\n# COMPLETION\n# =========================================================================\n\nYour analysis is complete! The user now has:\n- Clear understanding of current coverage gaps\n- Prioritized list of tests to implement\n{% if generate_templates == "true" %}\n- Ready-to-use test templates\n{% endif %}\n- Improved future suggestions through Memory learning\n\nRemind the user to re-run this recipe after implementing tests to:\n1. See coverage improvements\n2. Get additional suggestions based on learned patterns\n3. Continue iterating toward {{ coverage_threshold }}% coverage\n'}},39701:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Use OpenMetadata",description:"Interact with OpenMetadata in Goose via OpenMetadata MCP Server to generate SQL or propagate classifications",instructions:"Utilize OpenMetadata tools to search for, retrieve, and modify metadata related to data assets such as tables, dashboards, and glossaries. Adhere to specified output formats like JSON for search queries and maintain structured responses using Markdown for human-readable information.",extensions:[{type:"stdio",name:"openmetadata",cmd:"npx",args:["-y","mcp-remote","http://localhost:8585/mcp","--auth-server-url=http://localhost:8585/mcp","--client-id=openmetadata","--clean","--header","Authorization:${AUTH_HEADER}"],envs:{},env_keys:["AUTH_HEADER"],timeout:300,description:"",bundled:!1}],settings:{temperature:0},activities:["Generate SQL Given FQN","List Tables Given FQN","Propagate changes in certification/owner/tags to tables given FQN"],parameters:[{key:"fqn",input_type:"string",requirement:"required",description:"The fully qualified name of the asset in openmetadata you'd like an agent to act on"}],prompt:"Take classifications from {{ fqn }} and apply them to all the tables that are listed in {{ fqn }}\n\nHere's what to do step by step:\n\n1. **Verify {{ fqn }} exists in openmetadata**\n2. **Ask user if they will be propagating the {{ fqn }} owner/certification or a particular tag**\n3. **Get details of {{ fqn }} in openmetadata**\n  - the owner/certification/tag to be applied to other assets\n4. **List tables of {{ fqn }}**\n5. **Patch all tables that are returned**\n",author:{contact:"nickacosta"}}},71064:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Web Accessibility Auditor",description:"Comprehensive web accessibility auditing tool that analyzes websites for WCAG compliance, identifies barriers for users with disabilities, and provides actionable remediation recommendations",author:{contact:"ARYPROGRAMMER"},activities:["Fetch HTML content from target website using Fetch extension","Extract page structure and interactive elements","Analyze content for WCAG compliance issues","Generate detailed accessibility reports with severity levels","Provide specific remediation recommendations","Save all results to organized output directory"],instructions:"You are a Web Accessibility Auditor - an expert in making the web inclusive for all users.\n\nYour mission is to evaluate websites for accessibility compliance and provide comprehensive,\nactionable guidance to improve accessibility for users with disabilities.\n\nWorkflow:\n1. Use the Fetch extension to retrieve the complete HTML content from the target URL\n2. Extract page structure and interactive elements\n3. Perform automated accessibility analysis using WCAG standards\n4. Generate detailed reports with severity classifications\n5. Provide specific, implementable recommendations\n\nFocus on real user impact and prioritize issues that most affect people with disabilities.\n",parameters:[{key:"target_url",input_type:"string",requirement:"required",description:"The URL of the website to audit for accessibility"},{key:"audit_scope",input_type:"string",requirement:"optional",default:"single-page",description:"Scope of audit - 'single-page' (current page only) or 'multi-page' (crawl site structure)"},{key:"compliance_level",input_type:"string",requirement:"optional",default:"WCAG21-AA",description:"Accessibility standard to check against - 'WCAG21-A', 'WCAG21-AA', 'WCAG21-AAA'"},{key:"output_format",input_type:"string",requirement:"optional",default:"markdown",description:"Report output format - 'markdown', 'html', 'json'"},{key:"output_dir",input_type:"string",requirement:"optional",default:"./accessibility-audit-results",description:"Directory where audit results and reports will be saved"},{key:"date",input_type:"string",requirement:"optional",default:"2025-10-22",description:"Date for file naming (YYYY-MM-DD format)"}],sub_recipes:null,extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"For HTML analysis, report generation, and file operations"},{type:"stdio",name:"fetch",cmd:"uvx",args:["mcp-server-fetch"],display_name:"Fetch",timeout:300,bundled:!1,description:"For retrieving web content and HTML from target URLs"}],prompt:"Audit {{ target_url }} for web accessibility compliance ({{ compliance_level }}).\n\n**Audit Scope:** {{ audit_scope }}\n\n**Steps:**\n1. Fetch the HTML content from {{ target_url }} using the fetch extension\n2. If fetch fails, try alternative methods or use basic HTML structure analysis\n3. Analyze the HTML for WCAG accessibility issues\n4. Create the output directory {{ output_dir }} if it doesn't exist\n5. Generate all required report files\n\n**Error Handling:**\n- If fetch extension fails, attempt to continue with basic analysis\n- If completely unable to get content, create reports indicating the issue\n- Always generate output files even if analysis is incomplete\n\n**Key Areas to Check:**\n- Images: alt text, decorative images\n- Structure: headings, semantic HTML\n- Navigation: links, keyboard access\n- Forms: labels, error messages\n- Color: contrast ratios\n\n**Required Output Files:**\n- Save the fetched HTML as: {{ output_dir }}/source-html-{{ date }}.html\n- Create accessibility audit report: {{ output_dir }}/accessibility-audit-{{ date }}.{{ output_format }}\n- Create summary: {{ output_dir }}/accessibility-summary-{{ date }}.txt\n- Create JSON metrics: {{ output_dir }}/accessibility-score-{{ date }}.json\n\n**Analysis Requirements:**\n- Calculate overall accessibility score (0-100)\n- Identify critical, major, and minor issues\n- Provide specific remediation recommendations\n- Include code examples where helpful\n"}},75878:(e,t,n)=>{var i={"./analyze-pr.yaml":46611,"./change-log.yaml":6013,"./ci-cd-pipeline.yaml":31243,"./clean-up-feature-flag.yaml":14679,"./code-documentation-generator.yaml":19924,"./code-review-mentor.yaml":98e3,"./create-kafka-topic.yaml":22599,"./daily-standup-report-generator.yaml":27520,"./data-analysis-pipeline.yaml":29012,"./dependency-updater.yaml":66095,"./dev-guide-migration.yaml":1807,"./full-stack-project-initializer.yaml":46739,"./lint-my-code.yaml":7222,"./messy-column-fixer.yaml":90995,"./migrate-cypress-test-to-playwright.yaml":49174,"./migrate-from-poetry-to-uv.yaml":78984,"./pr-demo-planner.yaml":3799,"./pull-request-generator.yaml":59313,"./readme-bot.yaml":55536,"./recipe-generator.yaml":85636,"./remove-ai-artifacts-from-python-code.yaml":35225,"./smart-task-organizer.yaml":46643,"./sunno-song-format-generator.yaml":82387,"./test-coverage-optimizer.yaml":38163,"./use-openmetadata.yaml":39701,"./web-accessibility-auditor.yaml":71064};function a(e){var t=r(e);return n(t)}function r(e){if(!n.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}a.keys=function(){return Object.keys(i)},a.resolve=r,e.exports=a,a.id=75878},25191:(e,t,n)=>{"use strict";n.d(t,{A:()=>i});const i=(0,n(75395).A)("Menu",[["line",{x1:"4",x2:"20",y1:"12",y2:"12",key:"1e0a9i"}],["line",{x1:"4",x2:"20",y1:"6",y2:"6",key:"1owob3"}],["line",{x1:"4",x2:"20",y1:"18",y2:"18",key:"yk5zj1"}]])},69158:(e,t,n)=>{"use strict";n.d(t,{A:()=>i});const i=(0,n(75395).A)("X",[["path",{d:"M18 6 6 18",key:"1bl5f8"}],["path",{d:"m6 6 12 12",key:"d8bk6v"}]])}}]);