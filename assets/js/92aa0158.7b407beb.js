/*! For license information please see 92aa0158.7b407beb.js.LICENSE.txt */
(self.webpackChunkgoose=self.webpackChunkgoose||[]).push([[3186],{46792:(e,t,n)=>{"use strict";n.d(t,{$:()=>a});n(96540);var i=n(74848);const a=e=>{let{children:t,className:n="",variant:a="default",size:r="default",...o}=e;return(0,i.jsx)("button",{className:`flex rounded-full focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-accent dark:focus:ring-offset-gray-900 ${{default:"bg-black dark:bg-white text-white dark:text-black hover:bg-accent/90 dark:hover:bg-accent/80",ghost:"bg-transparent hover:bg-gray-100 dark:hover:bg-gray-700 dark:text-gray-300",link:"bg-transparent text-accent hover:underline hover:text-textProminent dark:text-accent/90"}[a]} ${{default:"px-6 py-3",icon:"p-2"}[r]} ${n}`,...o,children:t})}},57356:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>h});var i=n(43938),a=n(16445),r=n(56347),o=n(96540),s=n(56289),c=n(52362),l=n(58069),d=n(46792),p=n(54631),u=n(62636),m=n(74848);const f={"GitHub MCP":"bg-yellow-100 text-yellow-800 border-yellow-200","Context7 MCP":"bg-purple-100 text-purple-800 border-purple-200",Memory:"bg-blue-100 text-blue-800 border-blue-200"};function h(){const e=(0,r.zy)(),[t,n]=(0,o.useState)(null),[h,g]=(0,o.useState)(!0),[y,b]=(0,o.useState)(null),[v,w]=(0,o.useState)(!1),[k,x]=(0,o.useState)({});(0,o.useEffect)((()=>{(async()=>{try{g(!0),b(null),x({}),w(!1);const t=new URLSearchParams(e.search).get("id");if(!t)return void b("No recipe ID provided");const i=await(0,p.d)(t);i?n(i):b("Recipe not found")}catch(t){b("Failed to load recipe details"),console.error(t)}finally{g(!1)}})()}),[e]);const _=t?.parameters||[];_.filter((e=>"required"===e.requirement));if(h)return(0,m.jsx)(i.A,{children:(0,m.jsx)("div",{className:"min-h-screen flex items-start justify-center py-16",children:(0,m.jsxs)("div",{className:"container max-w-5xl mx-auto px-4 animate-pulse",children:[(0,m.jsx)("div",{className:"h-12 w-48 bg-bgSubtle dark:bg-zinc-800 rounded-lg mb-4"}),(0,m.jsx)("div",{className:"h-6 w-full bg-bgSubtle dark:bg-zinc-800 rounded-lg mb-2"}),(0,m.jsx)("div",{className:"h-6 w-2/3 bg-bgSubtle dark:bg-zinc-800 rounded-lg"})]})})});if(y||!t)return(0,m.jsx)(i.A,{children:(0,m.jsx)("div",{className:"min-h-screen flex items-start justify-center py-16",children:(0,m.jsx)("div",{className:"container max-w-5xl mx-auto px-4 text-red-500",children:y||"Recipe not found"})})});const C="string"==typeof t.author?t.author:t.author?.contact;return(0,m.jsxs)(i.A,{children:[(0,m.jsx)("div",{className:"min-h-screen py-12",children:(0,m.jsxs)("div",{className:"max-w-4xl mx-auto px-4",children:[(0,m.jsxs)("div",{className:"mb-8 flex justify-between items-start",children:[(0,m.jsx)(s.A,{to:"/recipes",children:(0,m.jsxs)(d.$,{className:"flex items-center gap-2",children:[(0,m.jsx)(a.A,{className:"h-4 w-4"}),"Back"]})}),C&&(0,m.jsxs)("a",{href:`https://github.com/${C}`,target:"_blank",rel:"noopener noreferrer",className:"flex items-center gap-2 text-sm text-textSubtle hover:underline",children:[(0,m.jsx)("img",{src:`https://github.com/${C}.png`,alt:C,className:"w-6 h-6 rounded-full"}),"@",C]})]}),(0,m.jsxs)("div",{className:"bg-white dark:bg-[#1A1A1A] border border-borderSubtle dark:border-zinc-700 rounded-xl p-8 shadow-md",children:[(0,m.jsx)("h1",{className:"text-4xl font-semibold mb-2 text-textProminent dark:text-white",children:t.title}),(0,m.jsx)("p",{className:"text-textSubtle dark:text-zinc-400 text-lg mb-6",children:t.description}),t.activities?.length>0&&(0,m.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,m.jsx)("h2",{className:"text-2xl font-medium mb-2 text-textProminent dark:text-white",children:"Activities"}),(0,m.jsx)("div",{className:"flex flex-wrap gap-2",children:t.activities.map(((e,t)=>(0,m.jsx)("span",{className:"bg-surfaceHighlight dark:bg-zinc-900 border border-border dark:border-zinc-700 rounded-full px-3 py-1 text-sm text-textProminent dark:text-zinc-300",children:e},t)))})]}),t.extensions?.length>0&&(0,m.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,m.jsx)("h2",{className:"text-2xl font-medium mb-2 text-textProminent dark:text-white",children:"Extensions"}),(0,m.jsx)("div",{className:"flex flex-wrap gap-2",children:t.extensions.map(((e,t)=>{const n="string"==typeof e?e:e.name;return(0,m.jsx)("span",{className:`border rounded-full px-3 py-1 text-sm ${f[n]||"bg-gray-100 text-gray-800 border-gray-200 dark:bg-zinc-900 dark:text-zinc-300 dark:border-zinc-700"}`,children:n},t)}))})]}),t.prompt&&(0,m.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,m.jsx)("h2",{className:"text-2xl font-medium mb-4 text-textProminent dark:text-white",children:"Initial Prompt"}),(0,m.jsx)(c.A,{type:"info",className:"mb-4",children:"This prompt auto-starts the recipe when launched in Goose."}),(0,m.jsx)(l.A,{language:"markdown",children:t.prompt})]}),t.instructions&&(0,m.jsxs)("div",{className:"mb-6 border-t border-borderSubtle dark:border-zinc-700 pt-6",children:[(0,m.jsx)("h2",{className:"text-2xl font-medium mb-4 text-textProminent dark:text-white",children:"Instructions"}),(0,m.jsx)(l.A,{language:"markdown",children:t.instructions})]}),(0,m.jsxs)("div",{className:"pt-8 border-t border-borderSubtle dark:border-zinc-700 mt-6 flex gap-4",children:[(0,m.jsx)(s.A,{to:t.recipeUrl,target:"_blank",className:"inline-block text-white bg-black dark:bg-white dark:text-black px-6 py-2 rounded-full text-sm font-medium hover:bg-gray-900 dark:hover:bg-gray-100 transition-colors",children:"Launch in Goose Desktop \u2192"}),(0,m.jsxs)("div",{className:"relative group inline-block",children:[(0,m.jsx)("button",{onClick:()=>{if(_.length>0)return x({}),void w(!0);const e=`goose run --recipe ${t?.localPath}`;navigator.clipboard.writeText(e),u.Ay.success("CLI command copied!")},className:"text-sm font-medium px-6 py-2 rounded-full bg-zinc-200 dark:bg-zinc-800 text-zinc-700 dark:text-white hover:bg-zinc-300 dark:hover:bg-zinc-700 transition-colors cursor-pointer",children:"Copy Goose CLI Command"}),(0,m.jsx)("div",{className:"absolute bottom-full mb-2 left-1/2 -translate-x-1/2 hidden group-hover:block bg-zinc-800 text-white text-xs px-2 py-1 rounded shadow-lg whitespace-nowrap z-50",children:"Copies the CLI command to run this recipe"})]})]})]})]})}),v&&(0,m.jsx)("div",{className:"fixed inset-0 bg-black bg-opacity-60 z-50 flex items-center justify-center",children:(0,m.jsxs)("div",{className:"bg-white dark:bg-zinc-800 p-6 rounded-lg w-full max-w-md",children:[(0,m.jsx)("h3",{className:"text-lg font-semibold mb-4 text-zinc-900 dark:text-white",children:"Fill in parameters"}),_.map((e=>(0,m.jsxs)("div",{className:"mb-3",children:[(0,m.jsxs)("label",{className:"block text-sm text-zinc-700 dark:text-zinc-200 mb-1",children:[e.key," ","optional"===e.requirement&&(0,m.jsx)("span",{className:"text-zinc-400",children:"(optional)"})]}),(0,m.jsx)("input",{type:"text",value:k[e.key]||"",onChange:t=>x((n=>({...n,[e.key]:t.target.value}))),className:"w-full px-3 py-2 border border-zinc-300 dark:border-zinc-600 rounded bg-white dark:bg-zinc-700 text-zinc-900 dark:text-white"})]},e.key))),(0,m.jsxs)("div",{className:"flex justify-end gap-3",children:[(0,m.jsx)("button",{onClick:()=>w(!1),className:"text-sm text-zinc-600 dark:text-zinc-300 hover:underline",children:"Cancel"}),(0,m.jsx)("button",{onClick:()=>{const e=Object.entries(k).filter((e=>{let[,t]=e;return""!==t})).map((e=>{let[t,n]=e;return`${t}=${n}`})).join(" "),n=`goose run --recipe ${t?.localPath}${e?` --params ${e}`:""}`;navigator.clipboard.writeText(n),u.Ay.success("CLI command copied with params!"),w(!1)},className:"bg-purple-600 text-white px-4 py-2 rounded text-sm hover:bg-purple-700",children:"Copy Command"})]})]})})]})}},54631:(e,t,n)=>{"use strict";n.d(t,{d:()=>a,q:()=>r});const i=n(75878);function a(e){return o().find((t=>t.id===e))||null}async function r(e){const t=o();return e?t.filter((t=>t.title?.toLowerCase().includes(e.toLowerCase())||t.description?.toLowerCase().includes(e.toLowerCase())||t.action?.toLowerCase().includes(e.toLowerCase())||t.activities?.some((t=>t.toLowerCase().includes(e.toLowerCase()))))):t}function o(){return i.keys().map((e=>function(e){const t={id:e.id||e.title?.toLowerCase().replace(/\s+/g,"-")||"untitled-recipe",title:e.title||"Untitled Recipe",description:e.description||"No description provided.",instructions:e.instructions,prompt:e.prompt,extensions:Array.isArray(e.extensions)?e.extensions.map((e=>"string"==typeof e?{type:"builtin",name:e}:e)):[],activities:Array.isArray(e.activities)?e.activities:[],version:e.version||"1.0.0",author:"string"==typeof e.author?{contact:e.author}:e.author||void 0,action:e.action||void 0,persona:e.persona||void 0,tags:e.tags||[],recipeUrl:"",localPath:`documentation/src/pages/recipes/data/recipes/${e.id}.yaml`};if(Array.isArray(e.parameters)){for(const t of e.parameters)"required"!==t.requirement||t.value||(t.value=`{{${t.key}}}`);t.parameters=e.parameters}const n={title:t.title,description:t.description,instructions:t.instructions,prompt:t.prompt,activities:t.activities,extensions:t.extensions,parameters:t.parameters||[]},i=function(e){if("undefined"!=typeof window&&window.btoa)return window.btoa(unescape(encodeURIComponent(e)));return Buffer.from(e).toString("base64")}(JSON.stringify(n));return t.recipeUrl=`goose://recipe?config=${i}`,t}({...i(e).default||i(e),id:e.replace(/^.*[\\/]/,"").replace(/\.(yaml|yml)$/,"")})))}},46611:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Analyse PR",author:{contact:"douwe"},description:"Analyse a pr",instructions:"Your job is to analyse and explain a PR",activities:["Query authentication logs","Investigate Sentry reports","Correlate device usage with auth events","Query Snowflake user identity tables","Review repo code for auth issues"],parameters:[{key:"pr",input_type:"string",requirement:"required",description:"name of the pull request"},{key:"repo",input_type:"string",requirement:"optional",description:"name of the repo. uses the current one if not selected",default:""}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing and retrieving formating preferences that might be present"}],prompt:"Analyze the pr with the name {{ pr }}. Find out what has changed, try to figure out why these\nchanges were made and tell the user in detail what you found out.\n{% if repo %}\nWe are working with the {{ repo }} repository, so make sure to add that to all commands.\n{% endif %}\n\nSteps:\n1. Find the actual pull request. {{ pr }} is the name or part of it. You can just run\n   `gh pr list`\n   and see which prs are open. Note which one the user is talking about\n2. Look at what is changed. You can run:\n   `gh pr view <pr-number> --comments --commits --files`\n   to get an overview.\n3. Optionally: if this looks complicated you could check out the relevant commit and have\n   a look at the files involved to get more context. If you do this, mark which branch you\n   were on. If there are pending changes, do a git stash\n4. Gather your thoughts and tell the user what changed, which changes look like they might\n   be worth an extra look and give them an idea of maybe why these changes were needed\n5. Clean up after yourself. If you cloned a repository or checked out a commit, make sure\n   you return the state to what it was before. So if in step 3 you changed branch, change\n   it back. If you had git stashed something, stash pop it again.\n"}},6013:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Generate Change Logs from Git Commits",description:"Generate Change Logs from Git Commits",instructions:"Follow the prompts to generate change logs from the provided git commits",activities:["Retrieve and analyze commits","Categorize changes","Format changelog entries","Update CHANGELOG.md"],prompt:'Task: Add change logs from Git Commits\n1. Please retrieve all commits between SHA {{start_sha}} and SHA {{end_sha}} (inclusive) from the repository.\n\n2. For each commit:\n  - Extract the commit message\n  - Extract the commit date\n  - Extract any referenced issue/ticket numbers (patterns like #123, JIRA-456)\n\n3. Organize the commits into the following categories:\n  - Features: New functionality added (commits that mention "feat", "feature", "add", etc.)\n  - Bug Fixes: Issues that were resolved (commits with "fix", "bug", "resolve", etc.)\n  - Performance Improvements: Optimizations (commits with "perf", "optimize", "performance", etc.)\n  - Documentation: Documentation changes (commits with "doc", "readme", etc.)\n  - Refactoring: Code restructuring (commits with "refactor", "clean", etc.)\n  - Other: Anything that doesn\'t fit above categories\n\n4. Format the release notes as follows:\n  \n  # [Version/Date]\n  \n  ## Features\n  - [Feature description] - [PR #number](PR link)\n  \n  \n  ## Bug Fixes\n  - [Bug fix description] - [PR #number](PR link)\n  \n  [Continue with other categories...]\n  \n  Example:\n  - Implement summary and describe-commands for better sq integration - [PR #369](https://github.com/squareup/dx-ai-toolbox/pull/369)\n  \n5. Ensure all the commit items has a PR link. If you cannot find it, try again. If you still cannot find it, use the commit sha link instead. For example: [commit sha](commit url)\n\n6. If commit messages follow conventional commit format (type(scope): message), use the type to categorize and include the scope in the notes.\n\n7. Ignore merge commits and automated commits (like those from CI systems) unless they contain significant information.\n\n8. For each category, sort entries by date (newest first).\n\n9. formatted change logs as a markdown document\n\n10. Create an empty CHANGELOG.md file if it does not exist\n\n11. Read CHANGELOG.md and understand its format.\n\n11. Insert the formatted change logs at the beginning of the CHANGELOG.md, and adjust its format to match the existing CHANGELOG.md format. Do not change any existing CHANGELOG.md content.\n',extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"start_sha",input_type:"string",requirement:"user_prompt",description:"the start sha of the git commits"},{key:"end_sha",input_type:"string",requirement:"user_prompt",description:"the end sha of the git commits"}],author:{contact:"lifeizhou-ap"}}},14679:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Clean Up Feature Flag",description:"Automatically clean up all references of a fully rolled out feature flag from a codebase and make the new behavior the default.",instructions:"Your job is to systematically remove a fully rolled out feature flag and ensure the new behavior is now the default. Use code search tools like ripgrep to identify all references to the flag, clean up definition files, usage sites, tests, and configuration files. Then create a commit and push changes with clear commit messages documenting the flag removal.\n",author:{contact:"amitdev"},extensions:[{type:"builtin",name:"developer"}],activities:["Remove feature flag definitions","Clean up feature flag usage sites","Update affected tests","Remove flag configurations","Document flag removal"],parameters:[{key:"feature_flag_key",input_type:"string",requirement:"required",description:"Key of the feature flag",value:"MY_FLAG"},{key:"repo_dir",input_type:"string",requirement:"optional",default:"./",description:"Directory of the codebase",value:"./"}],prompt:"Task: Remove a feature flag that has been fully rolled out, where the feature flag's functionality should become the default behavior.\n\nContext:\n\nFeature flag key: {{feature_flag_key}}\nProject: {{repo_dir}}\nFeature is fully rolled out and stable, meaning the feature flag is always evaluated to true or Treatment, etc.\n\nSteps to follow:\n\n1. Check out a *new* branch from main or master named using the feature flag key.\n2. Find the feature flag constant/object that wraps the key.\n3. Search for all references to the constant/object using ripgrep or equivalent tools.\n4. For each file that contains references:\n   - **Definition files**: Remove the flag definition and related imports.\n   - **Usage sites**: Remove conditional logic and default to the new behavior. Clean up related imports.\n   - **Test files**: Remove tests that cover the 'disabled' state of the flag and update remaining ones. Clean up mocks and imports.\n   - **Configuration files**: Remove entries related to the feature flag.\n5. Re-run a full-text search to ensure all references (and imports) are removed.\n6. Clean up now-unused variables or functions introduced solely for the flag.\n7. Double-check for and remove any leftover imports or dead code.\n8. Create a commit with **only the files affected by this cleanup** (don\u2019t use `git add .`).\n9. Push the branch to origin.\n10. Open a GitHub PR using: `https://github.com/squareup/<repo-name>/compare/<branch-name>` and replace the repo and branch placeholders.\n\nUse clear commit messages like:\n\n  chore(flag-cleanup): remove <feature_flag_key> flag from codebase\n\nExplain the flag was fully rolled out and the new behavior is now default.\n"}},98e3:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Code Review Mentor",description:"An intelligent code review assistant that learns your preferences and provides personalized, actionable feedback on code changes with improvement suggestions",author:{contact:"ARYPROGRAMMER"},activities:["Analyze code changes in git repository","Remember and apply reviewer preferences and coding standards","Identify bugs, security issues, and code smells","Suggest specific improvements with examples","Track and learn from feedback patterns over time"],instructions:"You are a Code Review Mentor - an intelligent assistant that provides thoughtful, personalized code reviews.\nYour goal is to help developers improve their code quality while learning and adapting to their specific preferences and team standards.\n\nKey capabilities:\n- Analyze git diffs to understand code changes\n- Remember coding preferences, style guidelines, and past feedback\n- Identify potential bugs, security vulnerabilities, and performance issues\n- Provide actionable suggestions with concrete examples\n- Learn from user interactions to improve future reviews\n- Track improvement patterns over time\n\nIMPORTANT: Always start by checking if there are any remembered preferences about coding standards, review priorities, or specific concerns for this project.\n",parameters:[{key:"review_scope",input_type:"string",requirement:"optional",default:"staged",description:"Scope of changes to review: 'staged' (staged changes), 'unstaged' (working directory), 'commit' (last commit), 'branch' (all commits in current branch vs main)"},{key:"review_depth",input_type:"string",requirement:"optional",default:"balanced",description:"Review depth level: 'quick' (focus on critical issues), 'balanced' (standard review), 'thorough' (detailed analysis including documentation and tests)"},{key:"focus_areas",input_type:"string",requirement:"optional",default:"all",description:"Comma-separated focus areas: 'security', 'performance', 'readability', 'testing', 'documentation', 'architecture', or 'all'"},{key:"language_specific",input_type:"string",requirement:"optional",default:"",description:"Optional: specify programming language for language-specific best practices (e.g., 'python', 'javascript', 'rust')"},{key:"project_context",input_type:"string",requirement:"optional",default:"",description:"Optional: brief project context or specific concerns to prioritize in this review"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0,description:"For git operations, file analysis, and code examination"},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing and retrieving coding preferences and review patterns"}],prompt:'Perform a comprehensive code review with the following parameters:\n- Review Scope: {{ review_scope }}\n- Review Depth: {{ review_depth }}\n- Focus Areas: {{ focus_areas }}\n{% if language_specific %}\n- Language: {{ language_specific }}\n{% endif %}\n{% if project_context %}\n- Project Context: {{ project_context }}\n{% endif %}\n\nFollow this systematic review process:\n\n1. Memory Check & Context Loading\n   First, retrieve any stored coding preferences and standards:\n   - Check for remembered coding style preferences\n   - Look for previously identified common issues or patterns\n   - Retrieve any project-specific guidelines or priorities\n   - Load language-specific best practices if applicable\n   \n   If this is a first-time review, note that preferences will be learned over time.\n\n2. Change Analysis\n   {% if review_scope == "staged" %}\n   Analyze staged changes using `git diff --staged`\n   {% elif review_scope == "unstaged" %}\n   Analyze unstaged changes using `git diff`\n   {% elif review_scope == "commit" %}\n   Analyze the last commit using `git show HEAD`\n   {% elif review_scope == "branch" %}\n   Analyze all commits in current branch vs main:\n   - First, identify current branch: `git branch --show-current`\n   - Compare with main: `git diff main...HEAD`\n   - List commits: `git log main..HEAD --oneline`\n   {% endif %}\n   \n   Extract and understand:\n   - Files modified and their purpose\n   - Nature of changes (new features, bug fixes, refactoring)\n   - Lines of code added/removed\n   - Complexity of changes\n\n3. Multi-Layered Review Analysis\n   \n   {% if review_depth == "quick" or review_depth == "balanced" or review_depth == "thorough" %}\n   \n   A. Critical Issues (Always check)\n      - Syntax errors and compilation issues\n      - Security vulnerabilities (SQL injection, XSS, insecure dependencies)\n      - Logic errors and potential bugs\n      - Memory leaks or resource management issues\n      - Error handling gaps\n   \n   {% endif %}\n   \n   {% if review_depth == "balanced" or review_depth == "thorough" %}\n   \n   B. Code Quality & Best Practices\n      - Code readability and maintainability\n      - Adherence to SOLID principles\n      - DRY (Don\'t Repeat Yourself) violations\n      - Naming conventions and consistency\n      - Code complexity and cognitive load\n      {% if language_specific %}\n      - {{ language_specific }}-specific idioms and best practices\n      {% endif %}\n   \n   C. Performance Considerations\n      {% if focus_areas == "all" or "performance" in focus_areas %}\n      - Algorithm efficiency (time complexity)\n      - Memory usage patterns\n      - Database query optimization\n      - Network calls and caching opportunities\n      - Resource cleanup and lifecycle management\n      {% endif %}\n   \n   {% endif %}\n   \n   {% if review_depth == "thorough" %}\n   \n   D. Testing & Documentation\n      {% if focus_areas == "all" or "testing" in focus_areas %}\n      - Test coverage for new/modified code\n      - Edge cases and error scenarios\n      - Unit test quality and assertions\n      - Integration test considerations\n      {% endif %}\n      \n      {% if focus_areas == "all" or "documentation" in focus_areas %}\n      - Code comments for complex logic\n      - Function/method documentation\n      - API documentation updates\n      - README or documentation updates needed\n      {% endif %}\n   \n   E. Architecture & Design\n      {% if focus_areas == "all" or "architecture" in focus_areas %}\n      - Design pattern appropriateness\n      - Separation of concerns\n      - Dependency management\n      - API design and contracts\n      - Future extensibility\n      {% endif %}\n   \n   {% endif %}\n\n4. Generate Structured Review Report\n   \n   Present your findings in this format:\n   \n   ## \ud83d\udcca Review Summary\n   - Files Changed: [count]\n   - Lines Added/Removed: [stats]\n   - Overall Assessment: [Excellent/Good/Needs Work/Critical Issues]\n   - Review Depth: {{ review_depth }}\n   {% if project_context %}\n   - Context: {{ project_context }}\n   {% endif %}\n   \n   ## \ud83d\udea8 Critical Issues\n   [List any blocking issues that must be fixed before merge]\n   - Issue description\n   - Location: file:line\n   - Why it\'s critical\n   - Suggested fix with code example\n   \n   ## \u26a0\ufe0f Important Improvements\n   [List significant issues that should be addressed]\n   - Issue description\n   - Location: file:line\n   - Impact if not fixed\n   - Suggested improvement with code example\n   \n   ## \ud83d\udca1 Suggestions & Best Practices\n   [List nice-to-have improvements]\n   - Suggestion description\n   - Location: file:line\n   - Benefit of implementing\n   - Example implementation (if applicable)\n   \n   ## \u2705 Positive Highlights\n   [Highlight good practices and well-implemented code]\n   - What was done well\n   - Why it\'s good practice\n   - Impact on code quality\n   \n   ## \ud83d\udcda Learning Points\n   [If applicable, share knowledge about patterns, idioms, or best practices]\n   - Key learning\n   - When to apply\n   - Resources for further reading\n   \n   ## \ud83d\udcc8 Improvement Tracking\n   [Compare with previous reviews if memory available]\n   - Patterns noticed\n   - Common issues from past reviews (if any)\n   - Progress indicators\n\n5. Interactive Feedback & Memory Update\n   \n   After presenting the review:\n   \n   A. Ask clarifying questions if needed:\n      - "Would you like me to elaborate on any of these points?"\n      - "Are there specific areas you\'d like me to focus more on?"\n      - "Do you have questions about any suggestions?"\n   \n   B. Learn from user responses:\n      - If user indicates a suggestion isn\'t applicable, remember context\n      - If user asks for more detail on certain topics, note the priority\n      - If user disagrees with a recommendation, understand why and adapt\n   \n   C. Store relevant memories:\n      - Project-specific coding standards\n      - User\'s priority areas (e.g., "User prefers security focus over performance")\n      - Language-specific preferences (e.g., "For Python, user prefers type hints")\n      - Review style preferences (e.g., "User prefers concise feedback")\n      - Common patterns in this codebase\n      - User\'s expertise level for adjusting explanation depth\n   \n   Use memory tool to save insights like:\n   - "Project uses [framework/pattern] - always check [specific concern]"\n   - "User prioritizes [focus area] over [other area]"\n   - "Common issue in this project: [pattern] - always flag"\n   - "User prefers [style/approach] for [situation]"\n\n6. Follow-up Actions\n   \n   Offer to:\n   - Generate a checklist of fixes to make\n   - Create example implementations for suggested improvements\n   - Review specific files in more detail\n   - Check related test files\n   - Update documentation based on changes\n   - Schedule a re-review after fixes\n\n## Review Principles\n\n- **Be Constructive**: Frame feedback positively and provide actionable solutions\n- **Be Specific**: Reference exact files, lines, and code snippets\n- **Prioritize**: Separate critical issues from nice-to-haves\n- **Teach**: Explain *why* something is an issue, not just *what* is wrong\n- **Adapt**: Learn from user preferences and adjust review style accordingly\n- **Encourage**: Recognize good practices and improvements\n- **Context-Aware**: Consider project stage, team experience, and business constraints\n\n## Remember\n\nThe goal is not perfect code, but better code. Help developers grow by:\n- Building confidence with positive reinforcement\n- Fostering learning through clear explanations\n- Adapting to individual and team preferences\n- Focusing on meaningful improvements over nitpicks\n- Creating a collaborative rather than critical tone\n\nStart the review now, and remember to check for any stored preferences first!\n'}},22599:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Create Kafka Topic",author:{contact:"danielst-block"},description:"Create a new Kafka topic with specified parameters.",activities:["Check for existing topic name conflicts","Validate publisher and subscriber names","Calculate optimal partition count","Generate Kafka topic configuration","Create topic directory and config files"],parameters:[{key:"topic_name",input_type:"string",requirement:"required",description:"The name of the Kafka topic to create"},{key:"owner",input_type:"string",requirement:"required",description:"The name/identifier of owner."},{key:"publisher",input_type:"string",requirement:"required",description:"The name/identifier of the publisher service or application"},{key:"subscribers",input_type:"string",requirement:"required",description:'Comma-separated list of subscriber services or applications that will consume from this topic (e.g., "service1,service2,service3")'},{key:"throughput",input_type:"string",requirement:"optional",description:"Expected throughput. Used to calculate optimal number of partitions for the topic",default:"unknown"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],instructions:"You are a Kafka topic creation assistant. Your job is to help create a new Kafka topic HCL \ndefinitions with the specified configuration including topic name, publisher, owner, \nsubscribers, and optional throughput. Follow the existing folder structure and conventions.\n",prompt:"1. Create a {{ topic_name }} directory for a Kafka topic based on the following parameters:\n  - Topic name: {{ topic_name }}\n  - Owner: {{ owner }}\n  - Publisher: {{ publisher }}\n  - Subscribers: {{ subscribers }}\n  - Throughput: {{ throughput }} messages/second (if provided)\n2. Ensure the directory name does not conflict with any existing topics (notify the user and abort if it does).\n3. Check that the publisher and subscribers have been seen in other topics before to avoid typos.\n4. If throughput is provided - calculate the optimal number of partitions. Otherwise, default to 4 partitions.\n5. Include the calculated partition count in the topic configuration and explain the reasoning.\n"}},29012:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Data Analysis Pipeline",description:"An advanced data analysis workflow that orchestrates multiple sub-recipes to clean, analyze, visualize, and report on datasets with intelligent format detection and conditional processing",author:{contact:"ARYPROGRAMMER"},activities:["Detect and validate data file format (CSV, JSON, Excel, Parquet)","Perform automated data cleaning and quality assessment","Conduct statistical analysis and identify patterns","Generate interactive visualizations and charts","Create comprehensive markdown reports with insights","Export results in multiple formats"],instructions:"You are a Data Analysis Pipeline orchestrator that intelligently processes datasets through multiple specialized stages.\n\nYour workflow:\n1. Detect the data format and validate structure\n2. Clean data and handle missing values\n3. Perform statistical analysis based on data type\n4. Generate appropriate visualizations\n5. Compile comprehensive reports\n\nUse sub-recipes for specialized tasks and coordinate their execution based on data characteristics.\nMaintain context between stages and pass relevant findings to subsequent analysis steps.\n",parameters:[{key:"data_file",input_type:"string",requirement:"required",description:"Path to the data file to analyze (supports CSV, JSON, Excel, Parquet)"},{key:"analysis_type",input_type:"string",requirement:"optional",default:"comprehensive",description:"Type of analysis - options are 'quick', 'comprehensive', 'statistical', 'exploratory'"},{key:"output_dir",input_type:"string",requirement:"optional",default:"./analysis_output",description:"Directory where analysis results and visualizations will be saved"},{key:"include_visualizations",input_type:"string",requirement:"optional",default:"true",description:"Whether to generate visualizations (true/false)"},{key:"report_format",input_type:"string",requirement:"optional",default:"markdown",description:"Output report format - options are 'markdown', 'html', 'pdf'"}],sub_recipes:[{name:"data_validator",path:"./subrecipes/data-validator.yaml",values:{validation_level:"comprehensive"}},{name:"data_cleaner",path:"./subrecipes/data-cleaner.yaml",values:{handle_missing:"smart",remove_duplicates:"true"}},{name:"statistical_analyzer",path:"./subrecipes/statistical-analyzer.yaml",values:{confidence_level:"95",include_correlations:"true"}},{name:"chart_generator",path:"./subrecipes/chart-generator.yaml",values:{chart_style:"modern",color_scheme:"viridis"}}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"For file operations, data processing, and script execution"},{type:"builtin",name:"memory",display_name:"Memory",timeout:300,bundled:!0,description:"For storing analysis context and intermediate results across stages"},{type:"stdio",name:"filesystem",cmd:"npx",args:["-y","@modelcontextprotocol/server-filesystem","{{ output_dir }}"],timeout:300,description:"Enhanced filesystem operations for managing analysis outputs"}],prompt:'Analyze {{ data_file }} with {{ analysis_type }} mode. Output to {{ output_dir }}.\n\nCRITICAL: Handle file paths correctly for all operating systems.\n- Detect the operating system (Windows/Linux/Mac)\n- Use appropriate path separators (/ for Unix, \\\\ for Windows)\n- Be careful to avoid escaping of slash or backslash characters\n- Use os.path.join() or pathlib.Path for cross-platform paths\n- Create output directories if they don\'t exist\n\nWorkflow:\n1. Validate: Run data_validator subrecipe on {{ data_file }}\n   - Store validation results in memory\n   - Check for critical issues before proceeding\n\n2. Clean: If issues found, run data_cleaner subrecipe\n   - Pass validation results to cleaner\n   - Handle cleaning errors gracefully\n\n{% if analysis_type == "statistical" or analysis_type == "comprehensive" %}\n3. Analyze: Run statistical_analyzer for stats and correlations\n   - Use cleaned data if available\n   - Store analysis results in memory\n{% endif %}\n\n{% if include_visualizations == "true" %}\n4. Visualize: Run chart_generator for key charts\n   - Create output directory structure\n   - Handle visualization errors\n{% endif %}\n\n5. Report: Create brief {{ report_format }} summary\n   - Save to {{ output_dir }}/report.{{ report_format }}\n   - Use OS-compatible path construction\n\nError Recovery:\n- If a sub-recipe fails, continue with remaining stages if possible\n- Log errors clearly with stage information\n- Provide partial results if complete analysis fails\n\nFor {{ analysis_type }}=="quick", skip heavy computations. Be efficient.\nUse memory extension to pass results between stages.\nAlways verify paths work on the current OS before file operations.\n'}},66095:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Dependency Updater",description:"Automatically checks for outdated dependencies in a project.",instructions:"As a Dependency Updater, your goal is to identify and report outdated dependencies within a project.",prompt:'You are analyzing the project at: {{ project_path }}\nOperation mode: {{ update_mode }}\n{% if package_filter %}Package filter: {{ package_filter }}{% endif %}\n\n1. Identify Project Type: Begin by analyzing the directory at {{ project_path }} to determine the programming language and its corresponding dependency management system (e.g., Node.js with `package.json`, Python with `requirements.txt`, Java with Maven/Gradle, Rust with `Cargo.toml`, Go with `go.mod`).\n\n2. Check for Outdated Dependencies: Execute the appropriate shell command to list all outdated dependencies for the identified project type.\n{% if package_filter %}Focus only on these packages: {{ package_filter }}{% endif %}\n\nCommon Examples:\n - For Node.js projects: `npm outdated` or `yarn outdated`\n - For Python projects: `pip list --outdated`\n - For Rust projects: `cargo outdated`\n - For Go projects: `go list -u -m all`\n - For Maven (Java) projects: `mvn versions:display-dependency-updates`\n - For Gradle (Java) projects: `gradle dependencyUpdates` (if the \'com.github.ben-manes.versions\' plugin is applied)\n - If the specific command is not immediately apparent, try to deduce it based on common practices for the project type or prompt the user for assistance.\n\n3. Report Findings: Clearly list all outdated dependencies you discover. For each outdated dependency, include its current installed version and the latest available version.\n\n4. Based on update_mode ({{ update_mode }}):\n - If "report": Only report the outdated dependencies\n - If "suggest": Provide specific update commands for each outdated dependency\n - If "interactive": Ask the user before suggesting updates for each package\n \nCommon update commands by Project Type:\n - For Node.js projects:\n   * Update all: `npm update` or `yarn upgrade`\n   * Update specific: `npm update [package-name]` or `yarn upgrade [package-name]`\n   * Update to latest: `npm install [package-name]@latest` or `yarn add [package-name]@latest`\n - For Python projects:\n   * Update specific: `pip install --upgrade [package-name]`\n   * Update all in requirements.txt: `pip install --upgrade -r requirements.txt`\n   * With poetry: `poetry update [package-name]` or `poetry update` (all)\n   * With uv: `uv add [package-name]@latest` or `uv sync --upgrade`\n - For Rust projects:\n   * Update all: `cargo update`\n   * Update specific: `cargo update [package-name]`\n   * Update to latest compatible: `cargo update --package [package-name]`\n - For Go projects:\n   * Update all: `go get -u ./...`\n   * Update specific: `go get -u [module-name]`\n   * Tidy dependencies: `go mod tidy`\n - For Maven (Java) projects:\n   * Update specific: `mvn versions:use-latest-versions -Dincludes=[group-id]:[artifact-id]`\n   * Update all: `mvn versions:use-latest-versions`\n - For Gradle (Java) projects:\n   * Check and apply updates: `gradle useLatestVersions` (requires plugin)\n   * Manual update: Edit build.gradle with new versions then `gradle build`\n - If the specific update command is not immediately apparent, try to deduce it based on common practices for the project type, check the project\'s documentation, or suggest general approaches like manually editing dependency files and running the build/install command.\n',activities:["Identify Project Type","Check for Outdated Dependencies","Report Findings","Suggest Update Commands"],parameters:[{key:"project_path",input_type:"string",requirement:"optional",default:".",description:"Path to the project directory to check for dependencies"},{key:"update_mode",input_type:"string",requirement:"optional",default:"report",description:"Mode of operation: 'report' (default) to only report outdated dependencies, 'suggest' to suggest update commands, 'interactive' to prompt before suggesting updates"},{key:"package_filter",input_type:"string",requirement:"optional",default:"",description:"Optional filter to check specific packages only (comma-separated list, e.g., 'react,lodash')"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],author:{contact:"abhijay007"}}},1807:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"dev guide migration from a specific file or files in a directory",description:"dev guide migration from a specific file or files in a directory",instructions:"Follow the prompts to migrate the doc page from source file(s) to target folder.",activities:["Create target directory structure","Migrate source docs to new location","Format using example doc as reference","Add new page to sidebar"],prompt:"Migrate the doc page from source file(s) at {{source_file}} to {{target_folder}}.  Please follow the instructions below:  \n1. Create the parent directory if the parent directory of the target file does not exist\n2. use {{example_file}} as a reference for the doc format\n3. retain all the information of the source file(s) in the target file \n4. If the page is not in the sidebar, add it in {{sidebar_file}}\n5. Ensure the target files \n      - has preserved the original content\n      - has correct formatting\n      - has clear and well-organized file structure\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],parameters:[{key:"source_file",input_type:"file",requirement:"user_prompt",description:"the source file(s) or the folder to migrate"},{key:"target_folder",input_type:"file",requirement:"user_prompt",description:"the target folder to migrate"},{key:"example_file",input_type:"file",requirement:"user_prompt",description:"the example file to follow the doc format"},{key:"sidebar_file",input_type:"file",requirement:"user_prompt",description:"the sidebar file to add the new doc page"}],author:{contact:"lifeizhou-ap"}}},46739:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Software Project Generator",description:"An advanced recipe that orchestrates complete project initialization (frontend, backend, full-stack, CLI, API) with intelligent framework detection, dependency management, code quality setup, and comprehensive documentation generation",author:{contact:"ARYPROGRAMMER"},activities:["Analyze project requirements and detect optimal tech stack","Initialize frontend and backend project structures","Configure development environment with linting and testing","Set up CI/CD pipeline and git repository","Generate comprehensive documentation and README","Install dependencies and verify project health"],instructions:"You are a Software Project Generator that creates production-ready project structures with best practices.\nCreate complete projects with frontend, backend, testing, linting, Docker, CI/CD, and documentation based on user parameters.\n\nIMPORTANT: Detect the operating system you're running on. If on Windows, be careful with disk paths - use forward slashes or properly escape backslashes to avoid path separators being interpreted as escape characters (e.g., avoid c:\\src\\react-project where \\r becomes a carriage return).\n",parameters:[{key:"project_name",input_type:"string",requirement:"required",description:"Name of the project to initialize (alphanumeric and dashes only)"},{key:"project_type",input_type:"string",requirement:"required",description:"Type of project - options are 'fullstack', 'frontend', 'backend', 'cli', 'api'"},{key:"frontend_framework",input_type:"string",requirement:"optional",default:"react",description:"Frontend framework to use (react, vue, svelte, nextjs, none)"},{key:"backend_framework",input_type:"string",requirement:"optional",default:"nodejs",description:"Backend framework to use (nodejs, python-fastapi, go, rust-actix, none)"},{key:"database",input_type:"string",requirement:"optional",default:"postgresql",description:"Database to integrate (postgresql, mongodb, mysql, sqlite, none)"},{key:"include_docker",input_type:"string",requirement:"optional",default:"true",description:"Whether to include Docker configuration (true/false)"},{key:"include_cicd",input_type:"string",requirement:"optional",default:"true",description:"Whether to include CI/CD pipeline (true/false)"},{key:"include_testing",input_type:"string",requirement:"optional",default:"true",description:"Whether to include testing setup with unit and integration tests (true/false)"},{key:"base_directory",input_type:"string",requirement:"optional",default:".",description:"Base directory where the project should be created"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:600,bundled:!0,description:"For file operations, git commands, and shell execution"}],prompt:'Initialize a {{ project_type }} project named "{{ project_name }}" with the following configuration:\n- Frontend: {{ frontend_framework }}\n- Backend: {{ backend_framework }}\n- Database: {{ database }}\n- Docker: {{ include_docker }}\n- CI/CD: {{ include_cicd }}\n- Testing: {{ include_testing }}\n\nFollow these steps:\n\n1. Create Project Structure\n   Navigate to {{ base_directory }} and create the project directory structure.\n   Initialize git repository with appropriate .gitignore for the tech stack.\n   {% if project_type == "fullstack" %}\n   Create frontend/ and backend/ directories.\n   {% endif %}\n\n{% if frontend_framework != "none" %}\n2. Frontend Setup\n   {% if frontend_framework == "react" %}\n     Create React app with Vite and TypeScript in the frontend directory.\n     Install react-router-dom and axios.\n     Create src/components, src/pages, src/hooks, src/utils, src/services directories.\n     Set up ESLint, Prettier, and Vitest for testing.\n     Create example Welcome component and test.\n   {% elif frontend_framework == "vue" %}\n     Create Vue 3 app with TypeScript, Router, and Pinia.\n     Install axios and dev dependencies.\n     Set up linting and testing.\n   {% elif frontend_framework == "nextjs" %}\n     Create Next.js app with TypeScript and Tailwind.\n     Install axios and swr.\n   {% elif frontend_framework == "svelte" %}\n     Create SvelteKit app with TypeScript.\n     Install axios and configure.\n   {% endif %}\n   Create .env.example and README.md for frontend.\n   {% if include_testing == "true" %}\n   Set up testing framework:\n     {% if frontend_framework == "react" %}\n       Configure Vitest with React Testing Library.\n       Create tests for components in __tests__ directories.\n       Add integration tests for key user flows.\n     {% elif frontend_framework == "vue" %}\n       Configure Vitest with Vue Test Utils.\n       Create component and integration tests.\n     {% elif frontend_framework == "nextjs" %}\n       Configure Jest with React Testing Library.\n       Create unit and integration tests.\n     {% elif frontend_framework == "svelte" %}\n       Configure Vitest with Svelte Testing Library.\n       Create component tests.\n     {% endif %}\n     Add test scripts to package.json (test, test:watch, test:coverage).\n   {% endif %}\n{% endif %}\n\n{% if backend_framework != "none" %}\n3. Backend Setup\n   {% if backend_framework == "nodejs" %}\n     Initialize Node.js backend with Express and TypeScript.\n     Install express, cors, dotenv, helmet, morgan, and types.\n     {% if database == "postgresql" %}Install pg and @types/pg{% elif database == "mongodb" %}Install mongoose{% elif database == "mysql" %}Install mysql2{% elif database == "sqlite" %}Install better-sqlite3{% endif %}.\n     Create src/ with routes, controllers, models, middleware, config, utils directories.\n     Create src/index.ts with Express server and health endpoint.\n     Set up TypeScript, ESLint, Prettier, Jest, and Supertest.\n     Create example health check test.\n   {% elif backend_framework == "python-fastapi" %}\n     Create Python backend with virtual environment.\n     Create requirements.txt with FastAPI, Uvicorn, and database drivers.\n     Install dependencies.\n     Create app/ with routers, models, schemas, services, core directories.\n     Create app/main.py with FastAPI and health endpoint.\n     Set up Black, Flake8, Mypy, and Pytest.\n   {% elif backend_framework == "go" %}\n     Initialize Go module.\n     Install Gin and database libraries.\n     Create cmd/api, internal/handlers, internal/models directories.\n     Create main.go with Gin server and health endpoint.\n     Set up golangci-lint and tests.\n   {% elif backend_framework == "rust-actix" %}\n     Create Rust project with cargo.\n     Add Actix-web dependencies to Cargo.toml.\n     Create main.rs with Actix server and health endpoint.\n     Set up rustfmt and clippy.\n   {% endif %}\n   Create .env.example and README.md for backend.\n   {% if include_testing == "true" %}\n   Set up testing framework:\n     {% if backend_framework == "nodejs" %}\n       Configure Jest and Supertest for API testing.\n       Create tests for routes, controllers, and services.\n       Add integration tests for database operations.\n       Create __tests__ directories alongside source files.\n     {% elif backend_framework == "python-fastapi" %}\n       Configure Pytest with pytest-asyncio and httpx.\n       Create tests for endpoints, services, and models.\n       Add integration tests for database operations.\n       Set up test fixtures and mocks.\n     {% elif backend_framework == "go" %}\n       Set up Go testing with testify.\n       Create _test.go files for handlers and services.\n       Add integration tests for API endpoints.\n     {% elif backend_framework == "rust-actix" %}\n       Configure cargo test with actix-web test utilities.\n       Create unit and integration tests.\n       Add test modules in src files.\n     {% endif %}\n     Add test scripts with coverage reporting.\n   {% endif %}\n{% endif %}\n\n4. Quality Tools\n   Create .editorconfig for consistent formatting across editors.\n   Create .vscode/settings.json with language-specific formatters.\n   Create .vscode/extensions.json with recommended extensions.\n   Add lint and test scripts to package.json files.\n   Create check-quality.sh script to run all linters and tests.\n\n5. Docker Setup\n   {% if include_docker == "true" %}\n   Create Dockerfile for each component (frontend/backend).\n   Create docker-compose.yml with services for:\n   {% if frontend_framework != "none" %}- Frontend{% endif %}\n   {% if backend_framework != "none" %}- Backend{% endif %}\n   {% if database != "none" %}- {{ database }} database{% endif %}\n   Configure volumes, environment variables, and port mappings.\n   Create .dockerignore file.\n   {% endif %}\n\n6. CI/CD Setup\n   {% if include_cicd == "true" %}\n   Create .github/workflows/ci.yml with jobs for:\n   - Checkout and setup\n   - Install dependencies\n   - Run linters\n   {% if include_testing == "true" %}\n   - Run unit tests with coverage\n   - Run integration tests\n   {% endif %}\n   - Build artifacts\n   {% if include_docker == "true" %}- Build Docker images{% endif %}\n   {% endif %}\n\n7. Documentation\n   Create comprehensive README.md with:\n   - Project overview and tech stack\n   - Prerequisites and installation\n   - Development and testing instructions\n   - Deployment guide\n   Create CONTRIBUTING.md with development guidelines.\n   Create QUICKSTART.md with common commands.\n   {% if project_type != "frontend" %}Create docs/API.md with endpoint documentation.{% endif %}\n   {% if database != "none" %}Create docs/DATABASE.md with schema info.{% endif %}\n\n8. Install and Verify\n   Install all dependencies for frontend and backend.\n   Run linters to verify configuration.\n   Run initial tests to ensure everything works.\n   Display summary of created project structure and next steps.\n\nWork through these steps systematically. Use shell commands to create files and directories.\n\nDetect the operating system that I\'m on, and use appropriate disk path separators, being careful to avoid \'escaping\' of slash or backslash characters as appropriate for my operating system when interpreting or using string values for filenames.\n'}},7222:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Lint My Code",author:{contact:"iandouglas"},description:"Analyzes code files for syntax and layout issues using available linting tools",instructions:"You are a code quality expert that helps identify syntax and layout issues in code files",activities:["Detect file type and programming language","Check for available linting tools in the project","Run appropriate linters for syntax and layout checking","Provide recommendations if no linters are found"],parameters:[{key:"file_path",input_type:"string",requirement:"required",description:"Path to the file you want to lint"}],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"I need you to lint the file at {{ file_path }} for syntax and layout issues only. Do not modify the file - just report any problems you find.\n\nHere's what to do step by step:\n\n1. **Verify the file exists and determine its type:**\n   - Check if {{ file_path }} exists\n   - Examine the file extension and content to determine the programming language/file type\n   - Focus on: Python (.py), JavaScript (.js, .jsx, .ts, .tsx), YAML (.yaml, .yml), HTML (.html, .htm), and CSS (.css)\n\n2. **Check for available linting tools in the project:**\n   - Look for common linting tools and configurations in the current project:\n     - Python: flake8, pylint, black, ruff, pycodestyle, autopep8\n     - JavaScript/TypeScript: eslint, prettier, jshint, tslint\n     - YAML: yamllint, yq\n     - HTML: htmlhint, tidy\n     - CSS: stylelint, csslint\n   - Check for configuration files like .eslintrc, .flake8, pyproject.toml, .yamllint, etc.\n   - Look in package.json, requirements.txt, or other dependency files\n\n3. **Run appropriate linting tools:**\n   - If linting tools are found, run them only on the specified file\n   - Use syntax-only or layout-only flags where available (e.g., `flake8 --select=E,W` for Python)\n   - Capture and report the output clearly\n\n4. **If no linters are found, provide recommendations:**\n   - For Python files: Suggest flake8, black, or ruff\n   - For JavaScript/TypeScript: Suggest ESLint and Prettier\n   - For YAML: Suggest yamllint\n   - For HTML: Suggest htmlhint or W3C validator\n   - For CSS: Suggest stylelint\n   - Provide installation commands and basic usage examples\n\n5. **Report results:**\n   - Clearly summarize any syntax or layout issues found\n   - If no issues are found, confirm the file appears to be clean\n   - If linting tools weren't available, explain what you checked manually and provide tool recommendations\n\nRemember: \n- Only check for syntax and layout issues, don't suggest code changes\n- Do not change the file on behalf of the user\n- Use tools that are already available in the project when possible\n- Be helpful by suggesting appropriate tools if none are found\n- Focus on the file types specified: Python, JavaScript, YAML, HTML, and CSS\n"}},90995:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Messy Column Fixer",author:{contact:"the-matrixneo"},description:"Fixes messy columns: normalizes and cleans CSV data.",instructions:"1. Provide the path to your CSV file.\n2. The recipe will scan all columns for type mismatches and missing values.\n3. It will suggest fixes (or automatically apply them, depending on your choice).\n4. Review the output and save your cleaned file.\n",activities:["Validate the input CSV file.","Analyze columns for data quality issues (mixed types, missing values).","Apply or suggest data cleaning and normalization fixes.","Generate a summary report and the cleaned CSV file.",'Provide the cleaned CSV file with a "_cleaned" suffix.'],parameters:[{key:"file_path",input_type:"string",requirement:"required",description:"Path to the CSV file you want to clean."},{key:"auto_fix_decision",input_type:"string",requirement:"optional",description:"Describe how fixes should be applied (e.g., 'apply automatically', 'suggest only').",default:"suggest only",choices:["apply automatically","suggest only"]}],extensions:[{type:"builtin",name:"developer",description:"Fixes messy columns in CSV files by normalizing and cleaning the data.",display_name:"Developer",timeout:300,bundled:!0}],prompt:"You are a CSV cleaning assistant.\n1.  First, validate that the file at {{ file_path }} exists and is a readable CSV. If not, inform the user and stop.\n2.  Scan the file to identify columns with mixed data types, missing values, or formatting issues.\n3.  Based on the {{ auto_fix_decision }} parameter, either suggest or apply fixes for the detected issues.\n4.  For each fix, briefly explain the reasoning (e.g., \"Converted 'Age' column to Integer because many values are numeric.\").\n5.  Provide a comprehensive summary of the changes and output the cleaned dataset.\n"}},49174:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Migrate Cypress tests to Playwright",author:{contact:"joahg"},description:"Migrate Cypress tests to Playwright",instructions:"Your job is to migrate cypress tests to playwright tests.",activities:["Analyze Cypress test file","Convert Cypress syntax to Playwright","Migrate custom commands and helpers","Update imports and async handling","Save Playwright test in target directory"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"You are tasked with migrating a Cypress test to Playwright. \n\nCypress test file: {{ cypress_test_file }}\nTarget directory: {{ target_directory }}\n\nPlease follow these steps:\n\n1. **Analyze the Cypress test file**: Examine the Cypress test file at {{ cypress_test_file }}, including its structure, commands, and any custom helper functions used.\n\n2. **Migrate the test structure**: Convert Cypress test syntax to Playwright:\n   - Replace `describe()` and `it()` with Playwright's `test.describe()` and `test()`\n   - Convert `cy.visit()` to `page.goto()`\n   - Convert `cy.get()` to appropriate Playwright locators\n   - Convert assertions from Cypress format to Playwright's `expect()` assertions\n   - Handle async/await patterns properly in Playwright\n\n3. **Migrate Cypress commands**: Convert common Cypress commands to Playwright equivalents:\n   - `cy.click()` \u2192 `locator.click()`\n   - `cy.type()` \u2192 `locator.fill()` or `locator.type()`\n   - `cy.should()` \u2192 `expect(locator).to**()`\n   - `cy.wait()` \u2192 `page.waitForTimeout()` or better, specific wait conditions\n   - `cy.intercept()` \u2192 `page.route()`\n\n4. **Migrate helper functions**: If the Cypress test uses custom commands or helper functions:\n   - Convert Cypress custom commands to Playwright helper functions\n   - Ensure helper functions are properly imported and available in the target directory\n   - Update function signatures to work with Playwright's page object\n\n5. **Update imports and setup**: \n   - Add proper Playwright imports (`import { test, expect } from '@playwright/test'`)\n   - Remove Cypress-specific imports\n   - Ensure proper test configuration and setup\n\n6. **Handle test data and fixtures**: Convert any Cypress fixtures or test data to work with Playwright\n\nCreate the migrated Playwright test in the target directory, maintaining the same test coverage and functionality as the original Cypress test. Use the same base filename but with appropriate Playwright test naming conventions (e.g., .spec.ts or .test.ts).\n",parameters:[{key:"cypress_test_file",input_type:"file",requirement:"user_prompt",description:"The specific Cypress test file to migrate (e.g., cypress/e2e/login.cy.js)"},{key:"target_directory",input_type:"file",requirement:"user_prompt",description:"The target directory where the Playwright test should be created"}]}},78984:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"migrate from poetry to uv",description:"migrate from poetry to uv",instructions:"Follow the instructions to move the project from using `poetry` to `uv`",author:{contact:"jamadeo"},activities:["Check if project already uses uv","Run migration using uvx","Remove poetry-related files and virtualenv","Run uv sync"],prompt:"The current project uses `poetry` for Python environment and dependency management. We want to use `uv` instead.\n\nFirst, verify that the above is true. If the project is actually already using `uv`, you can stop.\n\nStart by running `uvx migrate-to-uv`. If you don't have `uv` installed, use `hermit install uv` to add it. If hermit isn't set up, use `hermit init` to do so.\n\nOnce `migrate-to-uv` has run, delete any local virtualenvs (often located at ./.venv) and run `uv sync`.\n\nGrep for other uses of `poetry` in the project. If you can switch these commands to `uv`, do so. If not, just make a note of it.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}]}},3799:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"PR Demo Planner",author:{contact:"lifei"},description:"Transforms technical Pull Requests into effective demonstrations that showcase functionality and value",activities:["Analyze PR changes for demonstrable improvements","Create demo script and narrative flow","Build visual storyboard with before/after comparisons","Suggest environments and test data for effective demo","Translate technical changes into business value"],instructions:"You are a PR Demo Planner, an assistant specialized in transforming technical Pull Requests into engaging demonstrations.\n\nYour capabilities include:\n1. Analyzing PR changes to identify demonstrable features and improvements\n2. Creating structured demo scripts based on code changes\n3. Generating visual storyboards for demonstrations\n4. Helping prepare before/after comparisons that highlight improvements\n5. Crafting narratives that connect technical changes to business value\n6. Suggesting demo environments and test data\n\nWhen helping developers convert PRs to demos:\n\n- First understand the PR's purpose, scope, and technical changes\n- Identify the most visually demonstrable aspects of the changes\n- Create a narrative flow that showcases the improvements\n- Focus on before/after comparisons when applicable\n- Prepare for both technical and non-technical audiences\n- Include setup instructions to ensure smooth demonstrations\n- Suggest ways to highlight performance improvements or bug fixes\n\nYou have access to reference materials:\n- {{ recipe_dir }}/demo-formats.md for different demonstration approaches\n- {{ recipe_dir }}/demo-script-templates.md for structured presentation formats\n- {{ recipe_dir }}/technical-to-visual-guide.md for translating code changes to visual demonstrations\n\nAlways aim to create demonstrations that clearly show the value of the changes made in the PR.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"I need help converting my Pull Request into an effective demonstration. Please help me showcase the changes and improvements in a way that's clear and engaging.\n\nYou can assist me with:\n- Analyzing my PR to identify demonstrable features\n- Creating a structured demo script\n- Generating a visual storyboard\n- Preparing before/after comparisons\n- Crafting a narrative that explains the value\n- Setting up an effective demo environment\n\nThis is my PR: {{ pr_url }}\n",parameters:[{key:"pr_url",input_type:"string",requirement:"required",description:"The URL of the PR to convert into a demo."}]}},59313:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"PR Generator",description:"Automatically generate pull request descriptions based on changes in a local git repo.",instructions:"Your job is to generate descriptive and helpful pull request descriptions without asking for additional information. Generate commit messages and branch names based on the actual code changes.\n",author:{contact:"lifeizhou-ap"},extensions:[{type:"builtin",name:"developer"},{type:"builtin",name:"memory"}],parameters:[{key:"git_repo_path",input_type:"string",requirement:"required",description:"Path to the local git repository",value:"{{git_repo_path}}"},{key:"push_pr",input_type:"boolean",requirement:"optional",description:"Whether to push changes and create a PR",value:!1}],activities:["Generate PR","Analyze staged git changes","Create PR description"],action:"Generate PR",prompt:"Analyze the staged changes and any unpushed commits in the git repository {{git_repo_path}} to generate a comprehensive pull request description. Work autonomously without requesting additional information.\n\nAnalysis steps:\n1. Get current branch name using `git branch --show-current`\n2. If not on main/master/develop:\n   - Check for unpushed commits: `git log @{u}..HEAD` (if upstream exists)\n   - Include these commits in the analysis\n3. Check staged changes: `git diff --staged`\n4. Save the staged changes diff for the PR description\n5. Determine the type of change (feature, fix, enhancement, etc.) from the code\n\nGenerate the PR description with:\n1. A clear summary of the changes, including:\n   - New staged changes\n   - Any unpushed commits (if on a feature branch)\n2. Technical implementation details based on both the diff and unpushed commits\n3. List of modified files and their purpose\n4. Impact analysis (what areas of the codebase are affected)\n5. Testing approach and considerations\n6. Any migration steps or breaking changes\n7. Related issues or dependencies\n\nUse git commands:\n- `git diff --staged` for staged changes\n- `git log @{u}..HEAD` for unpushed commits\n- `git branch --show-current` for current branch\n- `git status` for staged files\n- `git show` for specific commit details\n- `git rev-parse --abbrev-ref --symbolic-full-name @{u}` to check if branch has upstream\n\nFormat the description in markdown with appropriate sections and code blocks where relevant.\n\n{% if push_pr %}\nExecute the following steps for pushing:\n1. Determine branch handling:\n   - If current branch is main/master/develop or unrelated:\n     - Generate branch name from staged changes (e.g., 'feature-add-user-auth')\n     - Create and switch to new branch: `git checkout -b [branch-name]`\n   - If current branch matches changes:\n     - Continue using current branch\n     - Note any unpushed commits\n\n2. Handle commits and push:\n   a. If staged changes exist:\n      - Create commit using generated message: `git commit -m \"[type]: [summary]\"`\n      - Message should be concise and descriptive of actual changes\n   b. Push changes:\n      - For existing branches: `git push origin HEAD`\n      - For new branches: `git push -u origin HEAD`\n\n3. Create PR:\n   - Use git/gh commands to create PR with generated description\n   - Set base branch appropriately\n   - Print PR URL after creation\n\nBranch naming convention:\n- Use kebab-case\n- Prefix with type: feature-, fix-, enhance-, refactor-\n- Keep names concise but descriptive\n- Base on actual code changes\n\nCommit message format:\n- Start with type: feat, fix, enhance, refactor\n- Followed by concise description\n- Based on actual code changes\n- No body text needed for straightforward changes\n\nDo not:\n- Ask for confirmation or additional input\n- Create placeholder content\n- Include TODO items\n- Add WIP markers\n{% endif %}\n"}},55536:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Readme Bot",author:{contact:"DOsinga"},description:"Generates or updates a readme",instructions:"You are a documentation expert",activities:["Scan project directory for documentation context","Generate a new README draft","Compare new draft with existing README.md"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"Here's what to do step by step:\n  1. The current folder is a software project. Scan it and learn as\n     much as possible.\n  2. Based on what you find, write a read me file that contains a\n     general description of the project, how to get started and how\n     to run the tests. Only mention future plans if you find explicit\n     todo's. Do not write about future plans or licenses or anything\n     that you can't find explicit support for.\n  3. Write this out as README.tmp.md.\n  4. Look at the existing README.md. If it exists and the version you\n     wrote out is not really better, just tell the user that what\n     exists is really good enough and you can exit.\n  5. If your version is better or no README.md exists, make your version\n     the current one\n  6. If you are on main or master, create a new branch\n  7. If the only chance at this point is the modification to the the\n     README.md, create a new commit \n  8. Clean up after yourself, delete the README.tmp.md after use.\n"}},85636:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Recipe Generator",author:{contact:"iYung"},description:"Creates other recipes",parameters:[{key:"prompt",input_type:"string",requirement:"required",description:"Description of what I want the recipe to do. Could be a file path"}],prompt:"Recipes are a set of instructions.\n\nHere is what a recipe should look like:\n```yaml\nversion: 1.0.0\ntitle: Title of my recipe\ndescription: Recipe Template\nprompt: Write your prompt in here\nextensions:\n  - type: builtin\n    name: developer\n    display_name: Developer\n    timeout: 300\n    bundled: true\n#only required if recipe description asks for user input\n#parameters are used in within prompt like \\{\\{ key }} and must be present\nparameters:\n  - key: example_parameter\n    input_type: string or number\n    requirement: required or optional\n    description: Description of the paramater.\n```\n\nImportant notes:\n- title is the name of the recipe\n- description is a short summary of what the recipe does\n- parameters are used within prompt like \\{\\{ key }} and must be present if mentioned in the recipe description\n\nUnder prompt can you write instructions that achieve\n{{ prompt }}\n\nIf the above is a file path, read the file to determine the goal.\n",extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}]}},35225:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Python un-AI",author:{contact:"douwe"},description:"Remove typical AI artifacts from Python code",instructions:"Your job is to write a remove AI artifacts from Python code",activities:["Remove redundant comments","Fix exception handling","Modernize typing","Inline trivial functions"],extensions:[{type:"builtin",name:"developer",display_name:"Developer",timeout:300,bundled:!0}],prompt:"Look at the file: {{ file_name }}\nApply the following fixes:\n1. Remove any comment that replicates the name of a function or describes the next statement\n   but does not add anything. Like if it says # call the server and it is followed by a\n   statement call_server(), that's pointless\n2. Any try.. except block where we catch bare Exception, remove that or if you can find a\n   specific exception to catch and it makes sense since we can actually do something better\n   catch that. But in general consider whether we need an exception like that, we don't want\n   to ignore errors and quite often the caller is in a better state to do the right thing\n   or even if it is a genuine error, the user can just take action\n3. Modernize the typing used (if any). Don't use List with a capital, just use list. Same for\n   Dict vs dict etc. Also remove Optional and replace with |None. Use | anywhere else where\n   it fits too.\n4. Inline trivial functions that are only called once, like reading text from a file.\n",parameters:[{key:"file_name",input_type:"file",requirement:"user_prompt",description:"the full path to the python file you want to sanitize"}]}},46643:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Smart Task Organizer",author:{contact:"research@aegntic.ai"},description:"Automatically organize and prioritize tasks from files, emails, and messages into an actionable todo list",instructions:"You are an intelligent task organizer that helps users turn scattered information into organized, actionable task lists. Your job is to scan various sources (files, messages, notes) and extract, categorize, and prioritize tasks effectively.\n\nFocus on:\n- Identifying concrete action items from text\n- Categorizing tasks by urgency and importance\n- Organizing tasks by project and context\n- Providing clear next steps for each task\n",activities:["Scan and parse text files for action items","Extract tasks from emails and messages","Categorize tasks by priority and project","Generate organized todo lists with deadlines","Create follow-up reminders"],parameters:[{key:"source_type",input_type:"string",requirement:"required",description:"Type of source to scan: files, emails, messages, notes, all"},{key:"priority_level",input_type:"string",requirement:"optional",description:"Filter by priority: urgent, high, medium, low, all",default:"all"},{key:"project_filter",input_type:"string",requirement:"optional",description:"Filter tasks by specific project name",default:""}],extensions:[{type:"stdio",name:"filesystem",cmd:"npx",args:["-y","@modelcontextprotocol/server-filesystem","/path/to/allowed/directory"],display_name:"Filesystem",timeout:300,bundled:!1}],prompts:{discovery_prompt:'You are a Task Discovery Agent. Your job is to scan and identify potential tasks from various sources.\n\n{% if source_type == "files" or source_type == "all" %}\nScan the current directory and subdirectories for files that might contain tasks:\n- Look for files with extensions: .txt, .md, .doc, .docx, .notes, .todo\n- Check README files, meeting notes, and project files\n- Search for task indicators: TODO, FIXME, ACTION ITEM, TASK, REMINDER, FOLLOW UP, NEED TO, SHOULD\n\nFor each file found, extract:\n- File path and name\n- Raw task text\n- Context surrounding the task\n- Any deadline or priority mentions\n{% endif %}\n\n{% if source_type == "emails" or source_type == "all" %}\nLook for email files or export files (.eml, .msg, .txt) that might contain tasks:\n- Search for action verbs: please, need to, should, must, review, complete, submit\n- Look for deadline indicators: by, due, EOD, EOW, ASAP, urgent\n- Extract sender/recipient context\n{% endif %}\n\n{% if source_type == "messages" or source_type == "all" %}\nCheck for chat logs, message exports, or conversation files:\n- Extract commitments and promises made\n- Identify questions that need responses\n- Find meeting follow-ups required\n- Note conversation participants and context\n{% endif %}\n\n{% if source_type == "notes" or source_type == "all" %}\nScan for note files and brain dumps:\n- Convert random thoughts into actionable items\n- Organize ideas into concrete tasks\n- Identify dependencies between tasks\n{% endif %}\n\nReturn a structured list of raw task candidates with their source context.\n',analysis_prompt:'You are a Task Analysis Agent. Your job is to analyze raw task candidates and extract structured information.\n\nFor each task candidate provided:\n1. Extract and standardize:\n   - Clear task description (what needs to be done)\n   - Priority level (urgent/high/medium/low) based on context\n   - Project or category affiliation\n   - Deadline (if mentioned, normalize to standard format)\n   - Dependencies (what needs to be done first)\n   - Estimated time to complete (if inferable)\n   - Source location and context\n\n2. Apply prioritization rules:\n   - URGENT: Today deadlines, blocking others, explicit "urgent" markers\n   - HIGH: This week deadlines, important milestones, commitments to others\n   - MEDIUM: Important but flexible, personal goals, nice-to-have-soon\n   - LOW: Ideas, future considerations, optional improvements\n\n3. Convert vague items to specific actions:\n   - "work on project" \u2192 "Review project requirements document and create task breakdown"\n   - "fix bugs" \u2192 "Identify and prioritize top 3 critical bugs in the backlog"\n   - "update documentation" \u2192 "Update API documentation for new endpoints added in v2.1"\n\nReturn structured task objects with all extracted fields.\n',organization_prompt:"You are a Task Organization Agent. Your job is to organize analyzed tasks into a structured, actionable format.\n\nOrganize the provided tasks into the following structure:\n\n## \ud83d\ude80 URGENT TASKS (Today)\n[Tasks that must be completed today - include specific deadlines]\n\n## \ud83d\udcc5 HIGH PRIORITY (This Week)\n[Important tasks with clear deadlines this week]\n\n## \ud83c\udfaf MEDIUM PRIORITY (This Sprint/Month)\n[Important but less time-sensitive tasks]\n\n## \ud83d\udcdd LOW PRIORITY (When Time Allows)\n[Nice-to-have tasks and ideas]\n\nFor each task, include:\n- \u2705 [Status] Task title (clear action verb + specific outcome)\n- \ud83d\udcc5 Deadline: [specific date or timeframe]\n- \ud83c\udfaf Project: [project/category]\n- \u23f1\ufe0f Estimate: [time estimate if available]\n- \ud83d\udd04 Dependencies: [what needs to be done first]\n- \ud83d\udccd Source: [where this task came from]\n\nGroup related tasks together when possible and suggest logical workflows.\n",summary_prompt:"You are an Action Planning Agent. Your job is to provide a comprehensive summary and immediate next steps.\n\nBased on the organized task list, provide:\n\n## \ud83d\udcca Task Summary\n- Total tasks found: [number]\n- Tasks by priority: Urgent: [X], High: [Y], Medium: [Z], Low: [W]\n- Tasks by project: [breakdown]\n- Estimated completion time: [total if available]\n\n## \ud83c\udfaf Immediate Next Steps (Top 3)\n1. [Most urgent task with clear first step]\n2. [Second priority task]\n3. [Third priority task]\n\n## \u26a0\ufe0f Potential Blockers\n- [List any dependencies, resource constraints, or timing conflicts]\n\n## \ud83d\udca1 Optimization Suggestions\n- [Suggest ways to batch similar tasks, delegate, or streamline workflow]\n\n## \ud83d\udd04 Recommended Workflow\n1. [Suggested order of operations]\n2. [How to track progress]\n3. [When to review and update]\n\nFocus on actionable insights that help the user get started immediately.\n"},prompt_chain:[{step:"discovery",prompt_ref:"discovery_prompt",output_filter:"Extract raw task candidates with source context"},{step:"analysis",prompt_ref:"analysis_prompt",input_from:"discovery",output_filter:"Structured task objects with priority and metadata"},{step:"organization",prompt_ref:"organization_prompt",input_from:"analysis",output_filter:"Organized task list by priority categories"},{step:"summary",prompt_ref:"summary_prompt",input_from:"organization",output_filter:"Comprehensive summary and action plan"}],prompt:'You are an intelligent Smart Task Organizer using MCP filesystem capabilities. Execute the complete task organization workflow:\n\n{% if source_type == "files" or source_type == "all" %}\n\ud83d\udcc1 **File Scanning Phase:**\nUse filesystem MCP to scan directories and read files containing potential tasks.\n{% endif %}\n\n{% if source_type == "emails" or source_type == "all" %}\n\ud83d\udce7 **Email Processing Phase:**\nLocate and parse email files for task-related content.\n{% endif %}\n\n{% if source_type == "messages" or source_type == "all" %}\n\ud83d\udcac **Message Analysis Phase:**\nProcess chat logs and conversation files for commitments and action items.\n{% endif %}\n\n{% if source_type == "notes" or source_type == "all" %}\n\ud83d\udcdd **Note Organization Phase:**\nExtract and structure tasks from notes and brain dumps.\n{% endif %}\n\nExecute the complete prompt chain:\n1. **Discovery**: Find all potential task sources\n2. **Analysis**: Extract and prioritize structured task data\n3. **Organization**: Create organized task lists by priority\n4. **Summary**: Generate actionable insights and next steps\n\n{% if priority_level != "all" %}\n**Priority Filter**: Focus exclusively on {{ priority_level }} priority tasks\n{% endif %}\n\n{% if project_filter %}\n**Project Focus**: Specialize in tasks related to "{{ project_filter }}"\n{% endif %}\n\nUse the filesystem MCP capabilities to:\n- Read file contents efficiently\n- Navigate directory structures\n- Process multiple files in parallel when possible\n- Maintain context across file operations\n\nApply intelligent task extraction, prioritization, and organization to transform scattered information into actionable, prioritized task lists.\n'}},82387:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Sunno Song Format Generator",description:"Generates a prompt for Sunno v4.5+",instructions:"I want to make some music on Sunno 4.5. The prompts are to be built up with style and lyrics separately. \nA song can be up to 8 minutes in length but this cannot be explicitly set by us. \nA song may be instrumental (no lyrics) or it may have lyrics. \n\nCRITICAL: Do not mention famous people or composers to avoid the song being copyright flagged.\n\nThe style of a song is to be vibrant in language and descriptive.\n\nIMPORTANT: Keep word count for the style prompt below 1000 characters.\n\nExample of style:\n```\nThe track opens with a dusty acoustic guitar riff and chopped banjo licks, quickly layered with gritty, filtered synth growls. \nA slow, stomping beat kicks in\u2014heavy on the low-end with punchy dubstep-style drums and distortion. \nGlitched vocal hooks echo like rowdy cowboy chants fed through a busted vocoder. \nThe drop slams with massive wobble basslines and screeching leads, underlaid with twangy string samples and distorted country riffs. \nMidway, the song breaks into a harmonica-driven buildup before unleashing another filthy drop\u2014this time with pitch-bent banjo stabs synced to grinding bass modulations. \nThe energy is rowdy, grimy, and unapologetically southern. Ends on a looped, glitched-out guitar hook fading into static and stomp.\n```\n\nYou may include a comma-separated list of tags to exclude from the style.\n\nExample:\n```\nspoken word, fast, edm, electronic\n```\n\nLyrics should be impactful and unique. Humanize the lyrics to make the song sound like a flowing poem with meaning instead of mechanical, soulless AI output.\n\nYou can use the following tags to structure sections of your song:\n\nSection Tags:\n```\n[Intro]     Soft or instrumental lead-in\n[Verse]     Lyrical development\n[Chorus]    Main hook or emotional core\n[Bridge]    Contrast section or pivot\n[Drop]      Beat-driven instrumental focus\n[Outro]     Closure or fade-out\n```\n\nVocal Tags:\n```\n[Vocalist: Female]      Suggests vocal gender\n[Vocalist: Alto]        Suggests range\n[Harmony: Yes]          Add background vocals\n[Vocal Effect: Reverb]  Suggest audio FX\n[Vocal Tone: Whisper]   Guide vocal style\n```\n\nMood and Texture Tags:\n```\n[Mood: Uplifting]       Emotion or feel\n[Tempo: Mid]            General rhythm\n[Energy: High]          Track momentum\n[Texture: Gritty]       Tonality influence\n```\n\nInstrument & Genre Tags:\n```\n[Instrument: Piano]                      Promote instrument use\n[Instrument: Electric Guitar (Distorted)]  Add edge or tone\n[Instrument: Strings (Legato)]           Elevate emotional feel\n[Instrument: 808]                        Suggest beat/bass format\n[Genre: Gospel]                          Set genre reference\n[Style: Lo-fi]                           Add texture/style filter\n[Era: 2000s]                             Suggest sound era\n```\n\nExample of a full lyrical structure:\n```\n[Intro]\n[Genre: Orchestral Rock]\n[Mood: Intense]\n[Instrument: Electric Guitar (Distorted)]\n[Instrument: Strings (Legato)]\n[Instrument: Drums (Heavy)]\n\n[Verse]\n[Energy: Medium]\nThrough the night, the echoes call,  \nA silent storm begins to fall.\n\n[Chorus]\n[Energy: High]\nLight the fire, feel the sound,  \nRise again, never back down.\n\n[Bridge]\n[Vocal Effect: Delay]\nWe rise and fall and rise again.\n```\n\nGuidelines:\n- Use one tag per category to start.\n- Place all key tags at the top of the song.\n- Use 2\u20133 instruments max per song.\n- Avoid conflicting tags (e.g., [Energy: Low] and [Energy: High]).\n- Refine with Replace, Extend, or Cover tools.\n\nWith all of this information, the user will now prompt you for a song creation!\n",author:{contact:"simonsickle"},prompt:"I will like my song to be about"}},39701:(e,t,n)=>{"use strict";n.r(t),n.d(t,{default:()=>i});const i={version:"1.0.0",title:"Use OpenMetadata",description:"Interact with OpenMetadata in Goose via OpenMetadata MCP Server to generate SQL or propagate classifications",instructions:"Utilize OpenMetadata tools to search for, retrieve, and modify metadata related to data assets such as tables, dashboards, and glossaries. Adhere to specified output formats like JSON for search queries and maintain structured responses using Markdown for human-readable information.",extensions:[{type:"stdio",name:"openmetadata",cmd:"npx",args:["-y","mcp-remote","http://localhost:8585/mcp","--auth-server-url=http://localhost:8585/mcp","--client-id=openmetadata","--clean","--header","Authorization:${AUTH_HEADER}"],envs:{},env_keys:["AUTH_HEADER"],timeout:300,description:"",bundled:!1}],settings:{temperature:0},activities:["Generate SQL Given FQN","List Tables Given FQN","Propagate changes in certification/owner/tags to tables given FQN"],parameters:[{key:"fqn",input_type:"string",requirement:"required",description:"The fully qualified name of the asset in openmetadata you'd like an agent to act on"}],prompt:"Take classifications from {{ fqn }} and apply them to all the tables that are listed in {{ fqn }}\n\nHere's what to do step by step:\n\n1. **Verify {{ fqn }} exists in openmetadata**\n2. **Ask user if they will be propagating the {{ fqn }} owner/certification or a particular tag**\n3. **Get details of {{ fqn }} in openmetadata**\n  - the owner/certification/tag to be applied to other assets\n4. **List tables of {{ fqn }}**\n5. **Patch all tables that are returned**\n",author:{contact:"nickacosta"}}},75878:(e,t,n)=>{var i={"./analyze-pr.yaml":46611,"./change-log.yaml":6013,"./clean-up-feature-flag.yaml":14679,"./code-review-mentor.yaml":98e3,"./create-kafka-topic.yaml":22599,"./data-analysis-pipeline.yaml":29012,"./dependency-updater.yaml":66095,"./dev-guide-migration.yaml":1807,"./full-stack-project-initializer.yaml":46739,"./lint-my-code.yaml":7222,"./messy-column-fixer.yaml":90995,"./migrate-cypress-test-to-playwright.yaml":49174,"./migrate-from-poetry-to-uv.yaml":78984,"./pr-demo-planner.yaml":3799,"./pull-request-generator.yaml":59313,"./readme-bot.yaml":55536,"./recipe-generator.yaml":85636,"./remove-ai-artifacts-from-python-code.yaml":35225,"./smart-task-organizer.yaml":46643,"./sunno-song-format-generator.yaml":82387,"./use-openmetadata.yaml":39701};function a(e){var t=r(e);return n(t)}function r(e){if(!n.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}a.keys=function(){return Object.keys(i)},a.resolve=r,e.exports=a,a.id=75878},75395:(e,t,n)=>{"use strict";n.d(t,{A:()=>s});var i=n(96540);const a=(...e)=>e.filter(((e,t,n)=>Boolean(e)&&""!==e.trim()&&n.indexOf(e)===t)).join(" ").trim();var r={xmlns:"http://www.w3.org/2000/svg",width:24,height:24,viewBox:"0 0 24 24",fill:"none",stroke:"currentColor",strokeWidth:2,strokeLinecap:"round",strokeLinejoin:"round"};const o=(0,i.forwardRef)((({color:e="currentColor",size:t=24,strokeWidth:n=2,absoluteStrokeWidth:o,className:s="",children:c,iconNode:l,...d},p)=>(0,i.createElement)("svg",{ref:p,...r,width:t,height:t,stroke:e,strokeWidth:o?24*Number(n)/Number(t):n,className:a("lucide",s),...d},[...l.map((([e,t])=>(0,i.createElement)(e,t))),...Array.isArray(c)?c:[c]]))),s=(e,t)=>{const n=(0,i.forwardRef)((({className:n,...r},s)=>{return(0,i.createElement)(o,{ref:s,iconNode:t,className:a(`lucide-${c=e,c.replace(/([a-z0-9])([A-Z])/g,"$1-$2").toLowerCase()}`,n),...r});var c}));return n.displayName=`${e}`,n}},16445:(e,t,n)=>{"use strict";n.d(t,{A:()=>i});const i=(0,n(75395).A)("ArrowLeft",[["path",{d:"m12 19-7-7 7-7",key:"1l729n"}],["path",{d:"M19 12H5",key:"x3x0zl"}]])}}]);